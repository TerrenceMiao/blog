[{"title":"Running latest MacOS Sequoia in VMware on Windows 11","date":"2024-10-16T05:06:59.000Z","path":"2024/10/16/Running-latest-MacOS-Sequoia-in-VMWare-on-Windows-11/","text":"This is the step by step guide, the fastest and easiest way install and run latest MacOS Sequoia 15.0.1 in VMware Workstation Pro on Windows 11. Due to MacOS Sequoia has added detection whether OS is running in Virtual Machine, so it’s better install MacOS Sonoma at first, then upgrade to latest version MacOS Sequoia. Build a bootable MacOS ISO image Do it on a Mac. Clone gibmasOS repo https://github.com/corpnewt/gibMacOS and run: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354$ ./gibMacOS.command######################################################## gibMacOS ########################################################Available Products:1. macOS Sequoia 15.0.1 (24A348) - 072-01382 - Added 2024-10-03 21:26:40 - 14.48 GB2. macOS Ventura 13.7 (22H123) - 062-78643 - Added 2024-09-16 17:44:05 - 12.22 GB3. macOS Sonoma 14.7 (23H124) - 062-78824 - Added 2024-09-16 17:42:25 - 13.68 GB4. macOS Sequoia 15.0 (24A335) - 062-78429 - Added 2024-09-16 17:30:21 - 14.48 GB5. macOS Monterey 12.7.6 (21H1320) - 062-40406 - Added 2024-08-14 20:45:56 - 12.42 GB...24. macOS High Sierra 10.13.6 (17G66) - 041-91758 - Added 2019-10-19 18:19:55 - 5.71 GB25. macOS Mojave 10.14.6 (18G103) - 061-26589 - Added 2019-10-14 20:51:08 - 6.52 GB26. macOS Mojave 10.14.5 (18F2059) - 061-26578 - Added 2019-10-14 20:38:26 - 6.52 GBM. Change Max-OS Version (Currently 12)C. Change Catalog (Currently publicrelease)I. Only Print URLs (Currently Off)S. Set Current Catalog to SoftwareUpdate CatalogL. Clear SoftwareUpdate CatalogR. Toggle Recovery-Only (Currently Off)U. Show Catalog URLQ. QuitPlease select an option: 3Downloading InstallAssistant.pkg for 062-78824 - 14.7 macOS Sonoma (23H124)...1.35 GB/14.48 GB | = 9.34% | 101.7 MB/s | 00:02:10 leftSucceeded: InstallAssistant.pkg MajorOSInfo.pkg com_apple_MobileAsset_MacSoftwareUpdate.plist InstallInfo.plist UpdateBrain.zipFailed: NoneFiles saved to: /Users/terrence/Projects/gibMacOS/macOS Downloads/publicrelease/062-78824 - 14.7 macOS Sonoma (23H124) Run InstallAssistant.pkg from above MacOS download directory. It will be using to build ISO image. Create a disk image with size 16GB: 12$ hdiutil create -o /tmp/MacOS -size 16000m -volname MacOS -layout SPUD -fs HFS+Jcreated: /tmp/MacOS.dmg Mount the disk image created above: 1234 $ hdiutil attach /tmp/MacOS.dmg -noverify -mountpoint /Volumes/MacOSISO/dev/disk6 Apple_partition_scheme /dev/disk6s1 Apple_partition_map /dev/disk6s2 Apple_HFS /Volumes/MacOSISO Create ISO image from Install masOS app: 1234567$ sudo /Applications/Install\\ macOS\\ Sonoma.app/Contents/Resources/createinstallmedia --volume /Volumes/MacOSISO --nointeractionErasing disk: 0%... 10%... 20%... 30%... 100%Copying essential files...Copying the macOS RecoveryOS...Making disk bootable...Copying to disk: 0%... 10%... 20%... 30%... 40%... 50%... 60%... 100%Install media now available at &quot;/Volumes/Install macOS Sonoma&quot; Unmount disk image and convert to an ISO image: 1234567891011121314151617181920$ hdiutil detach -force /Volumes/Install\\ macOS\\ Sonoma&quot;disk6&quot; ejected.$ ls -al /tmp/MacOS.dmg-rw-r--r--@ 1 terrence wheel 16777216000 15 Oct 21:54 /tmp/MacOS.dmg$ hdiutil convert /tmp/MacOS.dmg -format UDTO -o /tmp/MacOS-Sonoma-14.7.cdrReading Driver Descriptor Map (DDM : 0)…Reading Apple (Apple_partition_map : 1)…Reading (Apple_Free : 2)…Reading disk image (Apple_HFS : 3)…........................................Elapsed Time: 19.139sSpeed: 835.9MB/sSavings: 0.0%created: /tmp/MacOS-Sonoma-14.7.cdr$ mv /tmp/MacOS-Sonoma-14.7.cdr /tmp/MacOS-Sonoma-14.7.iso$ rm /tmp/MacOS.dmg Install VMware Workstation Pro Download VMware Workstation Pro from e.g. https://softwareupdate.vmware.com/cds/vmw-desktop/ws/17.6.1/24319023/windows/core/VMware-workstation-17.6.1-24319023.exe.tar Patch VMware Workstation Pro Clone unlocker https://github.com/paolo-projects/unlocker repo on Windows and enable Apple macOS option in VMware Workstation Pro: 12345678910111213141516171819PS C:\\Projects\\unlocker&gt; .\\win-install.cmdUnlocker 3.0.2 for VMware Workstation=====================================(c) Dave Parsons 2011-18Set encoding parameters...Active code page: 850VMware is installed at: C:\\Program Files (x86)\\VMware\\VMware Workstation\\VMware product version: 17.6.1.24319023Stopping VMware services......Starting VMware services...Finished! Add VMware Tools Copy darwin.iso and darwinPre15.iso files extracted from VMware Fusion e.g. _https://softwareupdate.vmware.com/cds/vmw-desktop/fusion/12.2.5/20904517/x86/core/com.vmware.fusion.zip.tar _ into VMware Workstation Pro directory: 12C:\\Projects\\unlocker\\tools\\darwin.iso -&gt; C:\\Program Files (x86)\\VMware\\VMware Workstation\\darwin.isoC:\\Projects\\unlocker\\tools\\darwinPre15.iso -&gt; C:\\Program Files (x86)\\VMware\\VMware Workstation\\darwinPre15.iso Create Virtual Machine for MacOS 14 Sonoma and update the settings Add: 1smc.version = &quot;0&quot; into MacOS Sonoma.vmx file. Clone GenSMBIOS repo https://github.com/corpnewt/GenSMBIOS and generate serial number on Windows: 123456789101112131415161718192021222324252627282930$ ./GenSMBIOS.bat######################################################## GenSMBIOS ########################################################MacSerial not found!Remote Version v2.1.8Current plist: NonePlist type: Unknown1. Install/Update MacSerial2. Select config.plist3. Generate SMBIOS4. Generate UUID5. Generate ROM6. List Current SMBIOS7. Generate ROM With SMBIOS (Currently Enabled)Q. QuitPlease select an option: 3Please type the SMBIOS to gen and the numberof times to generate [max 20] (i.e. iMac18,3 5): MacBookPro16,4Type: MacBookPro16,4Serial: C..........TBoard Serial: C0.............FBSmUUID: A0D50403-F256-4E17-A2EC-29964D889A1DApple ROM: 6..........7 Copy Serial, Board Serial and Apple ROM number, apply to: 12345678910board-id = &quot;Mac-A61BADE1FDAD7B05&quot;hw.model.reflectHost = &quot;FALSE&quot;hw.model = &quot;MacBookPro16,4&quot;serialNumber.reflectHost = &quot;FALSE&quot;serialNumber = &quot;C..........T&quot;smbios.reflectHost = &quot;FALSE&quot;efi.nvram.var.ROM.reflectHost = &quot;FALSE&quot;efi.nvram.var.MLB.reflectHost = &quot;FALSE&quot;efi.nvram.var.ROM = &quot;6..........7&quot;efi.nvram.var.MLB = &quot;C0.............FB&quot; then add above block into MacOS Sonoma.vmx file. Based on Apple Ethernet MAC Address range https://hwaddress.com/company/apple-inc/, change and add network settings from: 1ethernet0.addressType = &quot;generated&quot; to: 123ethernet0.addressType = &quot;static&quot;ethernet0.address = &quot;00:21:E9:c0:92:76&quot;ethernet0.checkMacAddress = &quot;FALSE&quot; in MacOS Sonoma.vmx file. Mount MacOS ISO image and install DON’T enable Location Service during the installation! Otherwise, you can’t setup Time Zone, Date Time based on your area. You can login with your Apple ID during the installation. Install VMware Tools After MacOS installed and VM restarted, mount darwin.iso and install VMware Tools. Then Display Memory in MacOS becomes 128 MB, and support the Full Screen mode. Upgrade MacOS After MacOS Sonoma installed, make sure everything is OK, then copy the whole MacOS Sonoma directory to a new directory MacOS Sequoia (Upgraded). Open the new directory in VMware, and select I copied it, then you can upgrade MacOS to latest verion in System Settings -&gt; Software Update. Run MacOS in VMware Player: References How to Install macOS Sequoia on VMware on Windows PC, https://www.youtube.com/watch?v=0ohUXE0Pj_U The Definitive Guide to Running MacOS in Proxmox, https://klabsdev.com/definitive-guide-to-running-macos-in-proxmox/ List of Mac BoardID, DeviceID, Model Identifiers &amp; Machine Models, https://mrmacintosh.com/list-of-mac-boardid-deviceid-model-identifiers-machine-models/","tags":[]},{"title":"How to access Ollama runs on localhost from internet","date":"2024-10-12T11:53:37.000Z","path":"2024/10/12/How-to-access-Ollama-runs-on-localhost-from-internet/","text":"By default, Ollama is only accessible from localhost. CORS error and HTTP 403 returned if try to invoke from other hosts. To enable Ollama can be visited from hosts on intranet, run: 1$ launchctl setenv OLLAMA_HOST &quot;0.0.0.0&quot; in MacOS. Then restart Ollama. Then invoke it from hosts on intranet. To enable Ollama can be visited from internet, with the help from ngrok, run: 12345678910111213$ ngrok http 11434ngrok (Ctrl+C to quit)Session Status onlineAccount Terrence Miao (Plan: Free)Version 3.17.0Region Australia (au)Web Interface http://127.0.0.1:4040Forwarding https://294b-2403-5802-1c44-0-341c-aae8-a501-8b73.ngrok-free.app -&gt; http://localhost:11434Connections ttl opn rt1 rt5 p50 p90 0 0 0.00 0.00 0.00 0.00 Then can visit Ollama on localhost from internet. 11434 is default port of Ollama. 12345678910111213141516171819202122232425262728293031$ curl --location &#x27;https://294b-2403-5802-1c44-0-341c-aae8-a501-8b73.ngrok-free.app/v1/models&#x27;&#123; &quot;object&quot;: &quot;list&quot;, &quot;data&quot;: [ &#123; &quot;id&quot;: &quot;codellama:latest&quot;, &quot;object&quot;: &quot;model&quot;, &quot;created&quot;: 1728736149, &quot;owned_by&quot;: &quot;library&quot; &#125;, &#123; &quot;id&quot;: &quot;deepseek-coder-v2:latest&quot;, &quot;object&quot;: &quot;model&quot;, &quot;created&quot;: 1728735883, &quot;owned_by&quot;: &quot;library&quot; &#125;, &#123; &quot;id&quot;: &quot;mxbai-embed-large:latest&quot;, &quot;object&quot;: &quot;model&quot;, &quot;created&quot;: 1713401203, &quot;owned_by&quot;: &quot;library&quot; &#125;, &#123; &quot;id&quot;: &quot;nomic-embed-text:latest&quot;, &quot;object&quot;: &quot;model&quot;, &quot;created&quot;: 1708780687, &quot;owned_by&quot;: &quot;library&quot; &#125; ]&#125; To write some code: 12345678910111213141516171819202122232425262728293031323334353637383940414243$ curl --location &#x27;https://294b-2403-5802-1c44-0-341c-aae8-a501-8b73.ngrok-free.app/v1/chat/completions&#x27; \\--header &#x27;Content-Type: application/json&#x27; \\--data &#x27;&#123; &quot;model&quot;: &quot;deepseek-coder-v2&quot;, &quot;messages&quot;: [ &#123; &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot; &#125;, &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write a function that outputs the fibonacci sequence in TypeScript.&quot; &#125; ]&#125;&#x27;&#123; &quot;id&quot;: &quot;chatcmpl-174&quot;, &quot;object&quot;: &quot;chat.completion&quot;, &quot;created&quot;: 1728738926, &quot;model&quot;: &quot;deepseek-coder-v2&quot;, &quot;system_fingerprint&quot;: &quot;fp_ollama&quot;, &quot;choices&quot;: [ &#123; &quot;index&quot;: 0, &quot;message&quot;: &#123; &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot; Certainly! Below is a TypeScript function that outputs the Fibonacci sequence up to a specified number of terms: This function starts with the initial terms of the Fibonacci sequence `[0, 1]` and then iteratively calculates each subsequent term by summing the two preceding terms. The function returns an array containing the first `n` terms of the Fibonacci sequence. &quot; &#125;, &quot;finish_reason&quot;: &quot;stop&quot; &#125; ], &quot;usage&quot;: &#123; &quot;prompt_tokens&quot;: 29, &quot;completion_tokens&quot;: 243, &quot;total_tokens&quot;: 272 &#125;&#125; 123456789101112131415function fibonacciSequence(n: number): number[] &#123; if (n &lt;= 0) return []; if (n === 1) return [0]; const result: number[] = [0, 1]; for (let i = 2; i &lt; n; i++) &#123; result.push(result[i - 1] + result[i - 2]); &#125; return result;&#125;// Example usage: console.log(fibonacciSequence(10));// Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] Check Ollama log: 1$ tail -f ~/.ollama/logs/server.log References Ollama FAQ, https://github.com/ollama/ollama/blob/main/docs/faq.md","tags":[]},{"title":"Proxmox VE Helper Scripts","date":"2024-09-27T12:58:54.000Z","path":"2024/09/27/Proxmox-VE-Helper-Scripts/","text":"Run Proxmox VE Post Install script: 1$ bash -c &quot;$(wget -qLO - https://github.com/tteck/Proxmox/raw/main/misc/post-pve-install.sh)&quot; References Proxmox VE Helper-Scripts, https://tteck.github.io/Proxmox Virtualizing OPNsense on Proxmox as Your Primary Router, https://www.youtube.com/watch?v=VcTGKBHcqmk","tags":[]},{"title":"Running a Mac app from Unidentified Developer on MacOS Sequoia","date":"2024-09-19T00:16:23.000Z","path":"2024/09/19/Running-a-Mac-app-from-Unidentified-Developer-on-MacOS-Sequoia/","text":"Latest MacOS Sequoia has made another security enforcement. To open and run a Mac application, e.g. VLC nightly build at https://nightlies.videolan.org/, Unidentified Developer , the following steps can take. Enable “Allow Applications from Anywhere” 1234base ~sudo spctl --master-disablePassword:Globally disabling the assessment system needs to be confirmed in System Settings. Go to System Settings -&gt; Privacy &amp; Security, select “Allow Applications from Anywhere”: 1234base ~sudo spctl --master-enablePassword:This operation is no longer supported. Please see the man page for more information. Disable MacOS Quarantine on the application In this case application VLC: 12base ~xattr -dr com.apple.quarantine /Applications/VLC.app","tags":[]},{"title":"Configure and setup Mellanox network adapter","date":"2024-09-17T01:42:01.000Z","path":"2024/09/17/Configure-and-setup-Mellanox-network-adapter/","text":"A Mellanox ConnectX-4 MCX455A-ECAT PCIe x16 3.0 100GBe VPI EDR IB network adapter. Enable VT-d(Intel Virtualization Technology for Directed I&#x2F;O), SR-IOV (Single Root IO Virtualization), and the number of Virtual Functions in Chipset and Network in BIOS. Configuration and SetupGo to NVIDIA Firmware Tools (MFT) https://network.nvidia.com/products/adapter-software/firmware-tools/ and download the MFT; go to Updating Firmware for ConnectX®-4 VPI PCI Express Adapter Cards (InfiniBand, Ethernet, VPI) https://network.nvidia.com/support/firmware/connectx4ib/ and download the updated firmware e.g. mft-4.26.1-6-x86_64-deb.tgz. After installation, start up Mellanox Software Tools service: 123456root@pve:~# mst startStarting MST (Mellanox Software Tools) driver setLoading MST PCI module - SuccessLoading MST PCI configuration module - SuccessCreate devicesUnloading MST PCI module (unused) - Success Check status: 1234567891011root@pve:~# mst statusMST modules:------------ MST PCI module is not loaded MST PCI configuration module loadedMST devices:------------/dev/mst/mt4115_pciconf0 - PCI configuration cycles access. domain:bus:dev.fn=0000:06:00.0 addr.reg=88 data.reg=92 cr_bar.gw_offset=-1 Chip revision is: 00 Query Mellanox network adapter: 1234567891011121314root@pve:~# flint -d /dev/mst/mt4115_pciconf0 queryImage type: FS3FW Version: 12.28.2006FW Release Date: 15.9.2020Product Version: 12.28.2006Rom Info: type=UEFI version=14.21.17 cpu=AMD64 type=PXE version=3.6.102 cpu=AMD64Description: UID GuidsNumberBase GUID: ec0d9a030076eae2 4Base MAC: ec0d9a76eae2 4Image VSD: N/ADevice VSD: N/APSID: LNV2180110032Security Attributes: N/A Check Mellanox network adapter configuration: 1234567891011121314root@pve:~# mlxconfig -d /dev/mst/mt4115_pciconf0 queryDevice #1:----------Device type: ConnectX4Name: 00KH925_AxDescription: Mellanox ConnectX-4 EDR IB VPI Single-port x16 PCIe 3.0 HCADevice: /dev/mst/mt4115_pciconf0Configurations: LINK_TYPE_P1 ETH(2) SRIOV_EN True(1) NUM_OF_VFS 8... Verify network adapter configuration: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748terrence@pve:~# ethtool enp1s0np0Settings for enp1s0np0: Supported ports: [ Backplane ] Supported link modes: 1000baseKX/Full 10000baseKR/Full 40000baseKR4/Full 40000baseCR4/Full 40000baseSR4/Full 40000baseLR4/Full 56000baseKR4/Full 25000baseCR/Full 25000baseKR/Full 25000baseSR/Full 50000baseCR2/Full 50000baseKR2/Full 100000baseKR4/Full 100000baseSR4/Full 100000baseCR4/Full 100000baseLR4_ER4/Full Supported pause frame use: Symmetric Supports auto-negotiation: Yes Supported FEC modes: None RS BASER Advertised link modes: 1000baseKX/Full 10000baseKR/Full 40000baseKR4/Full 40000baseCR4/Full 40000baseSR4/Full 40000baseLR4/Full 56000baseKR4/Full 25000baseCR/Full 25000baseKR/Full 25000baseSR/Full 50000baseCR2/Full 50000baseKR2/Full 100000baseKR4/Full 100000baseSR4/Full 100000baseCR4/Full 100000baseLR4_ER4/Full Advertised pause frame use: Symmetric Advertised auto-negotiation: Yes Advertised FEC modes: RS Speed: 100000Mb/s Duplex: Full Auto-negotiation: on Port: Direct Attach Copper PHYAD: 0 Transceiver: internal Link detected: yes Virtualization12345678root@pve:~# lspci | grep Mellanox06:00.0 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4]root@pve:~# ip link show...4: enp1s0np0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master vmbr2 state UP mode DEFAULT group default qlen 1000 link/ether ec:0d:9a:76:eb:2a brd ff:ff:ff:ff:ff:ff... Write to the sysfs file the number of Virtual Functions: 1root@pve:~# echo 8 &gt; /sys/class/infiniband/mlx5_0/device/sriov_numvfs Verify that the Virtual Functions were created: 123456789101112131415161718192021222324root@pve:~# lspci | grep Mellanox01:00.0 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4]01:00.1 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4 Virtual Function]01:00.2 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4 Virtual Function]01:00.3 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4 Virtual Function]01:00.4 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4 Virtual Function]01:00.5 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4 Virtual Function]01:00.6 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4 Virtual Function]01:00.7 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4 Virtual Function]01:01.0 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4 Virtual Function]root@pve:~# ip link show...4: enp1s0np0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master vmbr2 state UP mode DEFAULT group default qlen 1000 link/ether ec:0d:9a:76:eb:2a brd ff:ff:ff:ff:ff:ff vf 0 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off vf 1 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off vf 2 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off vf 3 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off vf 4 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off vf 5 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off vf 6 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off vf 7 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off... PersistencePrint out device information: 12345678root@pve:~# udevadm info /sys/class/infiniband/mlx5_0P: /devices/pci0000:00/0000:00:01.0/0000:01:00.0/infiniband/mlx5_0M: mlx5_0R: 0U: infinibandE: DEVPATH=/devices/pci0000:00/0000:00:01.0/0000:01:00.0/infiniband/mlx5_0E: SUBSYSTEM=infinibandE: NAME=mlx5_0 Make the network adapter Virtual Functions persistent after reboot: 12root@pve:~# cat /etc/udev/rules.d/mlx.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;infiniband&quot;, ENV&#123;NAME&#125;==&quot;mlx5_0&quot;, ATTR&#123;device/sriov_numvfs&#125;=&quot;8&quot; References Mellanox ConnectX-4 100Gb NIC Firmware Update and Ethernet Mode, https://www.youtube.com/watch?v=D1qN7Qg3bSg Single Root IO Virtualization (SR-IOV), https://docs.nvidia.com/networking/display/mlnxofedv53100143/single+root+io+virtualization+(sr-iov) Persisting SR-IOV virtual functions after reboot, https://www.reddit.com/r/homelab/comments/hf12mv/persisting_sriov_virtual_functions_after_reboot/ PCI Device Assignment with SR-IOV Devices, https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-pci_devices-pci_passthrough 100 Gbe &amp; TrueNAS Scale 23.10 iSCSI - Performance Unleashed, https://forum.level1techs.com/t/100-gbe-truenas-scale-23-10-iscsi-performance-unleashed/206452","tags":[]},{"title":"Create Storage Pool in Windows Server 2022","date":"2024-09-15T11:39:18.000Z","path":"2024/09/15/Create-Storage-Pool-in-Windows-Server-2022/","text":"","tags":[]},{"title":"Fix network object name already existed issue in Windows","date":"2024-09-15T11:18:03.000Z","path":"2024/09/15/Fix-network-object-name-already-existed-issue-in-Windows/","text":"When rename a network adapter in Windows: 1PS C:\\&gt; Rename-NetAdapter -Name Ethernet -NewName Mellanox an error Rename-NetAdapter : {Object Exists} An attempt was made to create an object and the object name already existed thrown. Work around solution is: Open Device Manager in Windows Control Panel Under menu View enable Show hidden devices Uninstall the old network adapter with the old name Then rename the network adapter again References Registry mismatch Get-NetAdapter, https://stackoverflow.com/questions/42220423/registry-mismatch-get-netadapter","tags":[]},{"title":"How to enable SMB Direct client/server in Windows 11 Pro for Workstations","date":"2024-09-14T07:52:08.000Z","path":"2024/09/14/How-to-enable-SMB-Direct-client-server-in-Windows-11-Pro-for-Workstations/","text":"In Windows 11 Pro Station, a Mellanox ConnectX-4 MCX455A-ECAT PCIe x16 3.0 100GBe VPI EDR IB network adatper, goes to support SMB Direct, client and server side SMB Multichannel and RDMA (Remote Direct Memory Access): Open Windows Terminal as Administrator. Enable SMB Direct: 1234PS C:\\&gt; Enable-WindowsOptionalFeature -Online -FeatureName SMBDirectPath :Online : TrueRestartNeeded : False Enable SMB Multichannel on the client-side: 12345678910PS C:\\&gt; Set-SmbClientConfiguration -EnableMultiChannel $trueConfirmAre you sure you want to perform this action?Performing operation &#x27;Modify&#x27; on Target &#x27;SMB Client Configuration&#x27;.[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is &quot;Y&quot;):PS C:\\&gt; Get-SmbClientConfiguration...EnableMultiChannel : True... Enable SMB Multichannel on the server-side: 12345678910PS C:\\&gt; Set-SmbServerConfiguration -EnableMultiChannel $trueConfirmAre you sure you want to perform this action?Performing operation &#x27;Modify&#x27; on Target &#x27;SMB Server Configuration&#x27;.[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is &quot;Y&quot;):PS C:\\&gt; Get-SmbServerConfiguration...EnableMultiChannel : True... Enable RDMA for a specific interface: 1PS C:\\&gt; Enable-NetAdapterRDMA Mellanox Verify which state of operability SMB Direct is currently configured to: 1234567PS C:\\&gt; Get-WindowsOptionalFeature -Online -FeatureName SMBDirectFeatureName : SmbDirectDisplayName : SMB DirectDescription : Remote Direct Memory Access (RDMA) support for the SMB 3.x file sharing protocolRestartRequired : PossibleState : EnabledCustomProperties : 12345678910PS C:\\&gt; Get-SmbClientNetworkInterfaceInterface Index RSS Capable RDMA Capable Speed IpAddresses Friendly Name--------------- ----------- ------------ ----- ----------- -------------22 True True 100 Gbps &#123;fe80::708:c529:1bcb:2432, 192.168.68.67&#125; MellanoxPS C:\\&gt; Get-SmbServerNetworkInterfaceScope Name Interface Index RSS Capable RDMA Capable Speed IpAddress---------- --------------- ----------- ------------ ----- ---------* 22 True True 100 Gbps fe80::708:c529:1bcb:2432* 22 True True 100 Gbps 192.168.68.67 Have a look TrueNAS disk speed benchmark, over a 100Gbps ethernet network, from Windows 11 Pro for Workstations with SMB Direct, client&#x2F;server SMB Multichannel and RDMA enabled: In Windows Server 2022, with SMB shared folder in Storage Spaces: Run Windows Powershell as Administrator user , which RDMA Capable are all True for both SMB client&#x2F;server: 1234567891011121314Windows PowerShellCopyright (C) Microsoft Corporation. All rights reserved.Install the latest PowerShell for new features and improvements! https://aka.ms/PSWindowsPS C:\\Users\\Administrator&gt; Get-SmbServerNetworkInterfaceScope Name Interface Index RSS Capable RDMA Capable Speed IpAddress---------- --------------- ----------- ------------ ----- ---------* 5 True True 100 Gbps fe80::9ee0:7f4c:5128:863b* 5 True True 100 Gbps 192.168.68.66PS C:\\Users\\Administrator&gt; Get-SmbClientNetworkInterfaceInterface Index RSS Capable RDMA Capable Speed IpAddresses Friendly Name--------------- ----------- ------------ ----- ----------- -------------5 True True 100 Gbps &#123;fe80::9ee0:7f4c:5128:863b, 192.168.68.66&#125; Mellanox Have a look Windows Server 2022 disk speed benchmark, over a 100Gbps ethernet network, from Windows 11 Pro for Workstations with SMB Direct, client&#x2F;server SMB Multichannel and RDMA enabled: References SMB Direct, https://learn.microsoft.com/en-us/windows-server/storage/file-server/smb-direct 100 Gbe &amp; TrueNAS Scale 23.10 iSCSI - Performance Unleashed, https://forum.level1techs.com/t/100-gbe-truenas-scale-23-10-iscsi-performance-unleashed/206452","tags":[]},{"title":"Fixing TrueNAS SMB IP binding","date":"2024-09-14T02:37:53.000Z","path":"2024/09/14/Fixing-TrueNAS-SMB-IP-binding/","text":"After changing network settings, TrueNAS IP address has been updated. If modify SMB configuration, error like: 1smb_update.bindip.0: IP address [192.168.0.51] is not a configured address for this server thrown. To reset and clean up already bind IP, login TrueNAS Console and run: 12345678910111213141516171819202122232425root@TrueNAS[~]# midclt call smb.update &#x27;&#123;&quot;bindip&quot;: []&#125;&#x27;&#123; &quot;id&quot;: 1, &quot;netbiosname&quot;: &quot;TRUENAS&quot;, &quot;netbiosalias&quot;: [], &quot;workgroup&quot;: &quot;IGLOO STUDIO&quot;, &quot;description&quot;: &quot;TrueNAS Server&quot;, &quot;unixcharset&quot;: &quot;UTF-8&quot;, &quot;loglevel&quot;: &quot;MINIMUM&quot;, &quot;syslog&quot;: false, &quot;aapl_extensions&quot;: true, &quot;Localmaster&quot;: true, &quot;guest&quot;: &quot;nobody&quot;, &quot;filemask&quot;&#x27;: &quot;&quot;, &quot;dirmask&quot;&#x27;: &quot;&quot;, &quot;smb_options&quot;: &quot;&quot;, &quot;bindip&quot;: [], &quot;cifs_SID&quot;: &quot;S-1-5-21-2487580926-3122677641-100607549&quot;, &quot;ntImv1_auth&quot;: false, &quot;enable_smb1&quot;: false, &quot;admin_group&quot;: null, &quot;next_rid&quot;: 0, &quot;multichannel&quot;: true, &quot;netbiosname_local&quot;: &quot;TRUENAS&quot;&#125; then can modify SMB configuration successfully. References Help with SMB IP binding, https://www.reddit.com/r/freenas/comments/fovpfz/help_with_smb_ip_binding","tags":[]},{"title":"Running MacOS on Proxmox","date":"2024-09-08T01:34:16.000Z","path":"2024/09/08/Running-MacOS-on-Proxmox/","text":"Following the instructions to run the latest MacOS, Sonoma 14.6.1, on Proxmox - The Definitive Guide to Running MacOS in Proxmox. A few more put on notes: You can login with Apple ID Modify config.plist file under EFI folder -&gt; OC folder, change the default Screen Resolution from 1080P 1920x1080@32 to 2K 2560x1440@32. Look more information in Configuration.pdf file in KVM-Opencore release Connect with Apple Remote Desktop, so can use exact same Apple Keyboard mapping: Current KVM-Opencore hasn’t setup support for Audio device and GPU accelerator. Also try to setup Parsec in MacOS on Proxmox, without success.","tags":[]},{"title":"Setup MikroTik CRS504-4XQ-IN and run a speed test","date":"2024-09-02T04:17:37.000Z","path":"2024/09/02/Setup-MikroTik-CRS504-4XQ-IN-and-run-a-speed-test/","text":"MikroTik CRS504-4XQ-IN, the Cloud Switch can handle FOUR QSFP28 100Gbps ports, equal to 16 x 25Gbps bandwidth. Setup single link mode, only the first QSFP28 sub-interface needs to be configured, while the remaining three sub-interfaces should remain enabled. For example, connect Mellanox MCX455A-ECAT ConnectX-4 InfiniBand/Ethernet adapter card (EDR IB 100Gbps and 100GbE, single-port QSFP28, PCIe 3.0x16) using ONTi DAC QSFP28 100Gbps cable to the switch. Change FEC Mode to fec91. Ethernet Forward Error Correction (FEC) is a technique used to improve the reliability of data transmission over Ethernet networks by detecting and correcting errors in data packets. The two most common types of FEC used in Ethernet networks are CL74 and CL91. CL74 and CL91 refer to two different types of FEC codes, each with its own characteristics and performance. Here’s a brief comparison between the two: Code Rate: CL91 has a higher code rate of 91.6%, which means that only 8.4% of the data transmitted is used for error correction. In addition, setup the swith port connected to ONTi QSFP28 40Gbps TO 4SFP+ breakout cable: 123456$ ssh -l admin MikroTik.local[admin@MikroTik] &gt; /interface ethernet set qsfp28-1-1 auto-negotiation=no speed=10G-baseCR[admin@MikroTik] &gt; /interface ethernet set qsfp28-1-2 auto-negotiation=no speed=10G-baseCR[admin@MikroTik] &gt; /interface ethernet set qsfp28-1-3 auto-negotiation=no speed=10G-baseCR[admin@MikroTik] &gt; /interface ethernet set qsfp28-1-4 auto-negotiation=no speed=10G-baseCR Speed testIn iperf3 server, run listens to 4 ports to manage connections in parallel: 1$ iperf3 -s -p 5201 &amp; iperf3 -s -p 5202 &amp; iperf3 -s -p 5203 &amp; iperf3 -s -p 5204 &amp; In a MacBook Pro with WiFi-6 connection, run: 1$ iperf3 -c MikroTik.local -p 5201 -P 4 -t 1000 In a Mac Studio with 10Gbps Ethernet connection, run: 1$ iperf3 -c MikroTik.local -p 5202 -P 8 -t 1000 -B 192.168.0.104 In a Windows 11 PC with 100Gbps Ethernet connection, run: 1$ iperf3 -c MikroTik.local -p 5203 -P 2 -t 1000 Check the speed on switch console: and in graph: References MikroTik wired interface compatibility, https://help.mikrotik.com/docs/display/ROS/MikroTik+wired+interface+compatibility MikroTik RouterOS, https://help.mikrotik.com/docs/display/ROS/RouterOS MikroTik CRS504-4XQ-IN Quick Guide, https://help.mikrotik.com/docs/display/QG/Quick+Guide+-+CRS504-4XQ-IN FEC (Forward Error Correction), https://www.reddit.com/r/fortinet/comments/127z3wd/fec_forward_error_correction/","tags":[]},{"title":"Setup network bond in Synology NAS","date":"2024-08-11T05:45:25.000Z","path":"2024/08/11/Setup-network-bond-in-Synology-NAS/","text":"Synology NAS DS920+ with two build-in 1Gbps ethernet adapters, and two 2.5Gbps USB 3.0 Ethernet Adapters. Now setup network bond &#x2F; link aggregation on them. Enable Network Link Aggregation Mode Pickup network devices into bond Setup network Accept network interface change after network bond Network bonded Network bond service order","tags":[]},{"title":"Setup TrueNAS Network and Link Aggregation","date":"2024-08-08T15:03:40.000Z","path":"2024/08/09/Setup-TrueNAS-Network-and-Link-Aggregation/","text":"Two network adapters have been setup with DHCP allocated addresses: ens18 192.168.0.246 ens19 192.168.0.105 Create a Network Link Aggregation Interface After Link Aggregation Interface bond1 created, original two network adapters ens18 and ens19 IP addresses are gone. bond1 with the ONLY ONE network interface address for TrueNAS.","tags":[]},{"title":"Samba setup and share in TrueNAS","date":"2024-08-07T12:19:00.000Z","path":"2024/08/07/Samba-setup-and-share-in-TrueNAS/","text":"TrueNAS installed and a few steps to setup to let users access shared Samba directories. Enable SMB Samba service in TrueNAS Create an user account with home directory and all access permisssions Share TrueNAS pool created Then can access shared directory from TrunNAS in both Windows and Mac: Make sure there is NO a lock icon on the folder (can unlock the folder in MacOS Finder)","tags":[]},{"title":"Increase, decrease and resize Proxmox disk volume","date":"2024-08-07T03:29:22.000Z","path":"2024/08/07/Increase-decrease-and-resize-Proxmox-disk-volume/","text":"After a default installation, a tiny disk space given to Proxmox root volume. Insufficient disk space issue raised up after several VMs installed and backup made, as iinstallation image files *.ISO and backup all put into root volume. In storage.cfg: 123456789root@pve:~# cat /etc/pve/storage.cfgdir: local path /var/lib/vz content iso,vztmpl,backuplvmthin: local-lvm thinpool data vgname pve content rootdir,images Run lvdisplay: 12345678910111213141516171819202122root@pve:~# lvdisplay --- Logical volume --- LV Name data VG Name pve # open 0 LV Size &lt;3.58 TiB --- Logical volume --- LV Path /dev/pve/swap LV Name swap VG Name pve LV Status available # open 2 LV Size 8.00 GiB --- Logical volume --- LV Path /dev/pve/root LV Name root VG Name pve LV Status available # open 1 LV Size &lt;112.25 GiB Solution is to decrease the size of pve/data volume, as this volume doesn’t support reducing thin pools in size yet, and then to increase the size of pdev/root volume. Backup all VMs, then remove pve/data volume: 12root@pve:~# lvremove pve/dataRemoving pool pve/data will remove 7 dependent volume(s). Proceed? [y/n]: y Based on the disk space has just released, increase the size of pdev/root volume, 20% for current FREE space in this case: 123456789root@pve:~# lvextend -l +20%FREE /dev/pve/root Size of logical volume pve/root changed from &lt;112.25 GiB (28735 extents) to &lt;851.09 GiB (217878 extents). Logical volume pve/root successfully resized.root@pve:~# resize2fs /dev/pve/rootresize2fs 1.47.0 (5-Feb-2023)Filesystem at /dev/pve/root is mounted on /; on-line resizing requiredold_desc_blocks = 15, new_desc_blocks = 107The filesystem on /dev/pve/root is now 223107072 (4k) blocks long. Create a new pve/data volume: 12root@pve:~# lvcreate -L2920G -ndata pve Logical volume &quot;data&quot; created. Check free disk space: 123456789101112131415161718192021root@pve:~# vgdisplay --- Volume group --- VG Name pve System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 126 VG Access read/write VG Status resizable MAX LV 0 Cur LV 3 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size &lt;3.73 TiB PE Size 4.00 MiB Total PE 976498 Alloc PE / Size 967446 / 3.69 TiB Free PE / Size 9052 / &lt;35.36 GiB VG UUID UEsIZR-TBsz-UYlP-u2FO-2AbC-uq4d-vcb35f Create thin pool volume of the metadata, usually size of 1% of pve/data volume: 1234567root@pve:~# lvconvert --type thin-pool --poolmetadatasize 36G pve/data Reducing pool metadata size 36.00 GiB to maximum usable size &lt;15.88 GiB. Thin pool volume with chunk size 64.00 KiB can address at most &lt;15.88 TiB of data. WARNING: Converting pve/data to thin pool&#x27;s data volume with metadata wiping. THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)Do you really want to convert pve/data? [y/n]: y Converted pve/data to thin pool. Verify current disk volumes, pve/root disk volume has more space now: 1234567891011root@pve:~# df -hFilesystem Size Used Avail Use% Mounted onudev 16G 0 16G 0% /devtmpfs 3.2G 1.8M 3.2G 1% /run/dev/mapper/pve-root 841G 71G 736G 9% /tmpfs 16G 46M 16G 1% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/lockefivarfs 192K 114K 74K 61% /sys/firmware/efi/efivars/dev/nvme3n1p2 1022M 12M 1011M 2% /boot/efi/dev/fuse 128M 20K 128M 1% /etc/pvetmpfs 3.2G 0 3.2G 0% /run/user/0 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152root@pve:~# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTSsda 8:0 0 223.6G 0 disk├─sda1 8:1 0 100M 0 part├─sda2 8:2 0 16M 0 part├─sda3 8:3 0 120.3G 0 part├─sda4 8:4 0 625M 0 part└─sda5 8:5 0 102.5G 0 partnvme2n1 259:0 0 3.6T 0 disknvme4n1 259:1 0 3.6T 0 disknvme0n1 259:2 0 3.6T 0 disknvme1n1 259:3 0 3.6T 0 disknvme5n1 259:4 0 3.6T 0 disknvme3n1 259:5 0 3.7T 0 disk├─nvme3n1p1 259:6 0 1007K 0 part├─nvme3n1p2 259:7 0 1G 0 part /boot/efi└─nvme3n1p3 259:8 0 3.7T 0 part ├─pve-swap 252:0 0 8G 0 lvm [SWAP] ├─pve-root 252:1 0 854.7G 0 lvm / ├─pve-data_tmeta 252:2 0 15.9G 0 lvm │ └─pve-data-tpool 252:4 0 2.9T 0 lvm │ ├─pve-data 252:5 0 2.9T 1 lvm │ ├─pve-vm--100--disk--0 252:6 0 240G 0 lvm │ ├─pve-vm--101--disk--0 252:7 0 4M 0 lvm │ ├─pve-vm--101--disk--1 252:8 0 240G 0 lvm │ ├─pve-vm--101--disk--2 252:9 0 4M 0 lvm │ ├─pve-vm--102--disk--0 252:10 0 4M 0 lvm │ ├─pve-vm--102--disk--1 252:11 0 240G 0 lvm │ └─pve-vm--102--disk--2 252:12 0 4M 0 lvm └─pve-data_tdata 252:3 0 2.9T 0 lvm └─pve-data-tpool 252:4 0 2.9T 0 lvm ├─pve-data 252:5 0 2.9T 1 lvm ├─pve-vm--100--disk--0 252:6 0 240G 0 lvm ├─pve-vm--101--disk--0 252:7 0 4M 0 lvm ├─pve-vm--101--disk--1 252:8 0 240G 0 lvm ├─pve-vm--101--disk--2 252:9 0 4M 0 lvm ├─pve-vm--102--disk--0 252:10 0 4M 0 lvm ├─pve-vm--102--disk--1 252:11 0 240G 0 lvm └─pve-vm--102--disk--2 252:12 0 4M 0 lvmroot@pve:~# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert data pve twi-aotz-- 2.85t 3.19 0.32 root pve -wi-ao---- &lt;854.69g swap pve -wi-ao---- 8.00g vm-100-disk-0 pve Vwi-a-tz-- 240.00g data 10.38 vm-101-disk-0 pve Vwi-a-tz-- 4.00m data 14.06 vm-101-disk-1 pve Vwi-a-tz-- 240.00g data 19.50 vm-101-disk-2 pve Vwi-a-tz-- 4.00m data 1.56 vm-102-disk-0 pve Vwi-a-tz-- 4.00m data 14.06 vm-102-disk-1 pve Vwi-a-tz-- 240.00g data 8.98 vm-102-disk-2 pve Vwi-a-tz-- 4.00m data 1.56 References Reduce size of local-lvm https://forum.proxmox.com/threads/reduce-size-of-local-lvm.78676/","tags":[]},{"title":"Proxmox and SR-IOV support for Intel GPU and Mellanox network adapter","date":"2024-08-04T01:56:12.000Z","path":"2024/08/04/Proxmox-and-SR-IOV-support-for-Intel-GPU-and-Mellanox-network-adapter/","text":"Enable VT-d(Intel Virtualization Technology for Directed I&#x2F;O), for IOMMU(Input Output Memory Management Unit) services, and SR-IOV (Single Root IO Virtualization), a technology that allows a physical PCIe device to present itself multiple times through the PCIe bus, in motherboard BIOS in Chipset, e.g. ASRock Z790 Riptide WiFi. Enable SR-IOV for Mellonax network adapter e.g. Mellanox ConnectX-4 MCX455A-ECAT PCIe x16 3.0 100GBe VPI EDR IB in the same motherboard BIOS. Add Proxmox No Subscription URL: 1234567891011root@pve:~# cat /etc/apt/sources.listdeb http://ftp.au.debian.org/debian bookworm main contribdeb http://ftp.au.debian.org/debian bookworm-updates main contrib# security updatesdeb http://security.debian.org bookworm-security main contrib# Proxmox VE pve-no-subscription repository provided by proxmox.com,# NOT recommended for production usedeb http://download.proxmox.com/debian/pve bookworm pve-no-subscription and run packages update: 1root@pve:~# apt update and install all build tools: 1root@pve:~# apt install build-* dkms Set&#x2F;Pin Proxmox kernel version: 12345678root@pve:~# proxmox-boot-tool kernel pin 6.8.4-2-pveSetting &#x27;6.8.4-2-pve&#x27; as grub default entry and running update-grub.Generating grub configuration file ...Found linux image: /boot/vmlinuz-6.8.4-2-pveFound initrd image: /boot/initrd.img-6.8.4-2-pveFound memtest86+ 64bit EFI image: /boot/memtest86+x64.efiAdding boot menu entry for UEFI Firmware Settings ...done and verify Proxmox kernal version: 123456789root@pve:~# proxmox-boot-tool kernel listManually selected kernels:None.Automatically selected kernels:6.8.4-2-pvePinned kernel:6.8.4-2-pve Install Proxmox kernel headers source code package: 12345678910111213141516root@pve:~# apt install proxmox-headers-6.8.4-2-pveReading package lists... DoneBuilding dependency tree... DoneReading state information... DoneThe following NEW packages will be installed: proxmox-headers-6.8.4-2-pve0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.Need to get 13.7 MB of archives.After this operation, 97.0 MB of additional disk space will be used.Get:1 http://download.proxmox.com/debian/pve bookworm/pve-no-subscription amd64 proxmox-headers-6.8.4-2-pve amd64 6.8.4-2 [13.7 MB]Fetched 13.7 MB in 1s (23.8 MB/s)Selecting previously unselected package proxmox-headers-6.8.4-2-pve.(Reading database ... 70448 files and directories currently installed.)Preparing to unpack .../proxmox-headers-6.8.4-2-pve_6.8.4-2_amd64.deb ...Unpacking proxmox-headers-6.8.4-2-pve (6.8.4-2) ...Setting up proxmox-headers-6.8.4-2-pve (6.8.4-2) ... Download Linux i915 driver with SR-IOV support for Linux kernel: 1root@pve:~# git clone https://github.com/strongtz/i915-sriov-dkms and change into the cloned repository and run: 12345root@pve:~/i915-sriov-dkms# cat VERSION2024.07.24root@pve:~/i915-sriov-dkms# dkms add .Creating symlink /var/lib/dkms/i915-sriov-dkms/2024.07.24/source -&gt; /usr/src/i915-sriov-dkms-2024.07.24 and build, install i915-sriov-dkms Linux kernel module: 123456789101112131415161718root@pve:~/i915-sriov-dkms# dkms install -m i915-sriov-dkms -v $(cat VERSION) --forceSign command: /lib/modules/6.8.4-2-pve/build/scripts/sign-fileSigning key: /var/lib/dkms/mok.keyPublic certificate (MOK): /var/lib/dkms/mok.pubCertificate or key are missing, generating self signed certificate for MOK...Building module:Cleaning build area...make -j20 KERNELRELEASE=6.8.4-2-pve -C /lib/modules/6.8.4-2-pve/build M=/var/lib/dkms/i915-sriov-dkms/2024.07.24/build.......Signing module /var/lib/dkms/i915-sriov-dkms/2024.07.24/build/i915.koCleaning build area...i915.ko:Running module version sanity check. - Original module - Installation - Installing to /lib/modules/6.8.4-2-pve/updates/dkms/depmod... and enable i915-sriov-dkms module with upto maximum 7 VFS (Virtual File System) in Linux kernel: 12root@pve:~# cat /etc/default/grub | grep GRUB_CMDLINE_LINUX_DEFAULTGRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet intel_iommu=on i915.enable_guc=3 i915.max_vfs=7&quot; Reboot Proxmox: 123456789root@pve:~# lspci | grep VGA00:02.0 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770] (rev 04)00:02.1 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770] (rev 04)00:02.2 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770] (rev 04)00:02.3 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770] (rev 04)00:02.4 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770] (rev 04)00:02.5 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770] (rev 04)00:02.6 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770] (rev 04)00:02.7 VGA compatible controller: Intel Corporation Raptor Lake-S GT1 [UHD Graphics 770] (rev 04) First VGA 00:02.0 is the REAL GPU. Other 7 are Virtual ones. Verify IOMMU has been enabled: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061root@pve:~# dmesg | grep -i iommu[ 0.000000] Command line: BOOT_IMAGE=/boot/vmlinuz-6.8.4-2-pve root=/dev/mapper/pve-root ro quiet intel_iommu=on i915.enable_guc=3 i915.max_vfs=7[ 0.036486] Kernel command line: BOOT_IMAGE=/boot/vmlinuz-6.8.4-2-pve root=/dev/mapper/pve-root ro quiet intel_iommu=on i915.enable_guc=3 i915.max_vfs=7[ 0.036515] DMAR: IOMMU enabled[ 0.090320] DMAR-IR: IOAPIC id 2 under DRHD base 0xfed91000 IOMMU 1[ 0.244292] pci 0000:00:02.0: DMAR: Skip IOMMU disabling for graphics[ 0.270125] iommu: Default domain type: Translated[ 0.270125] iommu: DMA domain TLB invalidation policy: lazy mode[ 0.303735] DMAR: IOMMU feature fl1gp_support inconsistent[ 0.303735] DMAR: IOMMU feature pgsel_inv inconsistent[ 0.303736] DMAR: IOMMU feature nwfs inconsistent[ 0.303736] DMAR: IOMMU feature dit inconsistent[ 0.303737] DMAR: IOMMU feature sc_support inconsistent[ 0.303737] DMAR: IOMMU feature dev_iotlb_support inconsistent[ 0.304175] pci 0000:00:02.0: Adding to iommu group 0[ 0.304544] pci 0000:00:00.0: Adding to iommu group 1[ 0.304554] pci 0000:00:01.0: Adding to iommu group 2[ 0.304562] pci 0000:00:01.1: Adding to iommu group 3[ 0.304570] pci 0000:00:06.0: Adding to iommu group 4[ 0.304582] pci 0000:00:14.0: Adding to iommu group 5[ 0.304589] pci 0000:00:14.2: Adding to iommu group 5[ 0.304598] pci 0000:00:15.0: Adding to iommu group 6[ 0.304607] pci 0000:00:16.0: Adding to iommu group 7[ 0.304614] pci 0000:00:17.0: Adding to iommu group 8[ 0.304630] pci 0000:00:1a.0: Adding to iommu group 9[ 0.304641] pci 0000:00:1b.0: Adding to iommu group 10[ 0.304651] pci 0000:00:1c.0: Adding to iommu group 11[ 0.304662] pci 0000:00:1c.1: Adding to iommu group 12[ 0.304671] pci 0000:00:1c.2: Adding to iommu group 13[ 0.304681] pci 0000:00:1c.3: Adding to iommu group 14[ 0.304692] pci 0000:00:1c.4: Adding to iommu group 15[ 0.304703] pci 0000:00:1d.0: Adding to iommu group 16[ 0.304721] pci 0000:00:1f.0: Adding to iommu group 17[ 0.304728] pci 0000:00:1f.3: Adding to iommu group 17[ 0.304735] pci 0000:00:1f.4: Adding to iommu group 17[ 0.304742] pci 0000:00:1f.5: Adding to iommu group 17[ 0.304750] pci 0000:01:00.0: Adding to iommu group 18[ 0.304758] pci 0000:02:00.0: Adding to iommu group 19[ 0.304765] pci 0000:03:00.0: Adding to iommu group 20[ 0.304781] pci 0000:04:00.0: Adding to iommu group 21[ 0.304791] pci 0000:05:00.0: Adding to iommu group 22[ 0.304801] pci 0000:06:00.0: Adding to iommu group 23[ 0.304810] pci 0000:07:00.0: Adding to iommu group 24[ 0.304834] pci 0000:08:00.0: Adding to iommu group 25[ 0.304845] pci 0000:09:00.0: Adding to iommu group 26[ 0.304857] pci 0000:0a:00.0: Adding to iommu group 27[ 0.304866] pci 0000:0b:00.0: Adding to iommu group 28[ 4.659395] pci 0000:00:02.1: DMAR: Skip IOMMU disabling for graphics[ 4.659438] pci 0000:00:02.1: Adding to iommu group 29[ 4.664441] pci 0000:00:02.2: DMAR: Skip IOMMU disabling for graphics[ 4.664479] pci 0000:00:02.2: Adding to iommu group 30[ 4.667692] pci 0000:00:02.3: DMAR: Skip IOMMU disabling for graphics[ 4.667727] pci 0000:00:02.3: Adding to iommu group 31[ 4.671096] pci 0000:00:02.4: DMAR: Skip IOMMU disabling for graphics[ 4.671129] pci 0000:00:02.4: Adding to iommu group 32[ 4.673545] pci 0000:00:02.5: DMAR: Skip IOMMU disabling for graphics[ 4.673572] pci 0000:00:02.5: Adding to iommu group 33[ 4.676357] pci 0000:00:02.6: DMAR: Skip IOMMU disabling for graphics[ 4.676402] pci 0000:00:02.6: Adding to iommu group 34[ 4.679192] pci 0000:00:02.7: DMAR: Skip IOMMU disabling for graphics[ 4.679221] pci 0000:00:02.7: Adding to iommu group 35 Enable Virtual GPU for Windows 11 VM in Proxmox Add a PCI device for Windows 11 VM, and choose one Virtual GPU: Enable Primary GPU and PCI Express in options: Choose none in Display and host in Processors options for Windows 11 VM: Start Windows 11 VM, login with Microsoft Remote Desktop https://apps.microsoft.com/detail/9wzdncrfj3ps and a Virtual GPU is available now. Run Task Manager and check CPU and GPU load: Enable Virtual GPU for Ubuntu VM in Proxmox Ubuntu version 22.04.4 for desktop. Add a PCI device for Ubuntu VM, and choose one Virtual GPU; Enable Primary GPU and PCI Express in options; Choose none in Display and host in Processors options: Setup remote desktop connection to Ubuntu: 12345root@nucleus:~# apt install ubuntu-desktoproot@nucleus:~# apt install xrdproot@nucleus:~# systemctl enable xrdp Fix Remote Desktop audio over HDMI issue with the script, enable the sound redirection: 1terrence@nucleus:~$ ./xrdp-installer-1.5.1.sh -s then reboot VM. Now Audio device becomes xrdp input &#x2F; output. Windows Server 2022 Windows Server 2022 is similar to Windows 11 setup in Proxmox. A few issues like GPU: just disable GPU then enable it, it will work correctly. And no sound after installation, but can enable Windows Audio Service and choose Remote Audio: then audio over HDMI to remote desktop can work. In addition, can setup User Auto Logon after Windows Server 2022 startup. And check Windows license by running: 1PS C:\\Users\\Administrator&gt; slmgr -dlv Now can remote desktop access Ubuntu, Windows 11 and Windows Server 2022 VMs both run in Proxmox: References 最新保姆级 PVE 8 安装教程！虚拟机 PCIE 设备及 SR-IOV 核显直通，最多分配7个虚拟化单独核显！最强虚拟机！https://v2rayssr.com/pve.html PVE 8.1 下部署 Intel 集显虚拟化驱动 https://zoe.red/2023/38.html 定制 OVMF 固件实现 PVE 8 环境 UEFI 模式直通 Intel 核显并显示 BIOS 适用于 intel 6-13 代处理器 PVE 8 核显直通画面 Windows 10 and MacOS 系统画面显示教程，可以外接显示器 https://imacos.top/2023/10/09/152/ PVE 下如何启用 PCI 直通显卡 GPU&#x2F;iGPU&#x2F;USB&#x2F;声卡 AUDIO 等硬件直通教程 https://imacos.top/2023/07/31/pci/ Enable &amp; Using vGPU Passthrough https://gist.github.com/scyto/e4e3de35ee23fdb4ae5d5a3b85c16ed3 Proxmox VE: Passthrough with Intel Integrated Graphics Card Alder Lake Architecture | vGPU, VT-d, SR-IOV https://github.com/kamilllooo/Proxmox Remote Desktop connection from Mac to Ubuntu https://askubuntu.com/questions/893831/remote-desktop-connection-from-mac-to-ubuntu xRDP – Easy install xRDP on Ubuntu 20.04,22.04,23.XX,24.04 (Script Version 1.5.1) https://c-nergy.be/blog/?p=19814 Autologon User at Startup in Windows Server https://jc-lan.org/2022/06/02/autologon-user-at-startup-in-windows-server/ Windows Server 2022 audio available but not playing https://answers.microsoft.com/en-us/windowserver/forum/all/windows-server-2022-audio-available-but-not/eca821f1-7b4a-4293-8273-fda49dd0e9d4","tags":[]},{"title":"The solution making old Intel 10Gbps network adapter work in Windows 11","date":"2024-05-20T12:27:09.000Z","path":"2024/05/20/The-solution-making-old-Intel-10Gbps-network-adapter-work-in-Windows-11/","text":"Buy some old Intel 10Gbps network adapter, X520, X540 … from AliExpress https://aliexpress.com/, and install old Intel network adapter driver for Windows 10 and make it working in Windows 11. The example is install version 25.0 Intel network adapter driver, https://www.intel.com/content/www/us/en/download/18293/29648/intel-network-adapter-driver-for-windows-10.html, to get it to work in Windows 11: Intel network adapters supported Operating Systems, https://www.intel.com/content/www/us/en/support/articles/000025890/ethernet-products.html Buy dual ports 10Gbps network adapter Install the network adapter in PCIE slot directly to the CPU Enable SMB Multichannel Enable Jumbo Frames","tags":[]},{"title":"How to enable SMB Multichannel in Windows 11","date":"2024-05-17T12:38:27.000Z","path":"2024/05/17/How-to-enable-SMB-Multichannel-in-Windows-11/","text":"Network adapter requires to support RSS (Receive Side Scaling). Open PowerShell as administrator in Windows 11, run and enable SMB Multichannel (should be enabled by default): 123456PS C:\\&gt; Set-SmbClientConfiguration -EnableMultiChannel $trueConfirmAre you sure you want to perform this action?Performing operation &#x27;Modify&#x27; on Target &#x27;SMB Client Configuration&#x27;.[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is &quot;Y&quot;): Check network interfaces which show “RSS capable &#x3D; True“: 12345678PS C:\\&gt; Get-SmbClientNetworkInterfaceInterface Index RSS Capable RDMA Capable Speed IpAddresses Friendly Name--------------- ----------- ------------ ----- ----------- -------------17 True False 20 Gbps &#123;&#125; X710-1-WFP Native MAC Layer LightWeight Filter-00008 False False 10 Gbps &#123;&#125; X710-113 False False 10 Gbps &#123;&#125; X710-226 True False 20 Gbps &#123;fe80::923a:90de:dedd:ef44, 192.168.0.98&#125; NIC-Team Verify there are any active SMB connections: 12345PS C:\\&gt; Get-SmbConnectionServerName ShareName UserName Credential Dialect NumOpens---------- --------- -------- ---------- ------- --------Synology NAS Drive RIPTIDE\\terrence MicrosoftAccount\\terrence.miao@mail.net 3.1.1 2 Copy a large file to a SMB device, e.g., Synology NAS which also has SMB Multichannel enabled, then verify the SMB Multichannel is working: 1234567PS C:\\&gt; Get-SmbMultichannelConnection -IncludeNotSelectedServer Name Selected Client IP Server IP Client Interface Index Server Interface Index Client RSS Capable Client RDMA Capable----------- -------- --------- --------- ---------------------- ---------------------- ------------------ -------------------Synology True 192.168.0.98 192.168.0.112 26 5 False FalseSynology False 192.168.0.98 192.168.0.34 26 4 False FalseSynology False 192.168.0.98 192.168.196.140 26 7 False False 192.168.0.98 is Windows 11 network address, after Network Teaming; 192.168.0.112 and 192.168.0.34 are Synology NAS network addresses. References HOW TO: Enable SMB Multichannel for &gt; 125 MB between Synology and Win 10, https://www.reddit.com/r/synology/comments/rrb4r1/how_to_enable_smb_multichannel_for_125_mb_between/ What is SMB3 Multichannel and how is it different from Link Aggregation?, https://kb.synology.com/en-global/DSM/tutorial/smb3_multichannel_link_aggregation SMB Multichannel available for testing!, https://community.synology.com/enu/forum/1/post/157265","tags":[]},{"title":"How to upgrade Synology NAS network from 1Gbps to 2.5Gbps","date":"2024-05-17T10:25:45.000Z","path":"2024/05/17/How-to-upgrade-Synology-NAS-network-from-1Gbps-to-2-5Gbps/","text":"Synology NAS DS920+ with two 1Gbps ethernet adapters. There is an affordable and easy upgrading its gigabytes network path to 2.5Gbps. Login Synology NAS Admin UI and run Control Panel -&gt; Network -&gt; Network Interface Get a USB 3.0 Ethernet Adapter 2.5Gbps with Realtek RTL8156 &#x2F; RTL8156B &#x2F; RTL8156BG chipset, e.g., UGREEN 2.5Gbps USB-C Ethernet Adapter: Find out the architecture name of CPU in NAS. For example, Synology DS920+ is equipped with Intel Celeron J4125 CPU. The architecture name of this processor is Geminilake. Go to driver releases site https://github.com/bb-qq/r8152/releases and download the latest version e.g. r8152-geminilake-2.17.1-1_7.2.spk, Synology DSM 7.2 and above, use packages with the suffix _7.2. Login Synology Admin UI, then go to Package Center -&gt; Manual Install and choose a driver package downloaded from above step. The installation will fail at the very first time. Then ssh into the NAS, and run the following command: 1$ sudo install -m 4755 -o root -D /var/packages/r8152/target/r8152/spk_su /opt/sbin/spk_su and also enable multiple identical USB devices, which SAME products have the SAME serial number: 1$ sudo bash /var/packages/r8152/scripts/install-udev-rules 1234$ sudo bash /var/packages/r8152/scripts/install-udev-rulesUpdating Hardware Database Index...UDEV rules have been installed to /usr/lib/udev/rules.dlrwxrwxrwx 1 root root 50 May 24 17:13 /usr/lib/udev/rules.d/51-usb-r8152-net.rules -&gt; /var/packages/r8152/scripts/51-usb-r8152-net.rules and continue &#x2F; retry the installation . Reboot NAS. Login Synology Admin UI, Package Center -&gt; Installed -&gt; RTL8152&#x2F;RTL8153 driver and check new installed Realtek network adapter driver is running: Control Panel -&gt; Network -&gt; Network Interface and check the new network interface LAN 3 and Lan 4 have been turned on, with MTU &#x2F; jumbo frame enabled 9000: Bind the USB network adapter and run iperf3 network performance test: 1234567891011121314151617181920$ iperf3 -c 192.168.0.244 -B 192.168.0.229Connecting to host 192.168.0.244, port 5201[ 5] local 192.168.0.229 port 46171 connected to 192.168.0.244 port 5201[ ID] Interval Transfer Bitrate Retr Cwnd[ 5] 0.00-1.00 sec 281 MBytes 2.36 Gbits/sec 0 450 KBytes[ 5] 1.00-2.00 sec 281 MBytes 2.35 Gbits/sec 0 450 KBytes[ 5] 2.00-3.00 sec 280 MBytes 2.35 Gbits/sec 0 450 KBytes[ 5] 3.00-4.00 sec 281 MBytes 2.35 Gbits/sec 0 450 KBytes[ 5] 4.00-5.00 sec 281 MBytes 2.35 Gbits/sec 0 450 KBytes[ 5] 5.00-6.00 sec 281 MBytes 2.35 Gbits/sec 0 450 KBytes[ 5] 6.00-7.00 sec 281 MBytes 2.35 Gbits/sec 0 450 KBytes[ 5] 7.00-8.00 sec 280 MBytes 2.35 Gbits/sec 0 450 KBytes[ 5] 8.00-9.00 sec 281 MBytes 2.36 Gbits/sec 0 450 KBytes[ 5] 9.00-10.00 sec 281 MBytes 2.36 Gbits/sec 0 670 KBytes- - - - - - - - - - - - - - - - - - - - - - - - -[ ID] Interval Transfer Bitrate Retr[ 5] 0.00-10.00 sec 2.74 GBytes 2.35 Gbits/sec 0 sender[ 5] 0.00-10.05 sec 2.74 GBytes 2.34 Gbits/sec receiveriperf Done. References DSM driver for realtek RTL8152&#x2F;RTL8153&#x2F;RTL8156 based USB Ethernet adapters, https://github.com/bb-qq/r8152 Hot plugging does not work, https://github.com/bb-qq/r8152/wiki/Troubleshooting#hot-plugging-does-not-work Multiple identical devices do not work, https://github.com/bb-qq/r8152/wiki/Troubleshooting#multiple-identical-devices-do-not-work","tags":[]},{"title":"How to team network (link aggregation) in Windows 11","date":"2024-05-03T11:40:36.000Z","path":"2024/05/03/How-to-team-network-link-aggregation-in-Windows-11/","text":"Intel Ethernet Converged Network Adapter X710, with two 10Gbps ports. This allows to team the two ports together for link aggregation. Install Optional Features Server Manager in Windows 11 Open and run Windows Powershell as administrator, then run: 1PS C:\\&gt; New-NetSwitchTeam -Name &quot;NIC-Team&quot; -TeamMembers &quot;X710-1&quot;,&quot;X710-2&quot; A new network interface created, with combined speed 20Gbps. To remove network team, run: 1PS C:\\&gt; Remove-NetSwitchTeam -Name &quot;NIC-Team&quot;","tags":[]},{"title":"Step by step root OnePlus 5T","date":"2024-04-26T11:29:42.000Z","path":"2024/04/26/Step-by-step-root-OnePlus-5T/","text":"OnePlus 5T, first announced in Nov 2017. 7 years later, has been upgraded to Android 10.0.1, still robust and fast. NOTE: Before you take on this brave journey, make sure backup all important files on the phone at first! First, unlock the bootloader, https://www.lifewire.com/how-to-unlock-bootloader-android-phone-4689186 Enable Developer Options in Settings -&gt; About phone -&gt; Build number In Settings -&gt; System -&gt; Developer options, enable Advanced reboot, OEM unlocking, USB Debugging Download and install Android SDK Platform Tools on Windows https://developer.android.com/tools/releases/platform-tools Run commands: 12345$ adb devicesList of devices attached9b26c76 device$ adb reboot bootloader Wait for phone to reboot till phone in the Bootloader mode, then run: 1$ fastboot flashing unlock ON the phone will ask to confirm “UNLOCK THE BOOTLOADER”. After UNLOCK, your phone WILL BE RESET, like a factory hard reset. ALL APPS AND DATA ARE GONE. Android system will be reinstalled. Go to OnePlus Smartphone Software Update site and download the latest version of OnePlus 5T update on Windows, https://oneplus.net/in/support/softwareupdate Unzip OnePlus 5T update on Widnows On the phone Settings, search for USB Preferences, select USE USB FOR File transfer On Windows, in File Explorer, copy OnePlus5TOxygen_43_OTA_069_all_2010292144_76910d123e3940e5&#x2F;boot.img file to ONEPLUS A5010 -&gt; Internal shared storage -&gt; Download directory on the phone On the phone, download and install latest version Magisk, https://github.com/topjohnwu/Magisk Run Magisk, select Magisk Install, https://topjohnwu.github.io/Magisk/install.html Select and patch boot.img file under &#x2F;Download directory A patched file magisk_patched-27000_nplRf successfully generated. On Windows, in File Explorer, copy it to local directory On Windows, run: 1$ fastboot flash boot magisk_patched-27000_nplRf.img NOTE: Always patch boot image on the SAME device where you run Magisk. Reboot the phone, and install Root Checker to verify Root access, https://play.google.com/store/search?q=root%20checker&amp;c=apps&amp;hl=en&amp;gl=US Now OnePlus 5T has been officially ROOTED! NOTE: NO need to install TWRP (Team Win Recovery Project), https://twrp.me, a customised recovery application for Android devices on OnePlus 5T.","tags":[]},{"title":"Upscayl - AI Image Upscaler","date":"2023-12-17T08:13:45.000Z","path":"2023/12/17/Upscayl-AI-Image-Upscaler/","text":"Bondi Beach in Upscayl Bondi Beach original Bondi Beach upscayled Young Girl in Upscayl Young Girl original Young Girl upscayled References Upscayl - AI Image Upscaler, https://www.upscayl.org/","tags":[]},{"title":"OpenSSH Server for Windows","date":"2023-11-24T04:59:14.000Z","path":"2023/11/24/OpenSSH-Server-for-Windows/","text":"Want to run OpenSSH Server on Windows e.g. Windows 10. From Windows 10, it natively supports OpenSSH. NOTE: The beta and nightly build of OpenSSH Server for Windows have a lot of runtime issues. Check OpenSSH installation: 1234567PS C:\\ProgramData\\ssh&gt; Get-WindowsCapability -Online | Where-Object Name -like &#x27;OpenSSH*&#x27;Name : OpenSSH.Client~~~~0.0.1.0State : InstalledName : OpenSSH.Server~~~~0.0.1.0State : NotPresent Install the missing OpenSSH Server: 123456789101112131415PS C:\\ProgramData\\ssh&gt; Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0&gt;&gt;Path :Online : TrueRestartNeeded : FalsePS C:\\ProgramData\\ssh&gt; Get-WindowsCapability -Online | Where-Object Name -like &#x27;OpenSSH*&#x27;Name : OpenSSH.Client~~~~0.0.1.0State : InstalledName : OpenSSH.Server~~~~0.0.1.0State : Installed Check OpenSSH for Windows version, check Windows Operating System version: 12345PS C:\\ProgramData\\ssh&gt; ((Get-Item (Get-Command sshd).Source).VersionInfo.FileVersion)8.1.0.1PS C:\\ProgramData\\ssh&gt; ((Get-ItemProperty &quot;HKLM:\\SOFTWARE\\Microsoft\\Windows nt\\CurrentVersion\\&quot; -Name ProductName).ProductName)Windows 10 Enterprise Check Windows Domain information: 123456789101112PS C:\\ProgramData\\ssh&gt; dsregcmd /status+----------------------------------------------------------------------+| Device State |+----------------------------------------------------------------------+ AzureAdJoined : YES EnterpriseJoined : NO DomainJoined : YES DomainName : CORP Device Name : WINDOWS.corp.paradise.local... Check OpenSSH Server for Windows run as a service: Make sure OpenSSH SSH Server firewall inbound rule allows ALL profiles: The default C:\\ProgramData\\ssh\\sshd_config file doesn’t work for Windows Domain users authentication, and does’t support .ssh\\authorized_keys public key authentication. Error lookup_principal_name: User principal name lokup failed for user ‘corp\\darling’ in OpenSSH Server C:\\ProgramData\\ssh\\logs\\ssd log file. A work around solution is to comment out lines: 12#Match Group administrators# AuthorizedKeysFile __PROGRAMDATA__/ssh/administrators_authorized_keys A complete sshd_config example file: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# This is the sshd server system-wide configuration file. See# sshd_config(5) for more information.# The strategy used for options in the default sshd_config shipped with# OpenSSH is to specify options with their default value where# possible, but leave them commented. Uncommented options override the# default value.#Port 22#AddressFamily any#ListenAddress 0.0.0.0#ListenAddress ::#HostKey __PROGRAMDATA__/ssh/ssh_host_rsa_key#HostKey __PROGRAMDATA__/ssh/ssh_host_dsa_key#HostKey __PROGRAMDATA__/ssh/ssh_host_ecdsa_key#HostKey __PROGRAMDATA__/ssh/ssh_host_ed25519_key# Ciphers and keying#RekeyLimit default none# LoggingSyslogFacility LOCAL0LogLevel DEBUG3# Authentication:#LoginGraceTime 2m#PermitRootLogin prohibit-password#StrictModes yes#MaxAuthTries 6#MaxSessions 10#PubkeyAuthentication yes# The default is to check both .ssh/authorized_keys and .ssh/authorized_keys2# but this is overridden so installations will only check .ssh/authorized_keysAuthorizedKeysFile .ssh/authorized_keys#AuthorizedPrincipalsFile none# For this to work you will also need host keys in %programData%/ssh/ssh_known_hosts#HostbasedAuthentication no# Change to yes if you don&#x27;t trust ~/.ssh/known_hosts for# HostbasedAuthentication#IgnoreUserKnownHosts no# Don&#x27;t read the user&#x27;s ~/.rhosts and ~/.shosts files#IgnoreRhosts yes# To disable tunneled clear text passwords, change to no here!#PasswordAuthentication yes#PermitEmptyPasswords no# GSSAPI options#GSSAPIAuthentication no#AllowAgentForwarding yes#AllowTcpForwarding yes#GatewayPorts no#PermitTTY yes#PrintMotd yes#PrintLastLog yes#TCPKeepAlive yes#UseLogin no#PermitUserEnvironment no#ClientAliveInterval 0#ClientAliveCountMax 3#UseDNS no#PidFile /var/run/sshd.pid#MaxStartups 10:30:100#PermitTunnel no#ChrootDirectory none#VersionAddendum none# no default banner path#Banner none# override default of no subsystemsSubsystem sftp sftp-server.exe# Example of overriding settings on a per-user basis#Match User anoncvs# AllowTcpForwarding no# PermitTTY no# ForceCommand cvs server#Match Group administrators# AuthorizedKeysFile __PROGRAMDATA__/ssh/administrators_authorized_keys Now run ssh client and log on SSH Server: 12345$ sshpass -f ~/.ssh/windows.passwd ssh -l darling windows.localMicrosoft Windows [Version 10.0.19044.2965](c) Microsoft Corporation. All rights reserved.corp\\darling@WINDOWS C:\\Users\\darling&gt; References The Ultimate Guide to Installing OpenSSH on Windows, https://petri.com/the-ultimate-guide-to-installing-openssh-on-windows Installing SFTP&#x2F;SSH Server on Windows using OpenSSH, https://winscp.net/eng/docs/guide_windows_openssh_server","tags":[]},{"title":"Setup Socks/Socks5 proxy and git repo via proxy","date":"2023-10-25T11:23:53.000Z","path":"2023/10/25/Setup-Socks-Socks5-proxy-and-git-repo-via-proxy/","text":"Host windows.local has VPN connection which is granted with git repository. Setup Socks&#x2F;Socks5 proxy 123$ ssh-copy-id -i id_rsa.pub darling@windows.local$ ssh -D 3128 -q -C -N -f darling@windows.local -q: quiet mode, don’t output anything locally -C: compress data in the tunnel, save bandwidth -N: do not execute remote commands, useful for just forwarding ports -f: keep it running in the background If PasswordAuthentication is enforced, and pubilc key authentication in SSH Server is not supported, try: 1$ sshpass -f ~/.ssh/windows.passwd ssh -D 3128 -q -C -N -f darling@windows.local Configure git with Sock&#x2F;Socks5 proxy 123456789$ git config http.proxy &#x27;socks5://localhost:3128&#x27;$ cat .git/config[user] name = Terrence Miao email = terrence.miao@paradise.net signingkey = EBCEB936[http] proxy = socks5://localhost:3128 Then can access git repository via proxy both on command line and in UI client.","tags":[]},{"title":"Setup ssh ProxyCommand/proxyJump on multiple jump hosts","date":"2023-10-25T11:04:00.000Z","path":"2023/10/25/Setup-ssh-ProxyCommand-proxyJump-on-multiple-jump-hosts/","text":"NOTE: Some SSH Server doesn’t allow public key authentication. Then sshpass is a friend here for you. Install sshpass in MacOS: 1$ brew install esolitos/ipa/sshpass Test sshpass: 1$ ssh -oProxyCommand=&quot;sshpass -f ~/.ssh/windows.passwd ssh -W %h:%p jumphost&quot; -l darling jumphost-npe.paradise.net Setup .ssh/config file: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859## Keeping SSH Sessions AliveHost * ServerAliveInterval 15Host jumphost.mac Hostname mac.local IdentityFile ~/.ssh/id_rsa User darlingHost jumphost.windows Hostname windows.local IdentityFile ~/.ssh/id_rsa User darlingHost jumphost-npe Hostname jumphost-npe.paradise.net User darling IdentityFile ~/.ssh/id_rsa ProxyCommand sshpass -f ~/.ssh/windows.passwd ssh -W %h:%p jumphost.windows IdentitiesOnly yes StrictHostKeyChecking no UserKnownHostsFile=/dev/null ServerAliveInterval 60 ServerAliveCountMax 5## DEVELOPMENT hosts in AWSHost ip-10-212-*.ap-southeast-2.compute.internal ProxyCommand ssh -W %h:%p jumphost-npe IdentityFile ~/.ssh/dev-stack.pem## PTEST hosts in AWSHost ip-10-213-*.ap-southeast-2.compute.internal ProxyCommand ssh -W %h:%p jumphost-npe IdentityFile ~/.ssh/test-stack.pem## STEST hosts in AWSHost ip-10-214-*.ap-southeast-2.compute.internal ProxyCommand ssh -W %h:%p jumphost-npe IdentityFile ~/.ssh/test-stack.pemHost jumphost-prod HostName jumphost-prod.paradise.net User darling IdentityFile ~/.ssh/id_rsa.prod ProxyCommand sshpass -f ~/.ssh/windows.passwd ssh -W %h:%p jumphost.windows IdentitiesOnly yes StrictHostKeyChecking no UserKnownHostsFile=/dev/null ServerAliveInterval 60 ServerAliveCountMax 5## PROD hosts in AWSHost ip-10-208-*.ap-southeast-2.compute.internal ProxyCommand ssh -W %h:%p jumphost-prod IdentityFile ~/.ssh/prod-ddc-stack.pem## SSH over Session Managerhost i-* mi-* ProxyCommand sh -c &quot;aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters &#x27;portNumber=%p&#x27;&quot;","tags":[]},{"title":"Mac and iPad and Android Benchmark Ceiling","date":"2023-07-29T13:20:30.000Z","path":"2023/07/29/Mac-and-iPad-and-Android-Benchmark-Ceiling/","text":"Benchmark head to head - Mac vs. iPad. vs. Android","tags":[]},{"title":"Micro Frontends","date":"2023-07-29T05:16:56.000Z","path":"2023/07/29/Micro-Frontends/","text":"Righ model, right tools - the new way of building and collaborating on frontend apps is the core element of Micro Frontends.","tags":[]},{"title":"Remove Image Background","date":"2022-10-12T11:59:54.000Z","path":"2022/10/12/Remove-Image-Background/","text":"Remove backgrounds 100% automatically in 5 seconds with one click. Thanks to remove.bg https://www.remove.bg/ clever AI.","tags":[]},{"title":"Geekbench head to head - Mac vs iPad vs Android","date":"2022-10-10T01:48:30.000Z","path":"2022/10/10/Geekbench-head-to-head-Mac-vs-iPad-vs-Android/","text":"Geekbench head to head - Mac vs iPad vs Android: 10 years old Mac mini Late 2012 (2.6GHz quad-core Intel Core i7 Turbo Boost up to 3.6GHz) 4 years old MacBook Pro 15-inch Mid 2018 (2.6GHz 6-core Intel Core i7, Turbo Boost up to 4.3GHz) 4 years old iPad Pro 11” 2018 3rd gen (A12X Bionic 7 nm, 4-core Vortex at 2.5GHz , 4-core Tempest at 1.6GHz) 1 year old MacBook Pro 16-inch 2021 (Apple Silicon M1 Pro, 10-core CPU at 3.2GHz) 没满岁的 Android (Snapdragon 8 Gen1, 1 core Cortex-X2 at 3GHz, 3-core Cortex-A710 at 2.5GHz, and 4-core Cortex-A510 at 1.8GHz) Conclusion: Intel 已经是江河日下，被时代淘汰 Apple Processors 实在太强。 四年前的处理器还可吊打当前最新的 Snapdragon Processors 同是 2018 年产品，iPad Pro 把 Intel CPU based 的 MacBook Pro 按在地上碾压 ARM vs x86 谁是未来，一目了然。 NOTE The Apple M1 chip is built for Macs, and the A15 for phones. They use completely different architectures. The A15 prioritizes battery over performance. The M1 has more firepower for graphics.","tags":[]},{"title":"How to use Web Inspector and debug Safari on iPhone/iPad","date":"2022-09-07T03:32:33.000Z","path":"2022/09/07/How-to-use-Web-Inspector-and-debug-Safari-on-iPhone-iPad/","text":"Prerequisites: iPhone &#x2F; iPad &#x2F; iPod and the Macbook on the same version of Safari a genuine Apple lightning or USB cable Step by step: On iPad, iPhone or iPod touch, go to menu Settings &gt; Safari &gt; Advanced and toggle on Web Inspector. Enable JavaScript if it isn’t already on On Macbook, launch Safari and go to menu Preferences &gt; Advanced then toggle on “Show Develop menu in menu bar” Connect iOS device to Macbook with the lightning or USB cable Now on iOS device, open Safari and go to the website you want to debug On Macbook, open Safari and go to “Develop” menu. You now see your iOS device that has connected with Macbook (if no page opened on iOS device, you see a message saying “No Inspectable Applications”) ![Safari Develop menu](&#x2F;img&#x2F;Safari Develop.png “Safari Develop menu”) Click on the website in Safari Develop &gt; iOS device menu, Web Inspector window opened, then you can debug as you used to debug in Safari ![Web Inspector](&#x2F;img&#x2F;Web Inspector.png “Web Inspector”)","tags":[]},{"title":"GitHub Actions","date":"2022-09-02T05:33:47.000Z","path":"2022/09/02/GitHub-Actions/","text":"Have you tried GitHub Actions? GitHub Actions GitHub Actions Workflow","tags":[]},{"title":"Powerful Zsh","date":"2022-08-02T09:42:06.000Z","path":"2022/08/02/Powerful-Zsh/","text":"First you have Zsh, next install Oh My Zsh https://ohmyz.sh/ 1sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot; Add Powerlevel10k https://github.com/romkatv/powerlevel10k and configure it 1git clone --depth=1 https://github.com/romkatv/powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom&#125;/themes/powerlevel10k Set ZSH_THEME to powerlevel10k in .zshrc 1ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot; Then run Powerlevel10k configure: 1p10k configure Add zsh-autosuggestions https://github.com/zsh-users/zsh-autosuggestions 1git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions Add zsh-syntax-highlighting https://github.com/zsh-users/zsh-syntax-highlighting 1git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlighting and enable them in .zshrc: 123456...plugins=( zsh-autosuggestions zsh-syntax-highlighting)... Install Fig (deprecated) https://fig.io/, an IDE-style autocomplete but for terminal, and configure in .zshrc 1234567...# Fig pre block. Keep at the top of this file.[[ -f &quot;$HOME/.fig/shell/zshrc.pre.zsh&quot; ]] &amp;&amp; . &quot;$HOME/.fig/shell/zshrc.pre.zsh&quot;...# Fig post block. Keep at the bottom of this file.[[ -f &quot;$HOME/.fig/shell/zshrc.post.zsh&quot; ]] &amp;&amp; . &quot;$HOME/.fig/shell/zshrc.post.zsh&quot;... References How To Make Your Boring Mac Terminal So Much Better, https://www.youtube.com/watch?v=CF1tMjvHDRA How To Setup Your Mac Terminal, https://www.josean.com/posts/terminal-setup","tags":[]},{"title":"Convert JSON to CSV by using jq","date":"2022-05-02T06:20:52.000Z","path":"2022/05/02/Convert-JSON-to-CSV-by-using-jq/","text":"Step by step convert exported JSON data from AWS DynamoDB table into CSV, by using jq. Export all the data from AWS DynamoDB table at first: 1𝜆 aws --profile production dynamodb scan --table-name tiles &gt; tiles.json The exported JSON data looks like: 12345678910111213141516171819202122232425262728293031323334&#123; &quot;Items&quot;: [ &#123; &quot;last_modified_date&quot;: &#123; &quot;S&quot;: &quot;2021-12-09T01:15:25.335516&quot; &#125;, &quot;valid_from&quot;: &#123; &quot;S&quot;: &quot;2021-12-09T01:00&quot; &#125;, &quot;created_date&quot;: &#123; &quot;S&quot;: &quot;2021-12-09T01:15:25.334965&quot; &#125;, &quot;status&quot;: &#123; &quot;S&quot;: &quot;PUBLISHED&quot; &#125;, &quot;valid_to&quot;: &#123; &quot;S&quot;: &quot;2022-01-31T23:00&quot; &#125;, &quot;id&quot;: &#123; &quot;S&quot;: &quot;b2c60f43-a81c-4363-a68a-dfe7682182d7&quot; &#125;, &quot;description&quot;: &#123; &quot;S&quot;: &quot;Hit the road Jack!&quot; &#125;, &quot;title&quot;: &#123; &quot;S&quot;: &quot;Novated Lease&quot; &#125; &#125;,... ], &quot;Count&quot;: 223, &quot;ScannedCount&quot;: 223, &quot;ConsumedCapacity&quot;: null&#125; Extract &#x2F; transform JSON data: 123456789101112𝜆 cat tiles.json | jq &#x27;[.Items[] | &#123; id: .id.S, title: .title.S, description: .description.S, status: .status.S, valid_from: .valid_from.S, valid_to: .valid_to.S &#125;]&#x27; &gt; tiles-extracted.json[ &#123; &quot;id&quot;: &quot;b2c60f43-a81c-4363-a68a-dfe7682182d7&quot;, &quot;title&quot;: &quot;Novated Lease&quot;, &quot;description&quot;: &quot;Hit the road Jack!&quot;, &quot;status&quot;: &quot;PUBLISHED&quot;, &quot;valid_from&quot;: &quot;2021-12-09T01:00&quot;, &quot;valid_to&quot;: &quot;2022-01-31T23:00&quot; &#125;,...] Convert JSON data into CSV: 1𝜆 cat tiles-extracted.json | jq -r &#x27;(.[0] | keys_unsorted) as $keys | $keys, map([.[ $keys[] ]])[] | @csv&#x27; &gt; tiles.csv References How to convert arbitrary simple JSON to CSV using jq, https://stackoverflow.com/questions/32960857/how-to-convert-arbitrary-simple-json-to-csv-using-jq","tags":[]},{"title":"Customise VS Code settings and keybindings with Geddski macros","date":"2021-07-30T10:28:19.000Z","path":"2021/07/30/Customise-VS-Code-settings-and-keybindings-with-Geddski-macros/","text":"In IntelliJ IDEA, you can comment a line, the cursor is moved to the next line automatically. This is a very easy way to comment several lines. However, in VS Code, default behaviour is that the cursor stays on the same line. To copy the behavior of IntelliJ, go with: Install macros author by geddski in VS Code. Edit settings.json and add: 123456&quot;macros&quot;: &#123; &quot;commentDown&quot;: [ &quot;editor.action.commentLine&quot;, &quot;cursorDown&quot; ]&#125;, Edit keybindings.json and add: 1234567[ &#123; &quot;key&quot;: &quot;cmd+/&quot;, &quot;command&quot;: &quot;macros.commentDown&quot;, &quot;when&quot;: &quot;editorTextFocus &amp;&amp; !editorReadonly&quot; &#125;]","tags":[]},{"title":"Export and Import AWS DynamoDB data","date":"2021-07-06T05:07:34.000Z","path":"2021/07/06/Export-and-Import-AWS-DynamoDB-data/","text":"A simple, straightforward way export and import AWS DynamoDB table’s data with AWS CLI and a few scripts. At first, export all the data from AWS DynamoDB table: 1𝜆 aws --profile production dynamodb scan --table-name tile-event &gt; tile-event-export.json Convert a list of items&#x2F;records (DynamoDB JSON) into individual PutRequest JSON with jq. 1𝜆 cat tile-event-export.json | jq &#x27;&#123;&quot;Items&quot;: [.Items[] | &#123;PutRequest: &#123;Item: .&#125;&#125;]&#125;&#x27; &gt; tile-event-import.json Transform the data if necessary: 1𝜆 sed &#x27;s/tile-images-prod/tile-images-pdev/g&#x27; tile-event-import.json &gt; tile-event-import-transformed.json Split all requests into 25 requests per file, with jq and awk (Note: There are some restriction with AWS DynamoDB batch-write-item request - The BatchWriteItem operation can contain up to 25 individual PutItem and DeleteItem requests and can write up to 16 MB of data. The maximum size of an individual item is 400 KB.) 123456789101112131415161718192021𝜆 cat tile-event-processed.awk#!/usr/bin/awk -fNR%25==1 &#123; x=&quot;tile-event-import-processed-&quot;++i&quot;.json&quot;; print &quot;&#123;&quot; &gt; x print &quot; \\&quot;tile-event\\&quot;: [&quot; &gt; x&#125;&#123; printf &quot; %s&quot;, $0 &gt; x;&#125;NR%25!=0 &#123; print &quot;,&quot; &gt; x&#125;NR%25==0 &#123; print &quot;&quot; &gt; x print &quot; ]&quot; &gt; x print &quot;&#125;&quot; &gt; x&#125;𝜆 jq -c &#x27;.Items[]&#x27; tile-event-import-transformed.json | ./tile-event-processed.awk Import all 22 processed JSON files into DynamoDB table: 1234$ for f in tile-event-import-processed-&#123;1..22&#125;.json; do \\ echo $f; \\ aws --profile development dynamodb batch-write-item --request-items file://$f; \\ done","tags":[]},{"title":"Get and read logs from AWS CloudWatch with saw","date":"2021-06-04T03:49:02.000Z","path":"2021/06/04/Get-and-read-logs-from-AWS-CloudWatch-with-saw/","text":"For all the people painfully read logs on AWS CloudWatch console, saw is your friend. Get CloudWatch log groups start with paradise-api: 12𝜆 saw groups --profile ap-prod --prefix paradise-apiparadise-api-CloudFormationLogs-mwwmzgYOtbcB Get last 2 hours logs for paradise-api from CloudWatch, with saw: 1𝜆 saw get --profile ap-prod --start -2h paradise-api-CloudFormationLogs-mwwmzgYOtbcB --prefix docker | jq .log | sed &#x27;s/\\\\\\n&quot;$//; s/^&quot;//&#x27;","tags":[]},{"title":"AWS EC2 CloudFormation templates in subnets","date":"2021-05-11T13:02:15.000Z","path":"2021/05/11/AWS-EC2-CloudFormation-templates-in-subnets/","text":"AWS EC2 instance CloudFormation templates in Public Tier and App Tier:","tags":[]},{"title":"Read environment variables of a process in Linux","date":"2021-04-30T12:44:39.000Z","path":"2021/04/30/Read-environment-varibles-of-a-process-in-Linux/","text":"When try to get the content of any &#x2F;proc&#x2F;PID&#x2F;environ file in more readable format, you can: 1234/proc/[pid]/environ This file contains the environment for the process. The entries are separated by null bytes (&#x27;\\0&#x27;), and there may be a null byte at the end. A simple way is to apply xargs -0 -L1 -a on it: -0 - read null-delimited lines, -L1 - read one line per execution of command -a - file read lines from file 12345678910111213# ps -aef10101 3629 3589 0 Apr27 ? 00:00:00 /bin/bash bin/start10101 3670 3629 0 Apr27 ? 00:00:00 /bin/bash bin/start-tomcat10101 3671 3670 0 Apr27 ? 00:07:36 /usr/lib/jvm/java-11-amazon-corretto.x86_64/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/# cat /proc/3629/environPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=27c44e8a5c7cJAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto.x86_64HOME=/usr/local/tomcat# xargs -0 -L1 -a /proc/3629/environPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=27c44e8a5c7cJAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto.x86_64HOME=/usr/local/tomcat","tags":[]},{"title":"AWS KMS decrypt for base64 encoded input","date":"2021-04-30T09:22:55.000Z","path":"2021/04/30/AWS-KMS-decrypt-for-base64-encoded-input/","text":"With AWS CLI version 2: 12𝜆 aws --versionaws-cli/2.1.17 Python/3.7.4 Darwin/20.3.0 exe/x86_64 prompt/off Encrypt with AWS KMS key: 1234567𝜆 aws kms encrypt --profile personal \\ --key-id e2695b79-cbe0-4c16-aa5e-b7dbf52df1f9 \\ --plaintext &quot;string-to-encrypt&quot; \\ --output text \\ --query CiphertextBlob \\ --cli-binary-format raw-in-base64-outAQICAHjbJrIPgME ... lILuBSUdA== Decrypt with AWS KMS key: 12345𝜆 echo &quot;AQICAHjbJrIPgME ... lILuBSUdA==&quot; | base64 -D | \\ aws kms decrypt --profile personal \\ --ciphertext-blob fileb:///dev/stdin \\ --output text \\ --query Plaintext | base64 -D Reference cli_binary_format, https://docs.aws.amazon.com/credref/latest/refdocs/setting-global-cli_binary_format.html","tags":[]},{"title":"A Modern Architecture Application","date":"2021-03-02T05:55:13.000Z","path":"2021/03/02/A-Modern-Architecture-Application/","text":"RAD (Rapid Application Development) of a Serverless application “Notification Service” on modern technologies, e.g. AWS CDK &amp; SAM, AWS Step Functions, TypeScript, VS Code, Open API Top Down Design and Test Driven Development, in order to rapidly build a prototype, or a POC, verify and test some technologies and approaches. Request Handler &#x3D;&gt; Step Functions (orchestration for Lambda functions, represents a single centralized executable business process, outsources low level operations like retry &#x2F; exception catch and handle. Another choice is SNS) &#x3D;&gt; Service Providers Have experienced of Terraform, Serverless, AWS SAM … now this time based on code over configuration principle, what you get is flexibility, predictability and more control. You focus on code you tell the tools what steps it has to complete directly. At the end of day, it is a simple matter of separation of concerns and single responsibility principle. • VS Code for API Spec editing • Postman API, Environment and Mock server, for QA team, then switch to real service in DEV&#x2F;TEST environment 1𝜆 npm run openapi • openapi-generator generates model classes; typescript-json-validator generates JSON Schema and validator 12𝜆 openapi-generator generate -g typescript-node -i Notification\\ API\\ openapi.json -o Notification\\ API\\ generated𝜆 npx typescript-json-validator notificationRequest.ts NotificationRequest • Onboard on Kong &#x2F; API Manager, https://konghq.com/kong/ • CDK, is based on CloudFormation but abstract layer on the top of it. It can generates CloudFormation template file template.yaml 1𝜆 cdk synth --no-staging &gt; template.yaml • Demo of local run and debug lambda, with background TSC watch process 1234𝜆 npm run watch𝜆 sam local invoke RequestNotification9F9F3C31 -e samples/api-gateway-notification-event.json𝜆 sam local invoke RequestNotification9F9F3C31 -e samples/api-gateway-notification-event.json -d 5858 Data validation to make data integrity unbreachable will take a lot time. ajv framework and performance benchmark, https://github.com/ebdrup/json-schema-benchmark • Code lint with eslint and prettier and automatically correction • Code commit rule enforcement • Change code and deploy AWS stack by CDK 1𝜆 cdk deploy --require-approval never --profile dev-cicd • Behavior Driven Test Framework Jest, https://github.com/facebook/jest, 2x &#x2F; 3x faster than Karma, with code coverage, easy mocking 1𝜆 npm t • Automatically generate application changelog and release notes 1𝜆 npm run release:minor • Automatically generate application document 1𝜆 npm run docs • AWS resources created by CDK • Not Mono Repo app, which multiple projects all under one giant Repo • ONE AWS Layers put all dependent NPM libs and shared code into; size of Lambda functions, readability • AWS EventBridge tro trigger and send event to Request Handler, for scheduling task • Health Check, with Service Monitoring Dashboard, verify dependencies at the endpoints, keep Lambda warming up 1𝜆 curl https://c81234xdae8w1a9.execute-api.ap-southeast-2.amazonaws.com/health Cloud computing and Serverless architecture let developers in fast lane for Application Development. Right now, there are so many low hanging fruit to pick up. As developers, we should not always think about our comfort zone, we need to think about people who take over your work, think about BAU team to support the application. The codebase is not about you, but about the value that your code brings to others, and the organization that you work for.","tags":[]},{"title":"Why Repairable Is Important","date":"2020-11-30T00:38:56.000Z","path":"2020/11/30/Why-Repairable-Is-Important/","text":"Fix our destructive and throwaway economy, save our planet by making products both repairable, durable, environmentally friendly, and by defending our Right to Repair. This is my second video created and composed by Apple Final Cut Pro X - Why Repairable Is Important https://www.youtube.com/watch?v=qSmFI6J7eRo","tags":[]},{"title":"Bring back MagSafe","date":"2020-11-25T00:08:16.000Z","path":"2020/11/25/Bring-back-MagSafe/","text":"My first published video, created by Apple Final Cut Pro, on YouTube for official channel title Bring back MagSafe regard to solution that bring one of the most innovative design from Apple, back to MacBook Pro, iPad … and Android phones https://www.youtube.com/watch?v=yvkJR4Y0FK0 WSKEN SHARK X4 TYPE-C TO TYPE-C PD 100W QUICK CHARGE MAGNETIC CABLE: http://wsken.my/WSKEN-Shark-X4-Type-C-To-Type-C-PD-100W-Quick-Charge-Magnetic-Cable WSKEN SHARK X5 TYPE-C 5A QUICK CHARGE MAGNETIC CABLE: http://wsken.my/WSKEN-Shark-X5-Type-C-5A-Quick-Charge-Magnetic-Cable WSKEN Official Store on AliExpress: https://www.aliexpress.com/store/3018058","tags":[]},{"title":"Risk Management for CI/CD processes","date":"2020-10-26T05:55:13.000Z","path":"2020/10/26/Risk-Management-for-CI-CD-processes/","text":"Consider a full development and deployment cycle, and the potential risks involved during the different stages in CDP (CI &#x2F; Continuous Integration, CD &#x2F; Continuous Delivery, CDP &#x2F; Continuous Deployment): Code Role Details Stakeholders Individual Developer Pair Programming Mentor DBA Security Team Failure Points Logic flaws Security flaws Code standards issues Safeguards Test Driven Development Red&#x2F;Green&#x2F;Refactor Linting tools Testing Docker containers Pair programming Query analysis Static code analysis Commit Role Details Stakeholders Security Team Member for sign-off Engineering Team Lead for sign-off Failure Points Force pushes Merge conflicts Safeguards Master branch protections 3 member sign-off before master merge Commit hooks Test Role Details Stakeholders Individual Developer QA Team Failure Points Broken tests Stale tests False positive tests Safeguards Weekly failure testing triage meeting to catch broken tests Daily cron runs of test suite against mock prod environment Deployment Role Details Stakeholders SysOps Team Individual Developers Support Team Customers Failure Points Broken deployments Dropped customer traffic Safeguards Blue&#x2F;Green deployment Traffic re-routing Pre deployment spare instance warmup Communicate out to support in order to verify proper staffing levels Runtime Role Details Stakeholders Security Team SysOps Team Engineering Teams Support Team Customers Failure Points High resource usage Slow queries Malicious actors MProvider downtime Safeguards Communicate out to support for new feature awareness and appropriate categories for issues regarding the component System resource alarms for various metrics and slow DB log alerts Instant maintenance page switchover capabilities Status page on redundant providers Application firewalls Database replicas","tags":[]},{"title":"AWS CloudWatch Metrics Example","date":"2020-10-13T12:29:39.000Z","path":"2020/10/13/AWS-CloudWatch-Metrics-example/","text":"AWS CloudWatch MetricsThe interface of Metrics in AWS CloudWatch console: The URL: 1https://ap-southeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-2#metricsV2:graph=~(metrics~(~(~&#x27;AWS*2fRoute53Resolver~&#x27;InboundQueryVolume)~(~&#x27;.~&#x27;OutboundQueryVolume))~view~&#x27;timeSeries~stacked~false~region~&#x27;ap-southeast-2~stat~&#x27;Sum~period~86400~start~&#x27;-P28D~end~&#x27;P0D);query=~&#x27;*7bAWS*2fRoute53Resolver*7d Metrics source: 123456789101112&#123; &quot;metrics&quot;: [ [ &quot;AWS/Route53Resolver&quot;, &quot;InboundQueryVolume&quot; ], [ &quot;.&quot;, &quot;OutboundQueryVolume&quot; ] ], &quot;view&quot;: &quot;timeSeries&quot;, &quot;stacked&quot;: false, &quot;region&quot;: &quot;ap-southeast-2&quot;, &quot;stat&quot;: &quot;Sum&quot;, &quot;period&quot;: 86400, &quot;title&quot;: &quot;Test&quot;&#125;","tags":[]},{"title":"Creating AWS Lambda with AWS SAM","date":"2020-02-14T01:12:06.000Z","path":"2020/02/14/Creating-AWS-Lambda-with-AWS-SAM/","text":"This is a simple Lambda with REST API and SNS enabled. Firstly, have a look the Nodejs script: AWS SAM template yaml file: Generate AWS CloudFormation yaml file and package &#x2F; zip &#x2F; create an artefact (need to create AWS S3 bucket hello-world-tub in advance): 123456𝜆 sam package --profile personal --template-file template.yml --output-template-file cloudFormation.yml --s3-bucket hello-sam-tubUploading to 7431f83ac979bfccc26980049807e595 1461 / 1461.0 (100.00%)Successfully packaged artifacts and wrote output template to file cloudFormation.yml.Execute the following command to deploy the packaged templatesam deploy --template-file /Users/terrence/Projects/hello-sam/cloudFormation.yml --stack-name &lt;YOUR STACK NAME&gt; Can also create artefact file with zip command, and upload zip file into AWS S3 bucket: 1𝜆 zip hello-sam.zip README.md index.js template.yml What AWS CloudFormation yaml file looks like: Deploy application’s AWS CloudFormation stack with AWS SAM command: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556𝜆 sam deploy --profile personal --template-file cloudFormation.yml --stack-name hello-sam --capabilities CAPABILITY_IAM Deploying with following values =============================== Stack name : hello-sam Region : None Confirm changeset : False Deployment s3 bucket : None Capabilities : [&quot;CAPABILITY_IAM&quot;] Parameter overrides : &#123;&#125;Initiating deployment=====================Waiting for changeset to be created..CloudFormation stack changeset---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Add HelloWorldFunctionHelloWorldApiPermissionProd AWS::Lambda::Permission+ Add HelloWorldFunctionRole AWS::IAM::Role+ Add HelloWorldFunction AWS::Lambda::Function+ Add HelloWorldTopic AWS::SNS::Topic+ Add ServerlessRestApiDeployment79454cea13 AWS::ApiGateway::Deployment+ Add ServerlessRestApiProdStage AWS::ApiGateway::Stage+ Add ServerlessRestApi AWS::ApiGateway::RestApi---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:ap-southeast-2:123456789012:changeSet/samcli-deploy1581737165/48e53ff2-1b50-45d8-bbfd-97652f20d9672020-02-15 14:26:10 - Waiting for stack create/update to completeCloudFormation events from changeset-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------CREATE_IN_PROGRESS AWS::SNS::Topic HelloWorldTopic Resource creation InitiatedCREATE_COMPLETE AWS::SNS::Topic HelloWorldTopic -CREATE_IN_PROGRESS AWS::IAM::Role HelloWorldFunctionRole Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role HelloWorldFunctionRole -CREATE_IN_PROGRESS AWS::Lambda::Function HelloWorldFunction Resource creation InitiatedCREATE_COMPLETE AWS::Lambda::Function HelloWorldFunction -CREATE_IN_PROGRESS AWS::ApiGateway::RestApi ServerlessRestApi Resource creation InitiatedCREATE_COMPLETE AWS::ApiGateway::RestApi ServerlessRestApi -CREATE_IN_PROGRESS AWS::Lambda::Permission HelloWorldFunctionHelloWorldApiPermissionProd Resource creation InitiatedCREATE_IN_PROGRESS AWS::ApiGateway::Deployment ServerlessRestApiDeployment79454cea13 Resource creation InitiatedCREATE_COMPLETE AWS::ApiGateway::Deployment ServerlessRestApiDeployment79454cea13 -CREATE_IN_PROGRESS AWS::ApiGateway::Stage ServerlessRestApiProdStage Resource creation InitiatedCREATE_COMPLETE AWS::ApiGateway::Stage ServerlessRestApiProdStage -CREATE_COMPLETE AWS::Lambda::Permission HelloWorldFunctionHelloWorldApiPermissionProd -CREATE_COMPLETE AWS::CloudFormation::Stack hello-sam --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Successfully created/updated stack - hello-sam in None NOTE: The --capabilities CAPABILITY_IAM option is necessary to authorise your stack to create IAM roles, which SAM applications do by default. After application deployed, user subscribes notification will receive email titled - AWS Notification - Subscription Confirmation After confirmation, user will receive an email every time API is invoked. Now log on AWS Console, have a look the resources this Lambda application used in CloudFormation, S3 Bucket, Lambda, IAM, SNS, CloudWatch, API Gateway. After this Lambda application successfully deployed into AWS, you will receive an email asking whether you want to subscribe the SNS topic. You can also unsubscribe the SNS topic. You can manually test Lambda function in AWS Console. References Creating AWS Lambda Applications With SAM by Daniel Weibel, https://itnext.io/creating-aws-lambda-applications-with-sam-dd13258c16dd","tags":[]},{"title":"2019: Year in Review","date":"2019-12-29T04:24:05.000Z","path":"2019/12/29/2019-Year-in-Review/","text":"2019 年是过去十年中最差的一年，但或许会是未来十年中最好的一年。 图 1：这不是传说，这是 reality. 1 月 24 日，有自媒体发出证监会人事变动的“虚假”消息，引起关注。中国证监会有关人士指出，自媒体不是法外之地，对于不负责任的虚假信息传播，应当依法依规处理。 1 月 26 日，中国官方宣布负责股市监管的中国证监会主席换人。三年前在中国股市遭遇“灾难性重创”的背景下接班的刘士余结束任期，其职务由中国另一国有银行工商银行董事长易会满接任。 “谣言”就是“遥遥领先的预言”。 老刘的这张配图，是在 salute 中国证监会，还是中国股民？ “问君能有几多愁，恰似满仓中石油。如若当初没割肉，而今想来愁更愁。” 这是 8 月 26 日新闻头条。中石油股价收于 6.02 元，刷新历史新低。中石油股价最高时 48 元，上市 12 年，蒸发了 7 万亿，从市值一哥到“套牢第一股”。7 万亿相当于跌掉 1 个苹果，或 2 个俄罗斯股市的总市值。 “我有无数次骑上大牛，结果半路被吓跑了。 然后看着人家一飞冲天，想大哭都没有眼泪。 我真想把自己的贱手给砍掉。” “我有无数次踩上大屎，结果硬挺在那几年。 然后等着分红一毛无有，想上访都没有胆子。 我真想把自己的脚也给砍掉。” 让中国股民怀念的牛市，都像是怀念的初恋一样，那消散已久的牛味仿佛做了一场春梦般的美妙。 图 2：Hayne’s Report. 2 月 The Royal Commission into Misconduct in the Banking, Superannuation and Financial Services Industry Final Report, lead by Keith Hayne published. 导致土澳经济支柱产业四大银行中三个的 CEO 先后在今年中被迫辞职。 图 3：超乎想像的宇宙。 4 月，人类有史以来获得的首张黑洞照片。位于处女座中的 M87 的超大质量黑洞距离我们大约 5500 万光年。其质量之大约合 66 亿个太阳。 图 4：I believe in miracles. 大选前一个月，各项民意调查都 tip Labor win the election. Sportsbets back Labor win at odds of $1.16, Coalition $5.80. Labor 竞选团队在全国各地拉票讲演时也是踌躇满志，势在必赢。星期六竞选日早上，“主流” TV &amp; newspapers 还预测 Labor’s landslide win。但选举最终在 Coalition 戏剧性的大逆转中结束。 民意调查机构不得不承认自己吹嘘的 Machine Learning &#x2F; AI 只是骗人的小把戏。Sportsbet 赔了 $5.2m on its costly error。“主流”媒体也不得不扇自己的脸，radical political idiosyncrasy and elitism bias 根本做不了三个代表。 Monash University professor Andrew Markus said Australians usually nominated jobs, the economy and financial security as their top concerns and may have recoiled from Labor’s sweeping plans for tax revenue increases. “If there’s a danger that your agenda challenges those economic factors, you’re on pretty rocky ground.” Now quiet Australians are heard loud and clear. The swing to the Liberals suggested voters were sceptical of policies to raise $56 billion from changes to dividend rules, $32 billion from negative gearing and $30 billion over a decade from superannuation. “What one person receives without working for, another person must work for without receiving.” Here is the wisdom from Adrian Rogers. Wasn’t it Labor shadow treasurer Mr. Chris Bowen who said “if you do t like us taking away your franking credits then don’t vote for Labor” before the election? Well, Mr. Bowen, thank you for your invitation! 图 5：Eluid Kipchoge. 10 月 12 日，34 岁的肯尼亚人，马拉松世界纪录保持者 Eluid Kipchoge，在“世界音乐之都”奥地利首都维也纳，向全程马拉松 Breaking 2 “破二”的宏伟目标发起个人职业生涯的第二度冲击。这是人类跨越马拉松新里程碑的历史性一刻。马拉松突破人类 2 小时大关。 1 小时 59 分 40 秒 In perspective, 想象一下，Eluid Kipchoge just runs every 100 meters in 17s, for two hours. 17 frigging seconds! 图 6：Reserve Bank Australia cuts interest rates to historic low. Central bank is literally behind the curve. More ominously, it’s an indication that asset bubbles are poised to burst, just like the Fed’s first interest rate cut warned directly ahead of both the tech bust and GFC. 现在的一个经济幽灵就是”债务”。下一个经济危机很有可能就是债务危机。 政府债，企业债，个人债，大量的负债，超过了借款者自身偿还能力而引发的债务危机，金融海啸。先财务困境，再经济由不稳定转至崩溃，中产变成破产，从而造成社会的大动荡。 图 7：Dow 28000. In November, another historical moment, Dow closes above 28,000. From 27,000 to 28,000, get there just in 90 trading days. 全球的股市可以说气势如虹。美国股市的市值和 GDP 比值已经超过了 150%，超越了 2000 年互联网泡沫和 2007 年房地产泡沫顶峰时期的估值。 房地产市场同样是高歌猛进。美国的房地已经超过了 2007 年顶峰 20% 的水平了。不管从什么指标衡量，一批指标都显示当前资本市场的估值都已经超过了 2007 年，这是个巨大的泡沫。 有人预测明年 2020 年将是次贷危机以来最难的一年。但因为似乎好像每经过一年都是过去 10 年中最难的一年，全球经济再也没有回到过快速增长的快车道上，所以明年才是刚开始，未来十年才是最艰难的。 目前债务杠杆超过了 250%。全球各国央行以零利率和负利率维持。在这种情形下，必然导致股市和房地产市场泡沫。 一旦这两个泡沫破灭，实体经济也必然会受到影响。从 400 多年的经济史中可以得出的结论就是市场规律可以推迟，但从未缺席。 图 8：Home grown farm. 以农补工。经济不景气，投资回报低，肉类，生蔬和水果价格居高临下的环境中依靠 home grown，家中盆栽的杏，苹果和橄榄弥补减少的收入，扶持增大的开销。 图 9：Year in sports - Personal Best. 创\b记录的一年。跑动距离（包括跑步和网球）达到 1300+ 公里，相当于从北京跑到上海。Climbing 14,652m，相当于攀登了 1.7 个珠穆朗玛峰的高度。 不因岁月裹步不前，不因磨难放弃梦想，不因极限停止前进。A journey of a thousand miles begins with a single step. Keep running!","tags":[]},{"title":"AWS EKS for Fargate, with eksctl","date":"2019-12-07T11:33:38.000Z","path":"2019/12/07/AWS-EKS-on-Fargate-with-eksctl/","text":"AWS EKS, with eksctlSecond try with AWS EKS on Fargate. This time with eksctl. Create EKS cluster: 12345678910111213141516171819202122232425𝜆 eksctl create cluster --name sandpit --version 1.14 --region us-east-2 --fargate[ℹ] eksctl version 0.11.1[ℹ] using region us-east-2[ℹ] setting availability zones to [us-east-2b us-east-2a us-east-2c][ℹ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19[ℹ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19[ℹ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19[ℹ] using Kubernetes version 1.14[ℹ] creating EKS cluster &quot;sandpit&quot; in &quot;us-east-2&quot; region with Fargate profile[ℹ] if you encounter any issues, check CloudFormation console or try &#x27;eksctl utils describe-stacks --region=us-east-2 --cluster=sandpit&#x27;[ℹ] CloudWatch logging will not be enabled for cluster &quot;sandpit&quot; in &quot;us-east-2&quot;[ℹ] you can enable it with &#x27;eksctl utils update-cluster-logging --region=us-east-2 --cluster=sandpit&#x27;[ℹ] Kubernetes API endpoint access will use default of &#123;publicAccess=true, privateAccess=false&#125; for cluster &quot;sandpit&quot; in &quot;us-east-2&quot;[ℹ] 1 task: &#123; create cluster control plane &quot;sandpit&quot; &#125;[ℹ] building cluster stack &quot;eksctl-sandpit-cluster&quot;[ℹ] deploying stack &quot;eksctl-sandpit-cluster&quot;[✔] all EKS cluster resources for &quot;sandpit&quot; have been created[✔] saved kubeconfig as &quot;/Users/terrence/.kube/config&quot;[ℹ] creating Fargate profile &quot;fp-default&quot; on EKS cluster &quot;sandpit&quot;[ℹ] created Fargate profile &quot;fp-default&quot; on EKS cluster &quot;sandpit&quot;[ℹ] &quot;coredns&quot; is now schedulable onto Fargate[ℹ] &quot;coredns&quot; is now scheduled onto Fargate[ℹ] &quot;coredns&quot; pods are now scheduled onto Fargate[ℹ] kubectl command should work with &quot;/Users/terrence/.kube/config&quot;, try &#x27;kubectl get nodes&#x27;[✔] EKS cluster &quot;sandpit&quot; in &quot;us-east-2&quot; region is ready Create and add EKS mansged node group: 123456789101112131415161718192021𝜆 eksctl create nodegroup --cluster sandpit --name workers --node-type t3a.medium --ssh-access --ssh-public-key aws-us-key --managed[ℹ] eksctl version 0.11.1[ℹ] using region us-east-2[ℹ] will use version 1.14 for new nodegroup(s) based on control plane version[ℹ] using EC2 key pair %!!(MISSING)q(*string=&lt;nil&gt;)[ℹ] 1 nodegroup (workers) was included (based on the include/exclude rules)[ℹ] will create a CloudFormation stack for each of 1 managed nodegroups in cluster &quot;sandpit&quot;[ℹ] 1 task: &#123; 1 task: &#123; create managed nodegroup &quot;workers&quot; &#125; &#125;[ℹ] building managed nodegroup stack &quot;eksctl-sandpit-nodegroup-workers&quot;[ℹ] deploying stack &quot;eksctl-sandpit-nodegroup-workers&quot;[✔] created 0 nodegroup(s) in cluster &quot;sandpit&quot;[ℹ] nodegroup &quot;workers&quot; has 2 node(s)[ℹ] node &quot;ip-192-168-47-175.us-east-2.compute.internal&quot; is ready[ℹ] node &quot;ip-192-168-87-98.us-east-2.compute.internal&quot; is ready[ℹ] waiting for at least 2 node(s) to become ready in &quot;workers&quot;[ℹ] nodegroup &quot;workers&quot; has 2 node(s)[ℹ] node &quot;ip-192-168-47-175.us-east-2.compute.internal&quot; is ready[ℹ] node &quot;ip-192-168-87-98.us-east-2.compute.internal&quot; is ready[✔] created 1 managed nodegroup(s) in cluster &quot;sandpit&quot;[ℹ] checking security group configuration for all nodegroups[ℹ] all nodegroups have up-to-date configuration Kubernetes DashboardInstall Kubernetes Dashboard in Kubernetes cluster: 12345678910111213141516171819𝜆 kubectl get services --all-namespacesNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.100.0.1 &lt;none&gt; 443/TCP 70mkube-system kube-dns ClusterIP 10.100.0.10 &lt;none&gt; 53/UDP,53/TCP 70mkube-system metrics-server ClusterIP 10.100.142.106 &lt;none&gt; 443/TCP 14mkubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.100.91.78 &lt;none&gt; 8000/TCP 11mkubernetes-dashboard kubernetes-dashboard ClusterIP 10.100.75.0 &lt;none&gt; 443/TCP 11m𝜆 kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system aws-node-cnzrv 1/1 Running 0 40mkube-system aws-node-m9tjp 1/1 Running 0 40mkube-system coredns-7f5cccffc-h44mz 1/1 Running 0 65mkube-system coredns-7f5cccffc-hmx7g 1/1 Running 0 65mkube-system kube-proxy-7kn62 1/1 Running 0 40mkube-system kube-proxy-g57ph 1/1 Running 0 40mkube-system metrics-server-7fcf9cc98b-ftl4k 1/1 Running 0 14mkubernetes-dashboard dashboard-metrics-scraper-677768c755-mxlmc 1/1 Running 0 11mkubernetes-dashboard kubernetes-dashboard-995fd6fb4-xqcj5 1/1 Running 0 11m Connect Kubernetes Dashboard via proxy: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960𝜆 cat .kube/configapiVersion: v1clusters:- cluster: certificate-authority: /Users/terrence/.minikube/ca.crt server: https://192.168.99.100:8443 name: minikube- cluster: certificate-authority-data: LS0tLS1CRUd ... tLS0tLQo= server: https://0559DE89F43B8766B56C7FD066C6C50F.yl4.us-east-2.eks.amazonaws.com name: sandpit.us-east-2.eksctl.iocontexts:- context: cluster: sandpit.us-east-2.eksctl.io user: ADMMiaoT@sandpit.us-east-2.eksctl.io name: ADMMiaoT@sandpit.us-east-2.eksctl.io- context: cluster: minikube user: minikube name: minikubecurrent-context: ADMMiaoT@sandpit.us-east-2.eksctl.iokind: Configpreferences: &#123;&#125;users:- name: ADMMiaoT@sandpit.us-east-2.eksctl.io user: exec: apiVersion: client.authentication.k8s.io/v1alpha1 args: - token - -i - sandpit command: aws-iam-authenticator env: - name: AWS_PROFILE value: paradise-dev- name: minikube user: client-certificate: /Users/terrence/.minikube/client.crt client-key: /Users/terrence/.minikube/client.key𝜆 kubectl -n kube-system get secret | grep eks-admin | awk &#x27;&#123;print $1&#125;&#x27;eks-admin-token-s2gf5𝜆 kubectl -n kube-system describe secret eks-admin-token-s2gf5Name: eks-admin-token-s2gf5Namespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: eks-admin kubernetes.io/service-account.uid: fa3cf514-18bc-11ea-bbdd-0a4cd5e8dc70Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIs ... hpY8upQlA2q40g𝜆 kubectl proxy Visit URL http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#!/login Choose Token, paste the token output from the previous command into the Token field, and choose SIGN IN. With AWS managed nodes, on Node EC2 Instance: First Docker applicationDeploy first Docker application react-typescript, from Docker Hub https://hub.docker.com/r/jtech/react-typescript, into Kubernetes. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687𝜆 kubectl run react-typescript --image=jtech/react-typescript --port 3000kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.deployment.apps/react-typescript created𝜆 kubectl describe deploymentsName: react-typescriptNamespace: defaultCreationTimestamp: Mon, 09 Dec 2019 14:56:09 +1100Labels: run=react-typescriptAnnotations: deployment.kubernetes.io/revision: 1Selector: run=react-typescriptReplicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 25% max unavailable, 25% max surgePod Template: Labels: run=react-typescript Containers: react-typescript: Image: jtech/react-typescript Port: 3000/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Conditions: Type Status Reason ---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdatedOldReplicaSets: &lt;none&gt;NewReplicaSet: react-typescript-867c948446 (1/1 replicas created)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 71s deployment-controller Scaled up replica set react-typescript-867c948446 to 1𝜆 kubectl describe pods react-typescriptName: react-typescript-867c948446-qtvrpNamespace: defaultPriority: 2000001000PriorityClassName: system-node-criticalNode: fargate-ip-192-168-183-250.us-east-2.compute.internal/192.168.183.250Start Time: Mon, 09 Dec 2019 14:56:59 +1100Labels: eks.amazonaws.com/fargate-profile=fp-default pod-template-hash=867c948446 run=react-typescriptAnnotations: kubernetes.io/psp: eks.privilegedStatus: RunningIP: 192.168.183.250Controlled By: ReplicaSet/react-typescript-867c948446Containers: react-typescript: Container ID: containerd://2ea5f1ea4fb731eb844f0e267581e9e188d29ab7a639b7b8ca50c83cfb12b4c3 Image: jtech/react-typescript Image ID: docker.io/jtech/react-typescript@sha256:0951fe4d9a24390123c7aa23612c8cdf1d8191a9d8e7d3cbc8bb4d8d763e0ce5 Port: 3000/TCP Host Port: 0/TCP State: Running Started: Mon, 09 Dec 2019 14:57:28 +1100 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-knpqq (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled TrueVolumes: default-token-knpqq: Type: Secret (a volume populated by a Secret) SecretName: default-token-knpqq Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulling 76s kubelet, fargate-ip-192-168-183-250.us-east-2.compute.internal Pulling image &quot;jtech/react-typescript&quot; Normal Pulled 49s kubelet, fargate-ip-192-168-183-250.us-east-2.compute.internal Successfully pulled image &quot;jtech/react-typescript&quot; Normal Created 49s kubelet, fargate-ip-192-168-183-250.us-east-2.compute.internal Created container react-typescript Normal Started 49s kubelet, fargate-ip-192-168-183-250.us-east-2.compute.internal Started container react-typescript Expose service: 123456789101112131415161718192021222324252627𝜆 kubectl expose deployment react-typescript --type=&quot;NodePort&quot;service/react-typescript exposed𝜆 kubectl describe services react-typescriptName: react-typescriptNamespace: defaultLabels: run=react-typescriptAnnotations: &lt;none&gt;Selector: run=react-typescriptType: NodePortIP: 10.100.54.37Port: &lt;unset&gt; 3000/TCPTargetPort: 3000/TCPNodePort: &lt;unset&gt; 31799/TCPEndpoints: 192.168.183.250:3000Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt;𝜆 kubectl get services --all-namespacesNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.100.0.1 &lt;none&gt; 443/TCP 46hdefault react-typescript NodePort 10.100.54.37 &lt;none&gt; 3000:31799/TCP 4m55skube-system kube-dns ClusterIP 10.100.0.10 &lt;none&gt; 53/UDP,53/TCP 46hkube-system metrics-server ClusterIP 10.100.142.106 &lt;none&gt; 443/TCP 45hkubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.100.91.78 &lt;none&gt; 8000/TCP 45hkubernetes-dashboard kubernetes-dashboard ClusterIP 10.100.75.0 &lt;none&gt; 443/TCP 45h Run kubectl proxy and connect to react-typscript application on URL: http://localhost:8001/api/v1/namespaces/default/services/http:react-typescript:3000/proxy/ References eksctl, a simple CLI tool for creating clusters on Amazon’s new managed Kubernetes service for EC2 - EKS. Written in Go, uses CloudFormation, https://eksctl.io/ AWS EKS, https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html Tutorial: Deploy Kubernetes Dashboard, https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html","tags":[]},{"title":"AWS EKS for Fargate","date":"2019-12-04T23:26:58.000Z","path":"2019/12/05/AWS-EKS-with-Fargate/","text":"AWS EKSAfter AWS EKS for Fargate annouced in Re:Invent 2019 - Amazon EKS on AWS Fargate Now Generally Available, I have a quick spin. General configuration: Fargate profile configuration: Fargate roles: CustomEKSRole role has AmazonEKSClusterPolicy and AmazonEKSServicePolicy. CustomEKSFargatePodExecutionRole role has AmazonEKSFargatePodExecutionRolePolicy, and Trust relationships: 12345678910111213&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Sid&quot;: &quot;&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &#123; &quot;Service&quot;: &quot;eks-fargate-pods.amazonaws.com&quot; &#125;, &quot;Action&quot;: &quot;sts:AssumeRole&quot; &#125; ]&#125; CustomEKSWorkerNodeRole role has AmazonEKSWorkerNodePolicy, AmazonEKS_CNI_Policy, AmazonEC2ContainerRegistryReadOnly, and Trust relationships: 123456789101112&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &#123; &quot;Service&quot;: &quot;ec2.amazonaws.com&quot; &#125;, &quot;Action&quot;: &quot;sts:AssumeRole&quot; &#125; ]&#125; Namespace for Fargate profile Pod Selectors is default. Subnets for Fargate, including private subnets (subnet without Internet Gateway): References Getting Started with AWS Fargate on Amazon EKS, https://docs.aws.amazon.com/eks/latest/userguide/fargate-getting-started.html Amazon EKS Worker Node IAM Role, https://docs.aws.amazon.com/eks/latest/userguide/worker_node_IAM_role.html EKS + Fargate &#x3D; Extensibility of Kubernetes + Serverless Benefits, https://itnext.io/eks-fargate-extensibility-of-kubernetes-serverless-benefits-77599ac1763","tags":[]},{"title":"Why React?","date":"2019-12-01T02:40:56.000Z","path":"2019/12/01/Why-React/","text":"A new pedigree of Frontend Development. The following are the preferred technical choices for any new projects that do not have a “hammer whats already there“ constraint that have been chosen for consistency. Programming Language: TypeScript Alternatives to avoid: Raw JS &#x2F; Babel: Types can prevent a significant amount of bugs and provide a great means of documenting code that can be validated by a compiler. Therefore we will not dabble in untyped programming languages for Frontend development. Other TypeSafe alternatives: Dart &#x2F; Elm &#x2F; Blazor - typed compile to JavaScript languages: Browsers operate on JavaScript using a language that is too far removed from JS means people will need to learn two language and fix bugs at two levels. Flow - A facebook alternative to TypeScript. Flow was a decent experiment into JavaScript type safety by the Facebook team, but news from inside Facebook (chats at conferences) is that it is not actively getting maintained. Therefore it should be avoided. UI Library: ReactAlternatives to avoid: Angular: Angular goes through significant breaking changes as can be witnessed by the changelog. Also Angular is a full framework (as opposed to a library), but people rarely use it as a framework as there will be inevitable external dependencies for alternative (even better) community tools e.g. rxjs for state, axios for api etc. Vue: Vue is the third most popular choice. However it not backed by a major enterprise (Facebook for React, Google for Angular) Svelte: Same reasons as Vue. State Management: MobXAlternatives to avoid: Redux: Redux is definitely the more popular choice in the React community. However most people will use it with some additional middleware library e.g. redux-thunk or redux-saga leading to multiple ways of doing Frontend. Also, Redux is significantly more verbose (because of always new object creation) and difficult to optimise (because of no internal knowledge of what changed). Styles: CSS in JSPrefer Emotion or TypeStyle over others. Alternatives to avoid: CSS preprocessor (e.g. sass &#x2F; css-modules). For CSS in JS there is an excellent presentation that you should checkout. Key reasons are clear code co-location and management practices for CSS, the same way React does it for Html (with JSX). End to End Testing : CypressCypress for E2E testing. Alternatives to avoid: Selenium based solutions (e.g. Protractor &#x2F; Nightwatch). Selenium operates over a wire transfer protocol that makes it impossible to provide many of the debugging features provided by Cypress. Cypress operates in process with the tests making automatic retries with greater flake resistance and a significantly improved debugging experience. HTTP RequestsPrefer axios due to is built in TypeScript definitions, great browser support and community guidance. Alternatives to avoid: fetch: Needs polyfill and needs explicit calls for json de-serialization.","tags":[]},{"title":"A PC person's guide to Mac","date":"2019-11-28T03:24:25.000Z","path":"2019/11/28/A-PC-person-s-guide-to-Mac/","text":"PreludeThis is a re-posted article I published on May 19th, 2012, long long time ago on the currently already died Google Plus. Bought a lowest entry level Macbook Pro 13” i5 refurbished during the week. This is the first ever Apple product I bought. I’m not a fan boy of sleeky, popular iPhone. I keep poo-pooing iPad. But because of the admiration of MBP’s impeccable design and high quality, moreover as a tribute to Steve Jobs, I join the cohort of yuppies, with a silver MPB on my laptop. Macbook Pro package comes without Mac OS X Lion installation DVD. You have to re-install OS from internet. Impeccable design and high quality product, even the refurbished one. However, as a typical PC person living a life of hacker. There should no boundary, no limitation, no rules can’t be broken. In order to maximise all the potentials inside this piece of hardware, and get my investment back, I also ordered 16GB DDR3 RAM, 240GB SATA3 SSD this week from two computer parts shops, at the best price I can find on the market. After some tweak to get rid of the bottleneck in the overall performance, finally this baby is muscled up. Refurbished MBP, $1189.00 paid on credit card with free shipping. 16GB Patriot DDR3 RAM for Mac, $150 on cash. Corsair Force SATA3 240GB SSD, $295 on cash. The happiness and achievement to get something done after a long time planning, researching, waiting, torture and agony, priceless. There is something money can’t buy. For everything else, there is a Mastercard. And you have to work harder to earn your dole and make this happen. Totally under 1700 dollars investment will keep me happy for a long time. I reckon it will be good money spent on. If you see the used to be memory hogging applications like M$ Office and Eclipse , suddenly they all jump on your face when you click and open them, the blissful feeling is hardly to describe in a single word, especially you need reward and motivate yourself if you were kept in the shity work. Lifespan? Certainly not. Longevity? Probably not. However, what else you can do if you don’t hack it? Need to change my PC mindset finally. Keep reminding me “Don’t panic, you are using a Mac.”","tags":[]},{"title":"Run ssh and scp with AWS Session Manager","date":"2019-11-22T01:09:05.000Z","path":"2019/11/22/Run-ssh-and-scp-with-AWS-Session-Manager/","text":"AWS Session ManagerNew AWS Systems Manager, including Session Manager is another step enhance security on Cloud. Here are step by step how to set up. NOTE: There is NO need to require to have a Public IP on EC2 instance, and have network inbound rule setup with opened SSH port 22, and VPN connection. You have ec2-user account on AWS EC2 instance. On localhost: 1234567891011121314151617181920212223242526272829𝜆 cat .aws/config[default]output = jsonregion = ap-southeast-2[session]output = jsonregion = ap-southeast-2𝜆 cat .aws/credentials[default]aws_access_key_id = ASIANPOWERHOUSEBLAHBLAHaws_secret_access_key = aGLMountainLionPIEXL0UK0TunalNB61Kt+GuavaVm4tAD[session]aws_access_key_id = ASIANPOWERHOUSEBLAHBLAHaws_secret_access_key = aGLMountainLionPIEXL0UK0TunalNB61Kt+GuavaVm4tADaws_session_token = FwoGZXIvYXdzEB8aDDuzeYcE5QDFo0 ... z2h/Px7nUMEWaZOZZw==aws_expiration=2019-11-22T20:33:18.000Z𝜆 ssh -i .ssh/aws-key.pem -l ec2-user ec2-3-121-69-96.ap-southeast-2.compute.amazonaws.comLast login: Fri Nov 22 01:17:33 2019 from 155.144.114.41 __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___|https://aws.amazon.com/amazon-linux-2/ NOTE: Need ROOT access key pair ASIANPOWERHOUSEBLAHBLAH above setup in session profile to run aws sts command. NOTE: IAM role for EC2 instance need to have AmazonSSMManagedInstanceCore policy. So create a customised role CustomAmazonSSMManagedInstanceCore in AWS IAM including AmazonSSMManagedInstanceCore policy, and bind this IAM role with EC2 instance, also with security group and key pair. Install all the dependencies latest Systems Manager Agent on your EC2 instance; enabled “Agent auto update” under Managed Instances in AWS Systems Manager latest AWS CLI on localhost latest Session Manager Plugin on localhost, https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html Add customised RunAs users via “Run Command” To elevated SSM_pwr_user, a customised user to allow login EC2 instance, with command: 12𝜆 useradd -g wheel SSM_pwr_user𝜆 echo &quot;SSM_pwr_user ALL=(ALL) NOPASSWD:ALL&quot; &gt; /etc/sudoers.d/SSM_pwr_user Updated above solution and add a DIFFERENT user ssm-user, if SSM throws error “Unable to start shell: failed to start pty since RunAs user ssm-user does not exist“: 12𝜆 useradd -g wheel ssm-user𝜆 echo &quot;ssm-user ALL=(ALL) NOPASSWD:ALL&quot; &gt; /etc/sudoers.d/ssm-agent-users Update SSH config file on localhost to proxy commands through the AWS Session Manager for any EC2 instance id 12345𝜆 cat .ssh/config# SSH over Session Managerhost i-* mi-* ProxyCommand sh -c &quot;aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters &#x27;portNumber=%p&#x27;&quot; Generate session token 123456789101112𝜆 dateThu 21 Nov 2019 04:33:03 UTC𝜆 aws sts get-session-token --duration-seconds 129600 --profile session&#123; &quot;Credentials&quot;: &#123; &quot;SecretAccessKey&quot;: &quot;I7brpY8XWDWYwwyUdp5PLq7cxpskuMSHyBtPjPNE&quot;, &quot;SessionToken&quot;: &quot;FwoGZXIvYXdzENL//////////wEaDKKdWTVmCrwKRiMbOSKCAbkQr ... YiNntciJszsZVRypXz1HTfa3gbcKoNXHon8=&quot;, &quot;Expiration&quot;: &quot;2019-11-22T16:33:12Z&quot;, &quot;AccessKeyId&quot;: &quot;ASIASZPQ3TMDVJVIGM7H&quot; &#125;&#125; If AWS user ec2-user has MFA enabled, generate session token like this: 12𝜆 aws sts get-session-token --duration-seconds 129600 --profile session \\ --serial-number arn:aws:iam::910218657901234:mfa/root-account-mfa-device --token-code 251556 Add session token in AWS credentials file on localhost, and test 12345678910111213141516𝜆 cat .aws/credentials[default]aws_access_key_id = ASIANPOWERHOUSEBLAHBLAHaws_secret_access_key = aGLMountainLionPIEXL0UK0TunalNB61Kt+GuavaVm4tAD[session]aws_access_key_id = ASIASZPQ3TMDVJVIGM7Haws_secret_access_key = I7brpY8XWDWYwwyUdp5PLq7cxpskuMSHyBtPjPNEaws_session_token = FwoGZXIvYXdzENL//////////wEaDKKdWTVmCrwKRiMbOSKCAbkQr ... YiNntciJszsZVRypXz1HTfa3gbcKoNXHon8=aws_expiration=2019-11-22T20:33:18.000Z𝜆 aws ssm start-session --target i-e2f189dashfdf65weqfwda2 --profile sessionStarting session with SessionId: ec2-user-094c34172bdc6fc22sh-4.2$ whoamiSSM_pwr_user Run Session Manager commands 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152𝜆 aws ssm send-command --instance-ids i-e2f189dashfdf65weqfwda2 --document-name AWS-RunShellScript --comment &quot;IP config&quot; --parameters commands=ifconfig --output text \\ --profile sessionCOMMAND 39e80533-376e-46fa-bb11-8daf040fe80f IP config 0 0 AWS-RunShellScript 0 1574377262.9 50 0 1574370062.9 Pending Pending 1CLOUDWATCHOUTPUTCONFIG FalseINSTANCEIDS i-e2f189dashfdf65weqfwda2NOTIFICATIONCONFIGCOMMANDS ifconfig𝜆 aws ssm list-command-invocations --command-id 39e80533-376e-46fa-bb11-8daf040fe80f --details --profile session&#123; &quot;CommandInvocations&quot;: [ &#123; &quot;Comment&quot;: &quot;IP config&quot;, &quot;Status&quot;: &quot;Success&quot;, &quot;CommandPlugins&quot;: [ &#123; &quot;Status&quot;: &quot;Success&quot;, &quot;ResponseStartDateTime&quot;: 1574370063.609, &quot;StandardErrorUrl&quot;: &quot;&quot;, &quot;OutputS3BucketName&quot;: &quot;&quot;, &quot;OutputS3Region&quot;: &quot;ap-southeast-2&quot;, &quot;OutputS3KeyPrefix&quot;: &quot;&quot;, &quot;ResponseCode&quot;: 0, &quot;Output&quot;: &quot;eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 9001\\n inet 172.31.9.155 netmask 255.255.240.0 broadcast 172.31.15.255\\n inet6 fe80::69:c9ff:fe76:91ae prefixlen 64 scopeid 0x20&lt;link&gt;\\n ether 02:69:c9:76:91:ae txqueuelen 1000 (Ethernet)\\n RX packets 60339 bytes 17254584 (16.4 MiB)\\n RX errors 0 dropped 0 overruns 0 frame 0\\n TX packets 56971 bytes 13112880 (12.5 MiB)\\n TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\\n\\nlo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536\\n inet 127.0.0.1 netmask 255.0.0.0\\n inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt;\\n loop txqueuelen 1000 (Local Loopback)\\n RX packets 0 bytes 0 (0.0 B)\\n RX errors 0 dropped 0 overruns 0 frame 0\\n TX packets 0 bytes 0 (0.0 B)\\n TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\\n\\n&quot;, &quot;ResponseFinishDateTime&quot;: 1574370063.634, &quot;StatusDetails&quot;: &quot;Success&quot;, &quot;StandardOutputUrl&quot;: &quot;&quot;, &quot;Name&quot;: &quot;aws:runShellScript&quot; &#125; ], &quot;ServiceRole&quot;: &quot;&quot;, &quot;CloudWatchOutputConfig&quot;: &#123; &quot;CloudWatchLogGroupName&quot;: &quot;&quot;, &quot;CloudWatchOutputEnabled&quot;: false &#125;, &quot;InstanceId&quot;: &quot;i-e2f189dashfdf65weqfwda2&quot;, &quot;DocumentName&quot;: &quot;AWS-RunShellScript&quot;, &quot;NotificationConfig&quot;: &#123; &quot;NotificationArn&quot;: &quot;&quot;, &quot;NotificationEvents&quot;: [], &quot;NotificationType&quot;: &quot;&quot; &#125;, &quot;DocumentVersion&quot;: &quot;&quot;, &quot;StatusDetails&quot;: &quot;Success&quot;, &quot;StandardOutputUrl&quot;: &quot;&quot;, &quot;StandardErrorUrl&quot;: &quot;&quot;, &quot;InstanceName&quot;: &quot;&quot;, &quot;CommandId&quot;: &quot;39e80533-376e-46fa-bb11-8daf040fe80f&quot;, &quot;RequestedDateTime&quot;: 1574370062.995 &#125; ]&#125; Open a connection forwarding session to a remote port 12345𝜆 aws ssm start-session --target i-e2f189dashfdf65weqfwda2 --document-name AWS-StartPortForwardingSessionToRemoteHost \\ --parameters &#x27;&#123;&quot;portNumber&quot;:[&quot;3306&quot;],&quot;localPortNumber&quot;:[&quot;13306&quot;],&quot;host&quot;:[&quot;remote-mysql-host-name&quot;]&#125;&#x27; \\ --profile session𝜆 mysql -h 127.0.0.1 --port 13306 -u admin -p Monitor AWS Session Manager log /var/log/amazon/ssm/amazon-ssm-agent.log on EC2 instance Test ssh command with session token 12345678𝜆 AWS_PROFILE=session ssh -i .ssh/aws-key.pem -l ec2-user i-e2f189dashfdf65weqfwda2Last login: Fri Nov 22 00:19:32 2019 from localhost __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___|https://aws.amazon.com/amazon-linux-2/ Test scp command with session token 12𝜆 AWS_PROFILE=session scp -i .ssh/aws-key.pem /tmp/stack-overflow.log ec2-user@i-e2f189dashfdf65weqfwda2:/tmpstack-overflow.log 100% 67KB 437.0KB/s 00:00 Without using scp, transferring files directly is not possible with the AWS Session Manager. You should use S3 bucket and the AWS CLI to exchange data. Test ssh tunnel 1𝜆 AWS_PROFILE=session ssh -i .ssh/aws-key.pem -L 443:www.google.com:443 -l ec2-user@i-e2f189dashfdf65weqfwda2 OKTASet up integrated OKTA authentication with session. Have you OKTA AWS CLI installed at first, and configure it: 12345678910111213141516171819𝜆 cat ~/.okta-aws[default]base-url = hello.paradise.org## The remaining parameters are optional.## You will be prompted for them, if they&#x27;re not included here.username = terrence.miao@paradise.org# Current choices are: GOOGLE or OKTAfactor = OKTA# AWS role name (match one of the options prompted for by &quot;Please select the AWS role&quot; when this parameter is not specifiedrole =# Found in Okta&#x27;s configuration for your AWS account.app-link = https://hello.paradise.org/home/amazon_aws/0oa1ch3l6/272# duration in seconds to request a session token for, make sure your accounts (both AWS itself and the associated okta application) allow for large durations. default: 3600duration = 28800 Create session: 123456789101112131415𝜆 bin/okta/okta --okta-profile default --force --profile sessionEnter password:Multi-factor Authentication required.Pick a factor:[ 0 ] Okta Verify App: SmartPhone_Android: ONEPLUS A5010[ 1 ] token:software:totp( OKTA ) : terrence.miao@paradise.orgSelection: 01: arn:aws:iam::994385754915:role/federation/DeveloperPowerUser2: arn:aws:iam::354184710243:role/federation/DeveloperPowerUser3: arn:aws:iam::855034721059:role/federation/DeveloperPowerUser4: arn:aws:iam::944986439712:role/federation/DeveloperPowerUser5: arn:aws:iam::944986439712:role/federation/Operations6: arn:aws:iam::015951833499:role/federation/DeveloperPowerUserPlease select the AWS role: 3Session token expires on: 2019-12-05 06:30:18+00:00 AzureSet up integrated Azure authentication with session. Have you AWS Azure CLI installed at first, and configure it: 12345678910111213141516𝜆 cat ~/.aws/config[profile session]output=jsonregion=ap-southeast-2azure_tenant_id=paradise.orgazure_app_id_uri=dac7d48b-7f45-47bb-bdae-d3c4f8351839azure_default_username=terrence.miao@paradise.orgazure_default_role_arn=arn:aws:iam::994385754915:role/federation/DeveloperPowerUserazure_default_duration_hours=12azure_default_remember_me=false[profile session-cicd]output=jsonregion=ap-southeast-2role_arn=arn:aws:iam::994385754915:role/ap-sc-iam-cicd-ap-southeast-2source_profile=session Create session: 1234567891011𝜆 bin/aws-azure-cli/aws-azure-cli --profile session --mode guiLogging in with profile &#x27;session&#x27;...Using AWS SAML endpoint https://signin.aws.amazon.com/samlLooking in /Applications/Google Chrome.app/Contents/MacOS/Google ChromeFound browser in /Applications/Google Chrome.app/Contents/MacOS/Google ChromePlease complete the login in the opened window? Role: arn:aws:iam::994385754915:role/federation/DeveloperPowerUser? Session Duration Hours (up to 12): 12Assuming role arn:aws:iam::994385754915:role/federation/DeveloperPowerUserRequesting session duration 43200sSession expires Sun Feb 19 2023 06:46:43 GMT+1100 (Australian Eastern Daylight Time) Login EC2 instance, overwritting default ap-southeast-2 region: 12𝜆 aws ssm start-session --target i-04ee902e33625c4f3 --profile session --region us-east-2 --debugStarting session with SessionId: terrence.miao@paradise.org-05411f2e2d5a58b0b Attach a new key pair to EC2 instance by: 12𝜆 ssh-keygen -y -f .ssh/aws-key.pemssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCL ... asasf2qcvASEFWF Find out EC2 Instance ID by querying Instance Name: 12345𝜆 aws ec2 describe-instances --profile session \\ --filter &quot;Name=tag:Name,Values=EC2Stack-Apptier/EC2&quot; \\ --query &quot;Reservations[].Instances[?State.Name == &#x27;running&#x27;].InstanceId[]&quot; \\ --output texti-04ee902e33625c4f3 References Troubleshooting Systems Manager Run Command, https://docs.aws.amazon.com/systems-manager/latest/userguide/troubleshooting-remote-commands.html#systems-manager-ssm-agent-log-files Provides Okta authentication for awscli, https://github.com/jmhale/okta-awscli How to add a new key pair to your exisitng AWS ec2 Instances, https://www.how2shout.com/linux/add-a-new-key-pair-to-your-exisitng-aws-ec2-instances/ Use port forwarding in AWS Systems Manager Session Manager to connect to remote hosts, https://aws.amazon.com/blogs/mt/use-port-forwarding-in-aws-systems-manager-session-manager-to-connect-to-remote-hosts/","tags":[]},{"title":"A simple approach to export / import AWS DynamoDB table items","date":"2019-04-23T08:59:29.000Z","path":"2019/04/23/A-simple-approach-to-export-import-AWS-DynamoDB-table-items/","text":"Try to export items from AWS Test environment DynamoDB tables into Production. A simple Bash SHELL script, with a few commands and AWS CLI could do the work. In AWS DynamoDB console, export selected items into .csv file like this: 1234567𝜆 cat dws-tile-publisher-ptest-tile.csv&quot;id (S)&quot;,&quot;available_to (SS)&quot;,&quot;benefit_type (S)&quot;,&quot;category_id (S)&quot;,&quot;created_date (S)&quot;,&quot;created_user (S)&quot;,&quot;description (S)&quot;,&quot;image_bucket (S)&quot;,&quot;image_key (S)&quot;,&quot;last_modified_date (S)&quot;,&quot;last_modified_user (S)&quot;,&quot;status (S)&quot;,&quot;sub_category_id (S)&quot;,&quot;title (S)&quot;,&quot;valid_from (S)&quot;,&quot;valid_to (S)&quot;,&quot;claim_url (S)&quot;,&quot;discount_code (S)&quot;&quot;950c529b-d6ae-472b-b44a-510ec201c167&quot;,&quot;&#123; &quot;&quot;0b1b9ed4-e171-4eaf-9d20-1a870ab7cc7c&quot;&quot;, &quot;&quot;2df9daf4-da50-4ff9-9322-969d02c178f4&quot;&quot;, &quot;&quot;b6fdcdc7-ffe6-4007-ae56-f5ab545768ec&quot;&quot;, &quot;&quot;bc1c7285-ac47-4d06-b4c6-6f3780cbfb3f&quot;&quot; &#125; &quot;,&quot;DISCOUNT&quot;,&quot;8d27fb2d-c5f8-487a-a869-2ffd0e476eb6&quot;,&quot;2019-04-23T01:02:18.123&quot;,&quot;Angelo.Woods@test.npe.paradise.org&quot;,&quot;Testing descriptions&quot;,&quot;dws-tile-images-ptest&quot;,&quot;Samsung-Galaxy-10-Leak.jpg&quot;,&quot;2019-04-23T01:02:18.125&quot;,&quot;Angelo.Woods@test.npe.paradise.org&quot;,&quot;ARCHIVED&quot;,&quot;6936a6ed-fa97-4439-9129-c43e288011b3&quot;,&quot;Samsung Galaxy 10&quot;,&quot;2019-03-24T01:00&quot;,&quot;2021-02-25T23:00&quot;,&quot;http://www.google.com&quot;,...&quot;095cc9b4-fd64-4479-8817-0b35b8ddcbc2&quot;,&quot;&#123; &quot;&quot;0b1b9ed4-e171-4eaf-9d20-1a870ab7cc7c&quot;&quot;, &quot;&quot;2df9daf4-da50-4ff9-9322-969d02c178f4&quot;&quot;, &quot;&quot;b6fdcdc7-ffe6-4007-ae56-f5ab545768ec&quot;&quot;, &quot;&quot;bc1c7285-ac47-4d06-b4c6-6f3780cbfb3f&quot;&quot; &#125; &quot;,&quot;DISCOUNT&quot;,&quot;1f028ecb-4115-4378-b00e-3fcd4cbdf54d&quot;,&quot;2019-04-23T03:50:31.226&quot;,&quot;Angelo.Woods@test.npe.paradise.org&quot;,&quot;&lt;div&gt;Employees now have access to an exclusive member offers portal through the Samsung Employee Purchase program. Get up to 35% off the RRP across a wide range of products including mobile phones, tablets, televisions, refrigerators, washing machines, monitors and more.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&quot;,&quot;dws-tile-images-ptest&quot;,&quot;samsung.jpg&quot;,&quot;2019-04-23T03:50:31.228&quot;,&quot;Angelo.Woods@test.npe.paradise.org&quot;,&quot;PUBLISHED&quot;,&quot;ce07c19b-dd87-4890-9ad3-6acfe0998496&quot;,&quot;Samsung&quot;,&quot;2019-04-17T01:00&quot;,&quot;2025-01-01T23:00&quot;,&quot;https://shop.samsung.com/au/multistore/auepp/austpost_au/&quot;,&quot;Simply go to link provided to access the site. Browse our categories, add your products to cart and checkout. Happy Shopping! &quot; Generate a key:value hashtable used later. Key is “id” column, value is “image_key” colomn: 123456𝜆 awk -F&quot;\\&quot;,\\&quot;&quot; &#x27;&#123;print $1 &quot;:&quot; $9 &quot;\\&quot;&quot;&#125;&#x27; dws-tile-publisher-test-tile.csv | sed &#x27;s/jpg/json/&#x27; | sed &#x27;s/png/json/&#x27; | sed -n &#x27;1d;p&#x27;&quot;950c529b-d6ae-472b-b44a-510ec201c167:Samsung-Galaxy-10-Leak.json&quot;...&quot;095cc9b4-fd64-4479-8817-0b35b8ddcbc2:samsung.json&quot; Run the bash script and retrieve item from AWS DyanmoDB table, tranform it and output into a JSON file for import into AWS Production production table: A generated JSON file looks like this: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859𝜆 cat Samsung-Galaxy-10-Leak.json&#123; &quot;status&quot;: &#123; &quot;S&quot;: &quot;PUBLISHED&quot; &#125;, &quot;valid_from&quot;: &#123; &quot;S&quot;: &quot;2019-03-24T01:00&quot; &#125;, &quot;sub_category_id&quot;: &#123; &quot;S&quot;: &quot;6936a6ed-fa97-4439-9129-c43e288011b3&quot; &#125;, &quot;description&quot;: &#123; &quot;S&quot;: &quot;Testing descriptions&quot; &#125;, &quot;image_key&quot;: &#123; &quot;S&quot;: &quot;Samsung-Galaxy-10-Leak.jpg&quot; &#125;, &quot;claim_url&quot;: &#123; &quot;S&quot;: &quot;http://www.google.com&quot; &#125;, &quot;last_modified_date&quot;: &#123; &quot;S&quot;: &quot;2019-04-23T01:02:18.125&quot; &#125;, &quot;valid_to&quot;: &#123; &quot;S&quot;: &quot;2021-02-25T23:00&quot; &#125;, &quot;created_user&quot;: &#123; &quot;S&quot;: &quot;Noel.French@paradise.org&quot; &#125;, &quot;benefit_type&quot;: &#123; &quot;S&quot;: &quot;DISCOUNT&quot; &#125;, &quot;last_modified_user&quot;: &#123; &quot;S&quot;: &quot;Noel.French@paradise.org&quot; &#125;, &quot;created_date&quot;: &#123; &quot;S&quot;: &quot;2019-04-23T01:02:18.123&quot; &#125;, &quot;title&quot;: &#123; &quot;S&quot;: &quot;Samsung Galaxy 10&quot; &#125;, &quot;category_id&quot;: &#123; &quot;S&quot;: &quot;8d27fb2d-c5f8-487a-a869-2ffd0e476eb6&quot; &#125;, &quot;image_bucket&quot;: &#123; &quot;S&quot;: &quot;dws-tile-images-prod&quot; &#125;, &quot;id&quot;: &#123; &quot;S&quot;: &quot;950c529b-d6ae-472b-b44a-510ec201c167&quot; &#125;, &quot;available_to&quot;: &#123; &quot;SS&quot;: [ &quot;0b1b9ed4-e171-4eaf-9d20-1a870ab7cc7c&quot;, &quot;2df9daf4-da50-4ff9-9322-969d02c178f4&quot;, &quot;b6fdcdc7-ffe6-4007-ae56-f5ab545768ec&quot;, &quot;bc1c7285-ac47-4d06-b4c6-6f3780cbfb3f&quot; ] &#125;&#125; Now run the bash script and import JSON file into AWS Production DynamoDB table:","tags":[]},{"title":"Keep running","date":"2019-01-11T00:27:56.000Z","path":"2019/01/11/Keep-running/","text":"许多杰出的人物都是长跑爱好者，比如阿兰·图灵。 图灵的头衔有很多，传奇性的数学家、逻辑学家、现代计算机之父。但很少人知道他也是一位出色的跑者，甚至差点站上了奥运会的舞台。 图灵从中学就开始跑步，但直到 30 多岁才正式进行长跑训练。 跑道上的图灵“大器晚成”，出色的运动天赋丝毫不逊色于他的头脑。 14 岁那年，他在谢伯恩寄宿高中就读的第一日，由于大罢工，当地的公共交通设施全部停运。图灵没有等到交通恢复，就从南安普敦一口气骑行 96 公里到达位于多塞特郡西北的学校上学，引发全校轰动。 1931 年，当他进入剑桥国王学院学习时，他曾多次在学校和伊利镇之间进行折返跑，一个来回就差不多 50 公里。 1947 年，在莱斯特郡拉夫堡（Loughborough）大学体育场举行的英国业余田径协会马拉松锦标赛上，图灵跑出了他在马拉松赛中的个人最好成绩 2 小时 46 分 03 秒，在那场比赛中名列第五。 在 1948 年伦敦的奥运会马拉松比赛上，当时冠军的成绩是 2 小时 34 分 52 秒。图灵仅比金牌得主慢了 11 分钟。而据英国《泰晤士报》报道，图灵的最好成绩在当年（1947年）足以排名世界前三。 When Alan Turing was asked why he punished himself so much in training. He said “I have such a stressful job that the only way I can get it out of my mind is by running hard; its the only way I can get some release.” 奥斯卡最佳改编剧本电影《模仿游戏》The Imitation Game 是根据他的传记《Alan Turing: The Enigma》拍摄的。 苹果公司的商标一度被误认为是源于图灵自杀时咬下的苹果，但该图案的设计师和苹果公司都否认这一说法。 Stephen Fry said on the BBC show QI XL in 2011 that his friend Steve Jobs said of the Turing tale, “It isn’t true, but God we wish it were!” 乔布斯被英国广播公司电视节目《QI》主持人史蒂芬·弗莱问到此事时说：“这（LOGO 向图灵致敬）不是真的，但是上帝啊，我们希望它是真的。” Keep running.","tags":[]},{"title":"Add final and javadoc for auto-generated getters and setters in IntelliJ","date":"2019-01-10T23:29:06.000Z","path":"2019/01/11/Add-final-and-javadoc-for-auto-generated-getters-and-setters-in-IntelliJ/","text":"Default final modifierTo satisfy Checkstyle setup. In IntelliJ (version 2018.3.2) Preferences | Editor &gt; Code Style &gt; Java, select “Code Generation” tab, check “Make generated parameters final” javadoc for auto-generated getters and settersCreate customised Getter Template: 123456789101112131415161718/*** Gets $field.name.** @return value of $field.name*/#if($field.modifierStatic)static ###end$field.type ###set($name = $StringUtil.capitalizeWithJavaBeanConvention($StringUtil.sanitizeJavaIdentifier($helper.getPropertyName($field, $project))))#if ($field.boolean &amp;&amp; $field.primitive) is###else get###end$&#123;name&#125;() &#123; return $field.name;&#125; Create customised Setter Template: 12345678910111213141516171819/*** Sets $field.name.** @param $field.name value of $field.name*/#set($paramName = $helper.getParamName($field, $project))#if($field.modifierStatic)static ###endvoid set$StringUtil.capitalizeWithJavaBeanConvention($StringUtil.sanitizeJavaIdentifier($helper.getPropertyName($field, $project)))($field.type $paramName) &#123;#if ($field.name == $paramName) #if (!$field.modifierStatic) this.## #else $classname.## #end#end$field.name = $paramName;&#125; Use the customised Getter and Setter templates when automatically generate getters and setters:","tags":[]},{"title":"Year 2018 in the Rear View Mirror","date":"2018-12-31T13:46:43.000Z","path":"2018/12/31/Year-2018-in-the-Rear-View-Mirror/","text":"2018 是一个让人敬畏恐惧“逢八必变”的年份。2018 是过去十年中最糟糕的一年，但也许是未来十年里最好的一年。 2018 年过去了，我很怀念它。 图 1：2018 年是在过去十年中唯一的各项投资都为负增长的一年。 图2：美国股市进入下半年一直跌跌不休。但在 Boxing Day 却创造了单日涨幅的纪录。 图 3：黎曼 Zeta 函数，用于计算素数分布。2018 年多人声称证明了百年未解决的黎曼猜想，但最终都是闹剧收场。黎曼猜想依然有如最为壮美，巍峨，险峻的奇山，矗立在人类的智力巅峰之上。 图 4：The cost of living，city by city。生活有多贵？中国六城市入选全球生活成本最昂贵的 top 25 list. 图 5：肯尼亚的 Eliud Kipchoge 在 2018 年柏林马拉松上创造的 2 小时 1 分 39 秒的马拉松世界纪录。人类仍然在试图突破 2 小时完成马拉松的运动极限。 图 6：中国·上海 图 7：Chinese President Xi Jinping,centre, arrives with Premier Li Keqiang, left, for the opening session of the Chinese People’s Political Consultative Conference in Beijing’s Great Hall of the People on March 3, 2018. 图 8：Chinese President Xi Jinping Speech. 图 9：在 2018 年 Pantone Colour of the year，Ultra Violet 紫外光后，潘通子给出 2019 年度流行色 Living Coral 珊瑚橙。 图 10：2018 年度汉字，由 “穷” 和 “丑” 合体所组成，读音是 “qiou”（音同糗），即又穷又丑的意思。不少网友表示，这字应该念 “wo”（我）。 图 11：电影《甲方乙方》的结局。杨立新走到雪地里，抬起头，和每个普通人都一样，要继续面对风雪。屋子里面是红灯笼，亮堂堂的。外面已经是冷色调了。外面就是现实的生活。 “我很怀念它”，经历越多，这句话带来的感触就越厚重。 时间，梦想和爱，都是如此。","tags":[]},{"title":"Step by step run Kafka on Mac","date":"2018-12-10T04:45:51.000Z","path":"2018/12/10/Step-by-step-run-Kafka-on-Mac/","text":"Download Kafka binary file from https://kafka.apache.org/downloads and install it Set up Kafka listen on PLAINTEXT:&#x2F;&#x2F;localhost:9092 123456terrence@igloo /usr/local/kafka15:51:25 𝜆 diff config/server.properties config/server.properties.orig31c31&lt; listeners=PLAINTEXT://localhost:9092---&gt; #listeners=PLAINTEXT://:9092 Start Zookeeper 1igloo:kafka root# bin/zookeeper-server-start.sh config/zookeeper.properties Start Kafka 1igloo:kafka root# bin/kafka-server-start.sh config/server.properties Create a new Kafka topic named “test” 1igloo:kafka root# bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test Initialise Producer in Console 12345terrence@igloo /usr/local/kafka_2.12-2.1.016:04:19 𝜆 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test&gt;Hello World&gt;Wow&gt; Initialize Consumer in Console to receive the messages 1234terrence@igloo /usr/local/kafka_2.12-2.1.016:06:11 𝜆 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningHello WorldWow Run Kafka Manager, an UI tool for managing Kafka. 12terrence@igloo ~/bin/kafka-manager-1.3.3.1816:16:47 𝜆 env ZK_HOSTS=&quot;localhost:2181&quot; bin/kafka-manager Then visit http://localhost:9000 Create a Kafka cluster at first: Visit “test” topic in Kafka cluster:","tags":[]},{"title":"This is how I feel when my code runs without a single bug","date":"2018-12-06T21:35:50.000Z","path":"2018/12/07/This-is-how-I-feel-when-my-code-runs-without-a-single-bug/","text":"","tags":[]},{"title":"Bind customised KeyStore,TrustStore and PoolingHttpClientConnectionManager in Apache Camel","date":"2018-12-03T07:54:16.000Z","path":"2018/12/03/Bind-customised-KeyStore-TrustStore-and-PoolingHttpClientConnectionManager-in-Apache-Camel/","text":"In Apache HTTPClient 4.x, PoolingHttpClientConnectionManager and BasicHttpClientConnectionManager can be used as Connection Managers for HTTP Client. By default, above two Connection Managers load cacerts trust store, from $JRE_HOME&#x2F;lib&#x2F;security directory, not programmatically specified. Since Apache Camel starts supporting HTTP4 Component, camel-http4 is using Apache HTTPClient 4.x. The following is a solution how to customise PoolingHttpClientConnectionManager and SSLContext to use specified KeyStore and TrustStore. In application.properties file defined environment specified KeyStore and TrustStore: 123456...keystore.location=classpath:certificates/client-certs.jkskeystore.password=client123truststore.location=classpath:certificates/trusted-cacerts.jkstruststore.password=changeit... Without Connection Managers like PoolingHttpClientConnectionManager and BasicHttpClientConnectionManager in Apache HTTPClient 4.x, Apache Camel Route can be written in this way, binding programmatically specified KeyStore and TrustStore. References Apache Camel HTTP4 Component, http://camel.apache.org/http4.html Java SSL with Multiple KeyStores, http://codyaray.com/2013/04/java-ssl-with-multiple-keystores Multiple Keystores in Apache Camel&#x2F;HttpClient, http://codyaray.com/2013/05/multiple-keystores-in-apache-camelhttpclient","tags":[]},{"title":"Inspirational Thought","date":"2018-11-21T22:56:10.000Z","path":"2018/11/22/Inspirational-Thought/","text":"Inspirational Thought from Jeff Bezos.","tags":[]},{"title":"After new Macbook Air, Mac Mini, iPad Pro 2018","date":"2018-11-06T01:02:29.000Z","path":"2018/11/06/After-new-Macbook-Air-Mac-Mini-iPad-Pro-2018/","text":"最新的苹果发布会是一次团结的大会，求实的大会，胜利的大会。在以苹果总书记库克同志为核心的领导下，Apple Design 拨乱反正，继往开来，重拾经典。 Macbook Air 得以继续，Mac Mini 得到重生。iPad Pro 在经过不断的探索、失败后，终于拿出一款震撼心灵的设计，以及一款与时共进的 Apple Pencil。 新 iPad Pro 的机身背面内置了102 块磁铁，保证了 flat 扁平的这款 Apple Pencil 可牢固吸附在机身上，同时也能给 Apple Pencil 进行无线充电。这个设计优雅，实用，点赞必须。Furthermore, 边框塑性换成了方正的造型，重回 iPhone 5 方正边框的精典设计。从屏幕到外形，从外设到连接性，新 iPad Pro 的所有功能都是朝着高生产力笔记本电脑的 ecosystem 前进。 今天墨尔本赛马节，全维多利亚州人民在春天的草地上撒野。信主也准备潇洒一把。 登上了苹果网站，发现新 Apple Pencil 提供个人定制激光刻字服务。输入洋文 “Best there ever will be“ （后无来者的意思），苹果嫌弃太长。换成国语 “民族英雄 小平赠“，苹果又 handle 不了。无奈只好取消 $2,349.00 新 iPad Pro 和 $199.00 新 Apple Pencil 的订单 。 有钱花不出去，郁闷啊 … …","tags":[]},{"title":"Setup Lambda migrate & sync AWS DynamoDB to new table","date":"2018-10-24T03:29:00.000Z","path":"2018/10/24/Setup-Lambda-function-sync-AWS-DynamoDB-to-a-new-table/","text":"AWS DynamoDB is a good persistence solution for a specific solution. However, it’s not for a growing and changing application that could need new indexes and queries for its ever expanding features at any time. At this moment it is where the flexibility and speed of a relational database really shined through. Now, a DynamoDB table with Consumers Preferences data urgently needs to update: RENAME one EXISTING attribute ADD a new attribute SET a value in the new attribute for ALL EXISTING items in DynamoDB’s new table AWS lambda function plays handy here to migrate data, and sync newly inserted, modified &#x2F; updated, deleted items between existing and new DynamoDB tables. Create new DynamoDB table 123456789101112131415161718192021222324252627282930313233343536373839404142$ aws dynamodb create-table \\ --table-name userpreferences-ptest-02-USER_PREFERENCESV2 \\ --attribute-definitions AttributeName=id,AttributeType=S AttributeName=preferenceType,AttributeType=S \\ --key-schema AttributeName=id,KeyType=HASH AttributeName=preferenceType,KeyType=RANGE \\ --provisioned-throughput ReadCapacityUnits=100,WriteCapacityUnits=100&#123; &quot;TableDescription&quot;: &#123; &quot;TableArn&quot;: &quot;arn:aws:dynamodb:ap-southeast-2:123456789012:table/userpreferences-ptest-02-USER_PREFERENCESV2&quot;, &quot;AttributeDefinitions&quot;: [ &#123; &quot;AttributeName&quot;: &quot;id&quot;, &quot;AttributeType&quot;: &quot;S&quot; &#125;, &#123; &quot;AttributeName&quot;: &quot;preferenceType&quot;, &quot;AttributeType&quot;: &quot;S&quot; &#125; ], &quot;ProvisionedThroughput&quot;: &#123; &quot;NumberOfDecreasesToday&quot;: 0, &quot;WriteCapacityUnits&quot;: 100, &quot;ReadCapacityUnits&quot;: 100 &#125;, &quot;TableSizeBytes&quot;: 0, &quot;TableName&quot;: &quot;userpreferences-ptest-02-USER_PREFERENCESV2&quot;, &quot;TableStatus&quot;: &quot;CREATING&quot;, &quot;TableId&quot;: &quot;d116efdc-1234-5678-90ab-011de3e124fe&quot;, &quot;KeySchema&quot;: [ &#123; &quot;KeyType&quot;: &quot;HASH&quot;, &quot;AttributeName&quot;: &quot;id&quot; &#125;, &#123; &quot;KeyType&quot;: &quot;RANGE&quot;, &quot;AttributeName&quot;: &quot;preferenceType&quot; &#125; ], &quot;ItemCount&quot;: 0, &quot;CreationDateTime&quot;: 1540273906.1059999 &#125;&#125; Migrate Data Create Migrate Data Lambda function Increase Memory and Runtime Timeout https://docs.aws.amazon.com/lambda/latest/dg/limits.html in case of execution pre-maturely ended without finishing the migration. Furthermore, in case of overcharging DynamoDB due to its limits https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html add time delay in Lambda function. migrate.js Add AWS DynamoDB Lambda execution role userpreferences-ptest-02-migrateRole Create a simple test event that can kick off function 12345&#123; &quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;, &quot;key3&quot;: &quot;value3&quot;&#125; Trigger run the function. Should see the data migrated from existing DynamoDB table into new table. Log can be found at AWS CloudWatch Log Groups &#x2F;aws&#x2F;lambda&#x2F;userpreferences-ptest-02-migrate Sync Data Enable Stream on the existing DynamoDB table Add a new trigger for DynamoDB table Create a new Lambda function linked to trigger sync.js Trigger Testing AWS Lambda built-in test can test trigger: Logging Lambda function log can be found on AWS CloudWatch Log Groups &#x2F;aws&#x2F;lambda&#x2F;userpreferences-ptest-02-sync Counter DataThis Lambda function can count the number of items in DynamoDB table. counter.js Async call, callback and Non-blocking, it’s very hard implement so in every applications. In addition, reject promises or async functions, don’t handle them with a catch, NodeJS will raise a warning. In a large complex applications with lots of async, having a single unhandled promise or await function terminate NodeJS, or have to handle them with try and catch in every place (spaghetti code again?) would be very bad. An example of AWS DynamoDB error: 12345678910111213141516172018-11-14T02:20:50.742Z 715f18fb-e7b3-11e8-b5c4-d75f9089dd50 Error thrown: &#123; ProvisionedThroughputExceededException: The level of configured provisioned throughput for the table was exceeded. Consider increasing your provisioning level with the UpdateTable API.at Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/json.js:48:27)at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:683:14)at Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)at AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)at /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:685:12)at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:115:18)message: &#x27;The level of configured provisioned throughput for the table was exceeded. Consider increasing your provisioning level with the UpdateTable API.&#x27;,code: &#x27;ProvisionedThroughputExceededException&#x27;,time: 2018-11-14T02:20:50.687Z,requestId: &#x27;C38MODOISAJEGTVPI2ISOPFGDBVV4KQNSO5AEMVJF66Q9ASUAAJG&#x27;,statusCode: 400,retryable: true &#125; AWS example doesn’t throw the error in the catch block, it returns error instead, so any errors end up in the catch block. And return promises early and use Promise.all() method. References DynamoDB: Changing table schema, https://www.abhayachauhan.com/2018/01/dynamodb-changing-table-schema/ Tutorial: Processing New Items in a DynamoDB Table, https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html How to escape async&#x2F;await hell, https://medium.freecodecamp.org/avoiding-the-async-await-hell-c77a0fb71c4c Node.js 8.10 runtime now available in AWS Lambda, https://aws.amazon.com/blogs/compute/node-js-8-10-runtime-now-available-in-aws-lambda/","tags":[]},{"title":"Apple Designs That Inspire / 吐槽苹果","date":"2018-10-13T09:27:43.000Z","path":"2018/10/13/Apple-Designs-That-Inspire/","text":"列举一下我认为的苹果产品设计的经典。我并不是苹果的粉丝，也从来没有拥有过任何一代 iPhone 产品，from first generation iPhone, to latest iPhone XS。 经典的 iPhone 5 &#x2F; 5s 适合单手操作，体重适中，不打滑，有握持感。 新 iPhone 早已没有了新鲜感。全面屏，无线充电，双卡双待，屏下指纹识别，前后双屏 … 这些都已不再成为苹果的专利。 手机设计还是有很多潜力开发。还没走到尽头。In other words, 手机是现在最具创新和有最多创新的产品了。但苹果现在已经变得保守，中庸，不思进取了。 BTW，耳机 maybe 下一个智能产品竞争领域。Headphones 已经成为手机以外第二个人手必备的设备了。 这是我在 October 23, 2012，the first generation iPad Mini was announced 时写的评论： “It takes 30 months and 4 generatons since first release of iPad on April 3rd 2010, Apple finally gets the right size, right weight, right price, right design, right OS, right applications, right market niche, right balanced, almost no annoying public complainted issues, nearly all bugs been fixed iPad delivered” iPad Mini 2 使用上 Retina Display. 至今仍然是轻便的不行，仍然流畅的不行。 新的“全”面屏 iPad Pro 本月要发布了。不明白“全”面屏在大屏幕设备上的意义。个人还是喜欢现在的 iPad pro 10.5. “全”面屏 iPad Pro 追求逼格，Bigger Than Bigger，牺牲掉 home button，少了一“键”钟情。 Why the hell MacBook has removed sleep light &#x2F; 呼吸灯? Think about it after you have a look Transformers Mac light： 为甚 turn off 信仰灯，信仰的灯塔？有多少用户会在乎多费那么一点点用电。留下的只是深深的执念，蛋蛋的忧伤？ Magsafe 磁吸充电接口一直被认为是饱含创新和智慧设计。论安全性与便捷性还有什么比 MagSafe 做的更好？ Why small is beautiful? 看看 Mac Mini 的结构。 能把一个功能完整的 PC 主机，放进如此狭小的空间内，功耗低又安静，除了 brilliant 还能用什么来形容？ Mac Mini 已经好长时间没有更新换代。不知道苹果是否在考虑砍掉这个产品。 近几年苹果的新产品设计是感觉在吃老本，going downhill. 好的设计没有继承发扬，反而在做减法被一一去掉。新产品亮相后总感觉缺少点什么。也许是一种精神，也许是一颗灵魂。","tags":[]},{"title":"The 10 Commandments of Good Products","date":"2018-09-13T22:22:00.000Z","path":"2018/09/14/The-10-Commandments-of-Good-Products/","text":"The Vespa scooter is an example of how great design truly transcends time. The Vespa has been relatively unchanged in decades. The simplicity and style of it still make it a desirable item to this day. The 10 Commandments of Good Products, https://medium.com/swlh/the-10-commandments-of-good-products-d1d0a97b30ee","tags":[]},{"title":"A Python script which implements Mantua Cipher","date":"2018-08-30T05:54:36.000Z","path":"2018/08/30/A-Python-script-which-implements-Mantua-Cipher/","text":"Run output from Console: 1234567891011$ PycharmProjects/cipher/venv/bin/python PycharmProjects/cipher/src/cipher.py[ 60 31 53 33 33 53 23 ] : blossom[ 60 55 73 66 66 73 23 ] : blossom[ 20 37 12 0 0 12 23 ] : blossom[ 60 37 82 14 14 82 89 ] : blossom[ 20 31 82 95 95 82 89 ] : blossom[ 60 36 90 0 0 90 23 ] : blossom[ 20 37 90 94 94 90 23 ] : blossom[ 20 55 90 94 94 90 89 ] : blossom[ 20 36 97 94 94 97 89 ] : blossom[ 60 36 82 0 0 82 23 ] : blossom","tags":[]},{"title":"Step by step resetting Bluetooth on Mac OS","date":"2018-08-23T10:30:49.000Z","path":"2018/08/23/Step-by-step-resetting-Bluetooth-on-Mac-OS/","text":"Latest Mac OS High Sierra version 10.13.6. Open Terminal and login as user “root”: 1# rm /Library/Preferences/com.apple.Bluetooth.plist Reset the Bluetooth Module with Option + Shift + click on bluetooth icon in taskbar. Click on Debug -&gt; Reset the Bluetooth Module Reset PRAM &#x2F; NVRAM: Shut down; power on, immediately press Command + Option + P + R until it resets itself a couple times. Official guide https://support.apple.com/en-us/HT204063 Reset SMC: Shut down; hold down Shift + Control + Option while holding the power button for 10 seconds. Official guide https://support.apple.com/en-us/HT201295","tags":[]},{"title":"Cloudcraft - visualise AWS environment as isometric architecture diagrams","date":"2018-08-14T00:05:03.000Z","path":"2018/08/14/Cloudcraft-visualise-AWS-environment-as-isometric-architecture-diagrams/","text":"References CloudCraft, https://cloudcraft.co/","tags":[]},{"title":"An bash, curl and awk example","date":"2018-08-02T23:00:06.000Z","path":"2018/08/03/An-bash-curl-and-awk-example/","text":"This is an example how in a bash script, curl and awk work together, on gist.","tags":[]},{"title":"Assumptions and Conditional Test execution, ignoring in JUnit","date":"2018-07-06T21:40:20.000Z","path":"2018/07/07/Assumptions-and-Conditional-Test-execution-ignoring-in-JUnit/","text":"Sometime you want to ignore some test depends on environment settings. Now both JUnit 4 and JUnit 5 support the concept of assumptions. Unit Test like this: 1234567@Testpublic void testRun() &#123; Assume.assumeTrue(BooleanUtils.toBoolean(System.getProperty(&quot;runtest&quot;))); // Rest of testcase&#125; With Gradle, pass in parameter like this: 1$ gradle test -Pruntest=false References Assumptions and Conditional Test Execution with JUnit 4 and 5, https://reflectoring.io/conditional-junit4-junit5-tests/","tags":[]},{"title":"Making VSCode like IntelliJ after line commented the cursor automatically moved to next line","date":"2018-06-16T05:44:06.000Z","path":"2018/06/16/Making-VSCode-like-IntelliJ-after-line-commented-the-cursor-automatically-moved-to-next-line/","text":"Want to make VSCode key bindings as closed as IntelliJ key bindings like. Firstly, install VSCode extension macros. Open Code -&gt; Preferences -&gt; Settings. Add following configuration into User Settings. 123456&quot;macros&quot;: &#123; &quot;commentLine&quot;: [ &quot;editor.action.commentLine&quot;, &quot;cursorDown&quot; ]&#125; Open Code -&gt; Preferences -&gt; Keyboard Shortcuts. Add following configuration into keybindings.json file. 12345678// Place your key bindings in this file to overwrite the defaults[ &#123; &quot;key&quot;: &quot;cmd+/&quot;, &quot;command&quot;: &quot;macros.commentLine&quot;, &quot;when&quot;: &quot;editorTextFocus &amp;&amp; !editorReadonly&quot; &#125;] That’s newly added Toggle Line Comment looks like after the change:","tags":[]},{"title":"Take over Mac OS X Bluetooth Control","date":"2018-06-09T01:45:17.000Z","path":"2018/06/09/Take-over-Mac-OS-X-Bluetooth-Control/","text":"Quite annoying when Bluetooth on Mac OS X is off, you can’t turn it on manually via Systme Preferences interface. Now you can install blueutil from https://github.com/toy/blueutil to solve this issue. 1234567891011121314151617igloo:cheatsheets root# blueutil --helpblueutil v2.1.0Usage: blueutil [options]Without options outputs current state -p, --power output power state as 1 or 0 -p, --power STATE set power state -d, --discoverable output discoverable state as 1 or 0 -d, --discoverable STATE set discoverable state -h, --help this help -v, --version show versionSTATE can be one of: 1, on, 0, off, toggle Firstly, turn off Bluetooth service via command line: 12igloo:cheatsheets root# blueutil -p 0igloo:cheatsheets root# blueutil -d 0 Then turn on Bluetooth service via command line: 12igloo:cheatsheets root# blueutil -d 1igloo:cheatsheets root# blueutil -p 1","tags":[]},{"title":"Setup and run DEVHINTS.IO cheatsheets on localhost","date":"2018-06-08T07:32:39.000Z","path":"2018/06/08/Setup-and-run-DEVHINTS-IO-cheatsheets-on-localhost/","text":"This is a static website based on Jekyll framework https://jekyllrb.com/. Need install Jekyll and . Login as user “root” due to write permission required into &#x2F;Library&#x2F;Ruby&#x2F;Gems directory. Install bundler and Jekyll at first: 1igloo:cheatsheets root# gem install bundler jekyll Then install all this application dependent libraries: 1igloo:cheatsheets root# bundler install Start up Jekyll website on localhost, with user “terrence” this time: 123456789101112131415terrence@igloo ~/Projects/cheatsheets (master ☡=)17:28:56 𝜆 bundle exec jekyll serve --incrementalConfiguration file: /Users/terrence/Projects/cheatsheets/_config.yml Source: /Users/terrence/Projects/cheatsheets Destination: /Users/terrence/Projects/cheatsheets/_site Incremental build: enabled Generating...DEPRECATION WARNING on line 83, column 12 of /Users/terrence/Projects/cheatsheets/_sass/vendor/modularscale/_modularscale.scss:Unescaped multiline strings are deprecated and will be removed in a future version of Sass.To include a newline in a string, use &quot;\\a&quot; or &quot;\\a &quot; as in CSS. done in 3.163 seconds. Auto-regeneration: enabled for &#x27;/Users/terrence/Projects/cheatsheets&#x27; Server address: http://127.0.0.1:4000 Server running... press ctrl-c to stop. Go to URL: http://127.0.0.1:4000 start searching and using cheatsheets in localhost environment: To generate static web content ONLY: 12345678910111213terrence@igloo ~/Projects/cheatsheets (master ☡=)17:28:56 𝜆 bundle exec jekyll buildConfiguration file: /Users/terrence/Projects/cheatsheets/_config.yml Source: /Users/terrence/Projects/cheatsheets Destination: ../terrencemiao.github.io/cheatsheets Incremental build: disabled. Enable with --incremental Generating...DEPRECATION WARNING on line 83, column 12 of /Users/terrence/Projects/cheatsheets/_sass/vendor/modularscale/_modularscale.scss:Unescaped multiline strings are deprecated and will be removed in a future version of Sass.To include a newline in a string, use &quot;\\a&quot; or &quot;\\a &quot; as in CSS. done in 43.269 seconds. Auto-regeneration: disabled. Use --watch to enable. Then can upload published website content from ..&#x2F;terrencemiao.github.io&#x2F;cheatsheets directory to https://terrencemiao.github.io.","tags":[]},{"title":"The Evolution of Mobile Phones","date":"2018-05-12T09:16:00.000Z","path":"2018/05/12/The-Evolution-of-Mobile-Phones/","text":"Bigger and bigger screen, fewer and fewer buttons, more and more entertaining, less and less thinking. 屏幕越来越大，按键越来越少。娱乐越来越多，思考越来越少。","tags":[]},{"title":"The Evolution of Communication","date":"2018-05-12T09:00:19.000Z","path":"2018/05/12/The-Evolution-of-Communication/","text":"","tags":[]},{"title":"Serenity, powered by Simple Design","date":"2018-05-02T22:43:08.000Z","path":"2018/05/03/Serenity-powered-by-Simple-Design/","text":"","tags":[]},{"title":"Data Science Knowledge Graph","date":"2018-04-28T12:14:35.000Z","path":"2018/04/28/Data-Science-Knowledge-Graph/","text":"","tags":[]},{"title":"Always safe, Always fun","date":"2018-04-17T11:42:41.000Z","path":"2018/04/17/Always-safe-Always-fun/","text":"If you’re not paying for the product, you are the product. Durex - Always safe, Always fun.","tags":[]},{"title":"心流 / 心流れ / Flow","date":"2018-04-06T14:27:24.000Z","path":"2018/04/07/Flow/","text":"A feeling of complete absorption when you’re engaged in something you love to do without being disrupted by anxiety or boredom. References Flow (psychology), https://en.wikipedia.org/wiki/Flow_(psychology)","tags":[]},{"title":"The longest official place name in Australia","date":"2018-03-28T11:16:57.000Z","path":"2018/03/28/The-longest-official-place-name-in-Australia/","text":"Mamungkukumpurangkuntjunya Hill, is a hill in South Australia. The name means “where the devil urinates” in the regional Pitjantjatjara language. The name is the longest official place name in Australia. The hill is located approximately 108.8 km west north-west of Marla.","tags":[]},{"title":"House Prices Down Under","date":"2018-03-26T08:59:11.000Z","path":"2018/03/26/House-Prices-Down-Under/","text":"A look at historical house prices in Sydney and Melbourne from 1880 to 2010. Melbourne Median vs Average Wage 1965 - 2010. It must be remembered that house prices are determined by the demand and supply of credit, not the demand for and supply of housing. 房价不与国民收入，社会福利放在一起进行讨论，无异于光天化日里耍流氓。","tags":[]},{"title":"AlphaGo movie","date":"2018-03-25T08:45:06.000Z","path":"2018/03/25/AlphaGo-movie/","text":"AlphaGo is an intriguing movie, full of striking imagery. Proud of witnessing DeepMind and Google are making history …","tags":[]},{"title":"The A.I. Revolution Will Not Be Supervised","date":"2018-03-11T09:22:21.000Z","path":"2018/03/11/The-A-I-Revolution-Will-Not-Be-Supervised/","text":"“The A.I. Revolution Will Not Be Supervised.” - Yann LeCun","tags":[]},{"title":"RESTful calls to create Index and Mappings in ElasticSearch","date":"2018-03-09T23:58:36.000Z","path":"2018/03/10/RESTful-calls-to-create-Index-and-Mappings-in-ElasticSearch/","text":"There is a simple way, using RESTful client to create Index and Mappings in ElasticSearch, for example, with ElasticSearch Head plugin. Create Index in ElasticSearch with Settings JSON file. URL: http://localhost:9200/orders/, Method: PUT 123456789101112131415161718192021222324252627282930313233&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0, &quot;analysis&quot;: &#123; &quot;filter&quot;: &#123; &quot;partial_matching_filter&quot;: &#123; &quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 1, &quot;max_gram&quot;: 40 &#125; &#125;, &quot;normalizer&quot;: &#123; &quot;lowercase_normalizer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;filter&quot;: [&quot;lowercase&quot;] &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;partial_matcher&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;partial_matching_filter&quot; ] &#125; &#125; &#125; &#125; &#125;&#125; Create Mappings in ElasticSearch with Mappings JSON file. URL: http://localhost:9200/orders/_mapping/order, Method: PUT 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&#123; &quot;dynamic&quot;: &quot;false&quot;, &quot;_all&quot;: &#123; &quot;enabled&quot;: false&#125;, &quot;properties&quot;: &#123; &quot;customer_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;encryption_key_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: &quot;false&quot; &#125;, &quot;metadata_tags&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;category&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;order_creation_date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;date_optional_time&quot; &#125;, &quot;order_reference&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;order_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;normalizer&quot;: &quot;lowercase_normalizer&quot; &#125;, &quot;to_name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;partial_matcher&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; &#125;, &quot;to_suburb&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;to_state&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;to_postcode&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;to_email&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125; &#125;&#125; To add a new fields to ElasticSearch, while dynamic mapping is off: 123&#123; &quot;dynamic&quot;: &quot;false&quot;&#125; Run with additional fields JSON file, with URL: http://localhost:9200/orders/_mapping/order, Method: PUT 1234567891011121314151617181920212223242526272829303132333435363738&#123; &quot;properties&quot;: &#123; &quot;from_name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;partial_matcher&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; &#125;, &quot;from_business_name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;partial_matcher&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; &#125;, &quot;from_suburb&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;from_state&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;from_postcode&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;from_country&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;from_email&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125;, &quot;from_phone&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;whitespace&quot; &#125; &#125;&#125; An ElasticSearch query example, with URL: http://localhost:9200/orders/_search, Method: POST 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;match&quot;: &#123;&quot;customer_id&quot;: &quot;1234567890&quot;&#125; &#125;, &#123;&quot;simple_query_string&quot;: &#123; &quot;query&quot;: &quot;unlabelled+unmanifested&quot;, &quot;fields&quot;: [&quot;metadata_tags&quot;] &#125; &#125;, &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;TerrenceMiao&quot;, &quot;fields&quot;: [ &quot;name&quot;, &quot;business_name&quot;, &quot;order_id&quot;, &quot;shipment_id&quot;, &quot;sender_references&quot;, &quot;article_ids&quot; ] &#125; &#125;, &#123;&quot;match&quot;: &#123;&quot;phone&quot;: &quot;abracadbravvv5/vv70qBu+/ve+/ve6EuA7vv70g77+9Su+/ve+/vX5y77+95JuV&quot;&#125; &#125;, &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;gdgdggddvvv75/vv70qBu+/ve+/ve6EuA7vv70g77+9Su+/ve+/vX5y77+95JuV&quot;, &quot;fields&quot;: [ &quot;suburb&quot;, &quot;state&quot;, &quot;country&quot;, &quot;postcode&quot;, &quot;email&quot; ] &#125; &#125; ] &#125; &#125;, &#123;&quot;simple_query_string&quot;: &#123; &quot;query&quot;: &quot;DESPATCH&quot;, &quot;fields&quot;: [&quot;movement_type&quot;] &#125; &#125; ] &#125; &#125;, &quot;sort&quot;: [ &#123;&quot;order_creation_date&quot;: &#123;&quot;order&quot;: &quot;desc&quot;, &quot;unmapped_type&quot;: &quot;date&quot;&#125; &#125;, &#123;&quot;shipment_creation_date&quot;: &#123;&quot;order&quot;: &quot;desc&quot;&#125; &#125; ], &quot;from&quot;: &quot;0&quot;,&quot;size&quot;: &quot;1&quot;&#125; An example query with date&#x2F;time type in a range: 12345678910111213141516&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;shipment_creation_date&quot;: &#123; &quot;gte&quot;: &quot;2018-04-19T15:30:00&quot;, &quot;lte&quot;: &quot;now&quot;, &quot;time_zone&quot;: &quot;+10:00&quot; &#125; &#125; &#125;, &quot;from&quot;: 0, &quot;size&quot;: 10, &quot;sort&quot;: [], &quot;aggs&quot;: &#123;&#125;&#125;","tags":[]},{"title":"AWS access key id and secret access key","date":"2018-03-09T23:28:06.000Z","path":"2018/03/10/AWS-access-key-id-and-secret-access-key/","text":"There are some secret of AWS credentials, i.e., its Access Key ID and Secret Access Key, which let you connect AWS services withouth authentication. In a quintessential project environment, you have dev, test, prod environment setup in AWS. Your local AWS_PROFILE is something like this so you can switch to different targeting end services while run your application on localhost: 12345678910111213141516𝜆 cat ~/.aws/credentials[default]aws_access_key_id=dev-aws_access_key_idaws_secret_access_key=dev-aws_secret_access_key[dev]aws_access_key_id=dev-aws_access_key_idaws_secret_access_key=dev-aws_secret_access_key[test]aws_access_key_id=test-aws_access_key_idaws_secret_access_key=test-aws_secret_access_key[prod]aws_access_key_id=prod-aws_access_key_idaws_secret_access_key=prod-aws_secret_access_key When test Jest client in Test environment, a AWS ElasticSearch client library, error thrown: 1232018-02-27 15:18:19 INFO org.paradise.search.routes.ElasticSearchRoute - Body: io.searchbox.core.Index@4804b850[uri=orders/order/4SMK1UmbtqIAAAFhmYAFt9V7,method=PUT] message: searchIndexRoute - updating search index2018-02-27 15:18:19 INFO org.apache.camel.processor.interceptor.Tracer - &gt;&gt;&gt; (searchIndexRoute) org.paradise.search.routes.ElasticSearchRoute$$Lambda$137/819330075@119b837 --&gt; org.paradise.search.routes.ElasticSearchRoute$$Lambda$138/2035788375@2acacdf &lt;&lt;&lt; Pattern:InOnly, BodyType:io.searchbox.core.Index2018-02-27 15:18:19 ERROR org.paradise.search.routes.ElasticSearchRoute - Error while updating search index. 403 Forbidden &#123;&quot;Message&quot;:&quot;User: arn:aws:iam::123456789012:user/svcbamboo is not authorized to perform: es:ESHttpPut on resource: paradise-esv5-test-esd&quot;&#125; at &#x27;orders/order/4SMK1UmbtqIAAAFhmYAFt9V7&#x27; It turns out that WRONG Dev AWS_PROFILE applied in Test environment. Replacing “aws_access_key_id” and “aws_secret_access_key” from the default profile with “aws_access_key_id” and “aws_secret_access_key” from test profile: 12345678910111213141516𝜆 cat ~/.aws/credentials[default]aws_access_key_id=test-aws_access_key_idaws_secret_access_key=test-aws_secret_access_key[dev]aws_access_key_id=dev-aws_access_key_idaws_secret_access_key=dev-aws_secret_access_key[test]aws_access_key_id=test-aws_access_key_idaws_secret_access_key=test-aws_secret_access_key[prod]aws_access_key_id=prod-aws_access_key_idaws_secret_access_key=prod-aws_secret_access_key Rerun the test and in log: 1232018-02-27 15:23:54 INFO org.paradise.search.routes.ElasticSearchRoute - Body: io.searchbox.core.Index@62b18125[uri=orders/order/RVwK1UIBXmoAAAFhdJkFo9WA,method=PUT] message: searchIndexRoute - updating search index2018-02-27 15:23:54 INFO org.apache.camel.processor.interceptor.Tracer - &gt;&gt;&gt; (searchIndexRoute) org.paradise.search.routes.ElasticSearchRoute$$Lambda$137/1716909005@a809a62 --&gt; org.paradise.search.routes.ElasticSearchRoute$$Lambda$138/612681832@4b2713b1 &lt;&lt;&lt; Pattern:InOnly, BodyType:io.searchbox.core.Index2018-02-27 15:23:55 INFO org.paradise.search.routes.ElasticSearchRoute - Finished updating search index at &#x27;orders/order/RVwK1UIBXmoAAAFhdJkFo9WA&#x27;.","tags":[]},{"title":"AWS ElasticSearch and Kibana proxy setup","date":"2018-02-17T09:26:41.000Z","path":"2018/02/17/AWS-ElasticSearch-and-Kibana-proxy-setup/","text":"There are several ways to access Amazon AWS ElasticSearch and Kibana services, which are HTTP based, without inject into HTTP request headers with authentication key … AWS ES ProxyInstall a proxy application - AWS ES&#x2F;Kibana Proxy. Download it from https://www.npmjs.com/package/aws-es-kibana and install: 1$ npm install -g aws-es-kibana Run AWS ES&#x2F;Kibana Proxy then connect to ElasticSearch and Kibana services on AWS: 123456789101112131415161718192021222324252627$ cat ~/.aws/config[default]output = jsonregion = ap-southeast-2[profile sandy-test]output = jsonregion = ap-southeast-2$ cat ~/.aws/credentials[default]aws_access_key_id=Academyaws_secret_access_key=Dingly Ding[ap-test]aws_access_key_id=Abracadbraaws_secret_access_key=Somewhere Over The Rainbow$ AWS_PROFILE=ap-test aws-es-kibana search-paradise-esv5-test-01-esd-blah23dlaoed81nz890adle4.ap-southeast-2.es.amazonaws.com__________ _________ _________________ ________ _________ |_ | / /_ ___/ ___ ____/_ ___/ ___ __ \\________________ ______ ____ /__ /| |_ | /| / /_____ \\ __ __/ _____ \\ __ /_/ /_ ___/ __ \\_ |/_/_ / / /_ /_ ___ |_ |/ |/ / ____/ / _ /___ ____/ / _ ____/_ / / /_/ /_&gt; &lt; _ /_/ / /_//_/ |_|___/|__/ /____/ /_____/ /____/ /_/ /_/ \\____//_/|_| _\\__, / (_) /____/AWS ES cluster available at http://127.0.0.1:9200Kibana available at http://127.0.0.1:9200/_plugin/kibana/ With Fish Shell: 123456789𝜆 env AWS_PROFILE=ap-test aws-es-kibana search-paradise-esv5-test-01-esd-blah23dlaoed81nz890adle4.ap-southeast-2.es.amazonaws.com__________ _________ _________________ ________ _________ |_ | / /_ ___/ ___ ____/_ ___/ ___ __ \\________________ ______ ____ /__ /| |_ | /| / /_____ \\ __ __/ _____ \\ __ /_/ /_ ___/ __ \\_ |/_/_ / / /_ /_ ___ |_ |/ |/ / ____/ / _ /___ ____/ / _ ____/_ / / /_/ /_&gt; &lt; _ /_/ / /_//_/ |_|___/|__/ /____/ /_____/ /____/ /_/ /_/ \\____//_/|_| _\\__, / (_) /____/AWS ES cluster available at http://127.0.0.1:9200Kibana available at http://127.0.0.1:9200/_plugin/kibana/ SSH TunnelSet up SSH Tunnel for AWS ElasticSearch https://search-paradise-esv5-test-01-esd-blah23dlaoed81nz890adle4.ap-southeast-2.es.amazonaws.com from 443 port to localhost 9200: 1𝜆 ssh -L 9200:search-paradise-esv5-test-01-esd-blah23dlaoed81nz890adle4.ap-southeast-2.es.amazonaws.com:443 -l ec2-user aws-jump-box Then access AWS ElasticSearch at: https://localhost:9200, AWS Kibana at: https://localhost:9200/_plugin/kibana ElasticSearch HeadWith plugin ElasticSearch Head, to query ElasticSearch, using URL and index “orders-search-box” e.g. https://localhost:9200/orders-search-test/, and context path “_search”","tags":[]},{"title":"Good things come to those who could wait till the last","date":"2018-02-06T12:35:51.000Z","path":"2018/02/06/Good-things-come-to-those-who-could-wait-till-the-last/","text":"And good things come in two (好事成双)。 想当初买耳机时对市场做过的一番调研，对欧洲，日本大厂的产品也略知一二。当同事介绍 Bluedio (蓝弦)，一家从没听说过的中国广州公司生产的耳机时，以为仅是一家 Made In China，价廉低质的山寨产品。但被告知 Bluedio 耳机在亚马逊销量排行第二后，着实地大吃一斤。 Si vous voyez un entrepreneur chinois sauter par la fenêtre, suivez-le, il y a srement de bonnes affaires à faire (If you see a Chinese entrepreneur jumping out the window, follow him, there is certainly of good bargains to be done). 亚马逊的用户多非呆瓜。当吃瓜群众争先恐后地 jumping out of window 时，you have to follow. There is certainly of good bargains to be done. 货比三家后在 Aliexpress 下了单。 Bluedio Ai 从澳洲本土发货。三天后收到。 我对运动耳机要求不高。轻便，舒适，锻炼时不要总分心它会掉下来。用过从 $5 的 cheapy 到高端 Apple EarPods，没一个令人满意。 经过一段时间的使用，已感觉 Ai 是我所求。跑步时戴上感觉不到存在。耳机，人与自然达成和谐，已融为一体了。 从国内经阿里特快（免费的），Bluedio F2 两周后也到了。”F” 代表 Faith (忠诚)。F2 在外观设计上也忠诚地拷贝了著名的 Bose QC-35。One on one 比较，F2 在重量上比 QC-35 上了一个级别。但全金属 frame，让强烈厌恶塑料感的用户体会到厂家的温暖。耳机各个方面做工优异，用料实足，多处细节表现出匠人用心。 试听了 a wild range 不同风格的音乐作品后，you could feel the rich bass, warmer tone, not overpowering highs, and very detailed clarity from F2. 音质与高端耳机足有一拼。 Active Noise Cancelation (ANC) 不如 QC-35. 外界的蜂鸣声在经过 noise cancelation 后统统淹没在一片白噪声中。 谈到音质，F2 还不是 Bluedio 的最强产品。Bluedio 有一款 Victory 系列的 headphones，with 12 drivers，相当于有十二个 speakers 的一对音箱 built-in 小小的 headphones 里，提供了 studio level 的音质享受。对高品质继续 following. Final thought. $52 for a 高性价比的 wireless wheadphone plus ANC like this, nothing could go wrong. 惊喜之余，你会觉得对这个耳机每一分钱的投资都是物超所值。 Top range products on same market now like Sennheiser PXC 550 (Best Audio Quality), Sony WH1000XM2 (Most Elegant Design) and Bose QC 35 II (People’s Choice), cost 10x what you pay for Bluedio。Bluedio 在有些方面与顶级产品还有距离，but spend $500 for one 有额外享受的 headphone 是否真有所值? Furthermore, think about if you wear them cross the road, and you are run over by car, that will seriously damage these expensive headphones. 长城永不倒，国货当自强。","tags":[]},{"title":"OnePlus 5T - a smartphone full of all the right ideas","date":"2018-01-12T08:13:47.000Z","path":"2018/01/12/OnePlus-5T-a-smartphone-full-of-all-the-right-ideas/","text":"Bought one OnePlus 5T Lava Red (熔岩红) special edition from OnePlus China website - http://rush.oneplus.cn/ in December 2017, when this edition is first available and exclusively sale in mainland China, which Red Colour is very popular in China that means bringing you a good fortune. Un-boxing Dual SIMsDual SIMs support. Having two mobile network carriers ALL in active mode. Performance Testby AnTuTu Benchmark by Geekbench by SpeedTest - ADSL2 connection Shooting TestSupport Dual Cameras and Portrait mode OnePlus Family OnePlus One，OnePlus 2，OnePlus 3，OnePlus 5, along with OnePlus 5T，一部完全一加手机的编年史，更显用户对高质量，高性价比的一加产品的喜爱与忠诚。 What OnePlus stands for is a no-gimmicks smartphone manufacturer build around a community. For a good example, OnePlus 5T, a phone is full of all the right ideas - hardware design, software responsiveness, and overall usability. 打动用户的好产品自己会热销，不在乎卖到哪里。一加所做的一切，不过是努力把该做的做得更好。 长城永不倒，国货当自强。 Never Settle.","tags":[]},{"title":"First ever half marathon trial","date":"2017-10-28T23:18:10.000Z","path":"2017/10/29/First-ever-half-marathon-trial/","text":"Try the first ever half marathon today - Yarra Boulevard run. New experience such routine, Yarra Boulevard from suburb Kew to Hawthorn, either by walking or cycling. Terrible weather condition today, windy and hot. Have to stop several time to get water supplied in case of dehydrated. After 17km, left leg calf muscles start cramping, force me to reduce the pace. After 19km, right leg calf muscles also start cramping, but I keep moving forward. New new personal best of Performance Level - 55 after run. Meanwhile, remind me there is limits inside me, but I know I can finish it. Quotes from Johnnie Walker commercial: “When the sun came shining and I was strolling and the wheat fields waving and the dust clouds rolling. As the fog was lifting, a voice was chanting, this land was made for you and me.” “As I was walking that ribbon of highway, I saw above me that endless sky way. I saw below me that golden valley, Esta tierra fue hecha para ti y para mí.” “I’ve roamed and rambled and I’ve followed my footsteps to the sparkling sands over diamond deserts, and all around me a voice was sounding, this land was made for you and me.” Keep running!","tags":[]},{"title":"浅谈程序员的数学修养","date":"2017-09-23T13:46:22.000Z","path":"2017/09/23/浅谈程序员的数学修养/","text":"可能有很多朋友在网上看过 Google 公司早几年的招聘广告，它的第一题如下了: &#123;first 10-digit prime found in consecutive digits e&#125;.com e 中出现的连续的第一个10个数字组成的质数。 据说当时这个试题在美国很多地铁的出站口都有大幅广告，只要正确解答了这道题，在浏览器的地址栏中输入这个答案，就可以进入下一轮的测试，整个测试过程如同一个数学迷宫，直到你成为 Google 的一员。 又如 Intel 某年的一道面试题目: 巴拿赫病故于 1945年8月31日。他的出生年份恰好是他在世时某年年龄的平方。 问:他是哪年出生的? 这道看似很简单的数学问题，你能不能能快地解答呢? 下面则是一道世界第一大软件公司微软的招聘测试题: 中间只隔一个数字的两个素数被称为素数对，比如 5 和 7，17 和 19，证明素数对之间的数字总能被 6 整除 (假设这两个素数都大于 6)，现在证明没有由三个素数组成的素数对。 这样的试题还有很多很多，这些题目乍初看上去都是一些数学问题。但是世界上一些著名的公司都把它们用于招聘测试，可见它们对新员工数学基础的重视。数学试题与应用程序试题是许多大型软件公司面试中指向性最明显的一类试题，这些试题就是考察应聘者的数学能力与计算机能力。某咨询公司的一名高级顾问曾说: 微软是一家电脑软件公司，当然要求其员工有一定的计算机和数学能力，面试中自然就会考察这类能力。微软的面试题目就考察了应聘人员对基础知识的掌握程度、对基础知识的应用能力，甚至暗含了对计算机基本原理的考察。所以，这样的面试题目的确很“毒辣”，足以筛选到合适的人。 四川大学数学学院的曹广福教授曾说过: “一个大学生将来的作为与他的数学修养有很大的关系”。 大学计算机专业学生都有感触，计算机专业课程中最难的几门课程莫过于离散数学、编译原理、数据结构，当然像组合数学、密码学、计算机图形学等课程也令许多人学起来相当吃力，很多自认为数据库学得很好的学生在范式、函数依赖、传递依赖等数学性比较强的概念面前感到力不从心，这些都是因为数学基础或者说数学知识的缺乏所造成的。数学是计算机的基础，这也是为什么考计算机专业研究生数学都采用最难试题 (数学一) 的原因，当然这也能促使一些新的交叉学科如数学与应用软件、信息与计算科学专业等飞速发展。 许多天才程序员本身就是数学尖子，众所周知，Bill Gates 的数学成绩一直都很棒，他甚至曾经期望当一名数学教授，他的母校 — 湖滨中学的数学系主任弗雷福·赖特曾这样谈起过他的学生: “他能用一种最简单的方法来解决某个代数或计算机问题，他可以用数学的方法来找到一条处理问题的捷径，我教了这么多年的书，没见过像他这样天分的数学奇才。他甚至可以和我工作过多年的那些优秀数学家媲美。当然，比尔也各方面表现得都很优秀，不仅仅是数学，他的知识面非常广泛，数学仅是他众多特长之一。” 影响一代中国程序人的金山软件股份有限公司董事长求伯君当年高考数学成绩满分进一步说明了问题。很多数学基础很好的人，一旦熟悉了某种计算机语言，他可以很快地理解一些算法的精髓，使之能够运用自如，并可能写出时间与空间复杂度都有明显改善的算法。 程序设计当中解决的相当一部分问题都会涉及各种各样的科学计算，这需要程序员具有什么样的基础呢? 实际问题转换为程序，要经过一个对问题抽象的过程，建立起完善的数学模型，只有这样，我们才能建立一个设计良好的程序。从中我们不难看出数学在程序设计领域的重要性。算法与计算理论是计算机程序设计领域的灵魂所在，是发挥程序设计者严谨，敏锐思维的有效工具，任何的程序设计语言都试图将之发挥得淋漓尽致。程序员需要一定的数学修养，不但是编程本身的需要，同时也是培养逻辑思维以及严谨的编程作风的需要。数学可以锻炼我们的思维能力，可以帮助我们解决现实中的问题。可以帮助我们更高的学习哲学。为什么经常有人对一些科学计算程序一筹莫展，他可以读懂每一行代码，但是却无法预测程序的预测结果，甚至对程序的结构与功能也一知半解，给他一个稍微复杂点的数学公式，他可能就不知道怎么把它变成计算机程序。很多程序员还停留在做做简单的 MIS，设计一下 MDI，写写简单的 Class 或用 SQL 语句实现查询等基础的编程工作上，对于一些需要用到数学知识的编程工作就避而远之，当然实现一个累加程序或者一个税率的换算程序还是很容易的，因为它们并不需要什么高深的数学知识。 一名有过 10 多年开发经验的老程序员曾说过: “所有程序的本质就是逻辑。技术你已经较好地掌握了，但只有完成逻辑能力的提高，你才能成为一名职业程序员。打一个比方吧，你会十八般武艺，刀枪棍棒都很精通，但就是力气不够，所以永远都上不了战场，这个力气对程序员而言就是逻辑能力 (其本质是一个人的数学修养，注意，不是数学知识)。” 程序员的数学修养不是一朝一夕就可以培养的。数学修养与数学知识不一样，修养需要一个长期的过程，而知识的学习可能只是一段短暂的时间。下面是一些我个人对于程序员如何提高与培养自己的数学修养的基本看法。 首先，应该意识到数学修养的重要性。作为一个优秀的程序员，一定的数学修养是十分重要也是必要的。数学是自然科学的基础，计算机科学实际上是数学的一个分支。计算机理论其实是很多数学知识的融合，软件工程需要图论，密码学需要数论，软件测试需要组合数学，计算机程序的编制更需要很多的数学知识，如集合论、排队论、离散数学、统计学，当然还有微积分。计算机科学一个最大的特征是信息与知识更新速度很快，随着数学知识与计算机理论的进一步结合，数据挖掘、模式识别、神经网络等分支科学得到了迅速发展，控制论、模糊数学、耗散理论、分形科学都促进了计算机软件理论、信息管理技术的发展。严格的说，一个数学基础不扎实的程序不能算一个合格的程序员，很多介绍计算机算法的书籍本身也就是数学知识的应用与计算机实现手册。 其次，自身数学知识的积累，培养自己的空间思维能力和逻辑判断能力。数学是一门分支众多的学科，我们无法在短暂的一生中学会所有的数学知识，像泛函理论、混沌理论以及一些非线性数学问题不是三五几天就可以掌握的。数学修养的培养并不在与数学知识的多少，但要求程序员有良好的数学学习能力，能够很快地把一些数学知识和自己正在解决的问题联系起来，很多理学大师虽然不是数学出身，但是他们对数学有很强的理解能力和敏锐的观察力，于是一系列新的学科诞生了，如计算化学、计算生物学、生物信息学、化学信息学、计算物理学，计算材料学等等。数学是自然学科的基础，计算机技术作为理论与实践的结合，更需要把数学的一些精髓融入其中。从计算机的诞生来看它就是在数学的基础上产生的，最简单的 0、1 进制就是一个古老的数学问题。程序设计作为一项创造性很强的职业，它需要程序员有一定的数学修养，也具有一定的数学知识的积累，可以更好地把一些数学原理与思想应用于实际的编程工作中去。学无止境，不断的学习是提高修养的必经之路。 第三，多在实践中运用数学。有些高等学校开设了一门这样的课程 —《数学建模》。我在大学时期也曾学过，这是一门内容很丰富的课程。它把很多相关的学科与数学都联系在一起，通过很多数学模型来解决实际的生产生活问题，很多问题的解决需要计算机程序来实现。我在大学和研究生阶段都参加过数学建模竞赛，获得了不少的经验，同时也进一步提高了自己的数学修养。实际上，现在的程序设计从某些角度来看就是一个数学建模的过程，模型的好坏关系到系统的成败，现在数学建模的思想已经用于计算机的许多相关学科中，不单只是计算机程序设计与算法分析。应该知道，数学是一门需要在实践中展示其魅力的科学，而计算机程序也是为帮助解决实际问题而编制的，因此，应该尽量使它们结合起来，在这个方面，计算机密码学是我认为运用数学知识最深最广泛的，每一个好的加密算法后面都有一个数学理论的支持，如椭圆曲线、背包问题、素数理论等。作为一名优秀的程序员，应该在实际工作中根据需要灵活运用数学知识，培养一定的数学建模能力，善于归纳总结，慢慢使自己的数学知识更加全面，数学修养得到进一步提高。 第四，程序员培养制度与教学的改革。许多程序员培养体制存在很多缺陷，一开始就要求学员能够快速精通某种语言，以语言为中心，对算法的核心思想与相关的数学知识都一笔带过，讲得很少，这造成很多程序员成为背程序的机器，这样不利于程序员自身的快速成长，也不利于程序员解决新问题。我在长期的程序员培训与计算机教学工作采用了一些与传统方式不一致的方法，收到了一定的效果。很多初学程序的人往往写程序时有时候会有思维中断，或者对一些稍难的程序觉得无法下手，我采用了一些课前解决数学小问题的方法来激励大家的学习兴趣，这些小问题不单单是脑筋急转弯，其中不少是很有代表意义的数学思考题。通过数学问题来做编程的热身运动，让学员在数学试题中激发自己的思维能力，记得有位专家曾经说过，经常做做数学题目会使自己变聪明，很长时间不去接触数学问题会使自己思维迟钝。通过一些经典的数学问题来培养学员的思维的严谨性和跳跃性。很多人可能不以为然，其实有些看似简单的问题并不一定能够快速给出答案，大脑也是在不断的运用中变更加灵活的。不信吗? 大家有兴趣可以做做下面这道题目，看看能不能在1分钟之内想到答案，这只是一道小学数学课后习题。很多人认为自己的数学基础很好，但是据说这道题目 90% 以上的人不能在一个小时内给出正确答案。试试，如果你觉得我说的是错的。 证明: AB + AC &gt; DB + DC (D 为三角形 ABC 的一个内点) 最后，多学多问，多看好书，看经典。我在这里向大家推荐两部可能大家已经很熟悉的经典的计算机算法教材，它们中间很多内容其实就是数学知识的介绍。 第一部是《算法导论》，英文名称: Introduction to Algorithms，作者: Thomas H. Cormen ，Charles E. Leiserson ，Ronald L. Rivest ，Clifford Stein。本书的主要作者来自麻省理工大学计算机，作者之一 Ronald L. Rivest 由于其在公开秘钥密码算法 RSA 上的贡献获得了图灵奖。这本书目前是算法的标准教材，美国许多名校的计算机系都使用它，国内有些院校也将本书作为算法课程的教材。另外许多专业人员也经常引用它。本书基本包含了所有的经典算法，程序全部由伪代码实现，这更增添了本书的通用性，使得利用各种程序设计语言进行程序开发的程序员都可以作为参考。语言方面通俗，很适合作为算法教材和自学算法之用。 另一部是很多人都应该知道的 Donald E. Knuth 所著《计算机程序设计艺术》，英文名称: The Art of Computer Programming。 Donald E. Knuth 人生最辉煌的时刻在斯坦福大学计算机系渡过，美国计算机协会图灵奖的获得者，是本领域内当之无愧的泰斗。有戏言称搞计算机程序设计的不认识 Knuth 就等于搞物理的不知道爱因斯坦，搞数学的不知道欧拉，搞化学的不知道道尔顿。被简称为 TAOCP 的这本巨著内容博大精深，几乎涵盖了计算机程序设计算法与理论最重要的内容。现在发行的只有三卷，分别为基础运算法则，半数值算法，以及排序和搜索 (在写本文之际，第四卷已经出来了，我也在第一时间抢购了一本)。本书结合大量数学知识，分析不同应用领域中的各种算法，研究算法的复杂性，即算法的时间、空间效率，探讨各种适用算法等，其理论和实践价值得到了全世界计算机工作者的公认。书中引入的许多术语、得到的许多结论都变成了计算机领域的标准术语和被广泛引用的结果。另外，作者对有关领域的科学发展史也有深入研究，因此本书介绍众多研究成果的同时，也对其历史渊源和发展过程做了很好的介绍，这种特色在全球科学著作中是不多见的。至于本书的价值我觉得 Bill Gates 先生的话足以说明问题: “如果你认为你是一名真正优秀的程序员读 Knuth 的《计算机程序设计艺术》，如果你能读懂整套书的话，请给我发一份你的简历”。作者数学方面的功底造就了本书严谨的风格，虽然本书不是用当今流行的程序设计语言描述的，但这丝毫不损伤它 “程序设计史诗” 的地位。道理很简单，它内涵的设计思想是永远不会过时的。除非英语实在有困难，否则建议读者选用英文版。我个人就是阅读的该书的英文版，虽然花了不少 money 和时间，但是收获颇丰，值得。 总之，要想成为一名有潜力有发展前途的程序员，或者想成为程序员中的佼佼者，你一定要培养良好的数学修养。切记: 对于一名能够灵活自如编写各种程序的人， 数学是程序的灵魂。 Reference http://www.cs.xmu.edu.cn/cs/node/213","tags":[]},{"title":"Geoffrey Hinton interview","date":"2017-09-20T13:29:03.000Z","path":"2017/09/20/Geoffrey-Hinton-interview/","text":"About this courseIf you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new “superpower” that will let you build AI systems that just were not possible a few years ago. In this course, you will learn the foundations of deep learning. When you finish this class, you will: Understand the major technology trends driving Deep Learning Be able to build, train and apply fully connected deep neural networks Know how to implement efficient (vectorized) neural networks Understand the key parameters in a neural network’s architecture This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions. This is the first course of the Deep Learning Specialization. Lecture transcriptAs part of this course by deeplearning.ai, hope to not just teach you the technical ideas in deep learning, but also introduce you to some of the people, some of the heroes in deep learning. The people that invented so many of these ideas that you learn about in this course or in this specialization. In these videos, I hope to also ask these leaders of deep learning to give you career advice for how you can break into deep learning, for how you can do research or find a job in deep learning. As the first of this interview series, I am delighted to present to you an interview with Geoffrey Hinton. AN: I think that at this point you more than anyone else on this planet has invented so many of the ideas behind deep learning. And a lot of people have been calling you the godfather of deep learning. Although it was not until we were chatting a few minutes ago, until I realized you think I’m the first one to call you that, which I’m quite happy to have done. But what I want to ask is, many people know you as a legend, I want to ask about your personal story behind the legend. So how did you get involved in, going way back, how did you get involved in AI and machine learning and neural networks? So when I was at high school, I had a classmate who was always better than me at everything, he was a brilliant mathematician. And he came into school one day and said, did you know the brain uses holograms? And I guess that was about 1966, and I said, sort of what’s a hologram? And he explained that in a hologram you can chop off half of it, and you still get the whole picture. And that memories in the brain might be distributed over the whole brain. And so I guess he’d read about Lashley’s experiments, where you chop off bits of a rat’s brain and discover that it’s very hard to find one bit where it stores one particular memory. So that’s what first got me interested in how does the brain store memories. And then when I went to university, I started off studying physiology and physics. I think when I was at Cambridge, I was the only undergraduate doing physiology and physics. And then I gave up on that and tried to do philosophy, because I thought that might give me more insight. But that seemed to me actually lacking in ways of distinguishing when they said something false. And so then I switched to psychology. And in psychology they had very, very simple theories, and it seemed to me it was sort of hopelessly inadequate to explaining what the brain was doing. So then I took some time off and became a carpenter. And then I decided that I’d try AI, and went of to Edinburgh, to study AI with Langer Higgins. And he had done very nice work on neural networks, and he’d just given up on neural networks, and been very impressed by Winograd’s thesis. So when I arrived he thought I was kind of doing this old fashioned stuff, and I ought to start on symbolic AI. And we had a lot of fights about that, but I just kept on doing what I believed in. AN: And then what? I eventually got a PhD in AI, and then I couldn’t get a job in Britain. But I saw this very nice advertisement for Sloan Fellowships in California, and I managed to get one of those. And I went to California, and everything was different there. So in Britain, neural nets was regarded as kind of silly, and in California, Don Norman and David Rumelhart were very open to ideas about neural nets. It was the first time I’d been somewhere where thinking about how the brain works, and thinking about how that might relate to psychology, was seen as a very positive thing. And it was a lot of fun there, in particular collaborating with David Rumelhart was great. AN: I see, great. So this was when you were at UCSD, and you and Rumelhart around what, 1982, wound up writing the seminal back prop paper, right? Actually, it was more complicated than that. In, I think, early 1982, David Rumelhart and me, and Ron Williams, between us developed the back prop algorithm, it was mainly David Rumelhart’s idea. We discovered later that many other people had invented it. David Parker had invented, it probably after us, but before we’d published. Paul Werbos had published it already quite a few years earlier, but nobody paid it much attention. And there were other people who’d developed very similar algorithms, it’s not clear what’s meant by back prop. But using the chain rule to get derivatives was not a novel idea. AN: I see, why do you think it was your paper that helped so much the community latch on to back prop? It feels like your paper marked an infection in the acceptance of this algorithm, whoever accepted it. So we managed to get a paper into Nature in 1986. And I did quite a lot of political work to get the paper accepted. I figured out that one of the referees was probably going to be Stuart Sutherland, who was a well known psychologist in Britain. And I went to talk to him for a long time, and explained to him exactly what was going on. And he was very impressed by the fact that we showed that back prop could learn representations for words. And you could look at those representations, which are little vectors, and you could understand the meaning of the individual features. So we actually trained it on little triples of words about family trees, like Mary has mother Victoria. And you’d give it the first two words, and it would have to predict the last word. And after you trained it, you could see all sorts of features in the representations of the individual words. Like the nationality of the person there, what generation they were, which branch of the family tree they were in, and so on. That was what made Stuart Sutherland really impressed with it, and I think that’s why the paper got accepted. AN: Very early word embeddings, and you’re already seeing learned features of semantic meanings emerge from the training algorithm. Yes, so from a psychologist’s point of view, what was interesting was it unified two completely different strands of ideas about what knowledge was like. So there was the old psychologist’s view that a concept is just a big bundle of features, and there’s lots of evidence for that. And then there was the AI view of the time, which is a formal structurist view. Which was that a concept is how it relates to other concepts. And to capture a concept, you’d have to do something like a graph structure or maybe a semantic net. And what this back propagation example showed was, you could give it the information that would go into a graph structure, or in this case a family tree. And it could convert that information into features in such a way that it could then use the features to derive new consistent information, ie generalize. But the crucial thing was this to and fro between the graphical representation or the tree structured representation of the family tree, and a representation of the people as big feature vectors. And in fact that from the graph-like representation you could get feature vectors. And from the feature vectors, you could get more of the graph-like representation. AN: So this is 1986? In the early 90s, Bengio showed that you can actually take real data, you could take English text, and apply the same techniques there, and get embeddings for real words from English text, and that impressed people a lot. AN: I guess recently we’ve been talking a lot about how fast computers like GPUs and supercomputers that’s driving deep learning. I didn’t realize that back between 1986 and the early 90’s, it sounds like between you and Bengio there was already the beginnings of this trend. Yes, it was a huge advance. In 1986, I was using a list machine which was less than a tenth of a mega flop. And by about 1993 or thereabouts, people were seeing ten mega flops. So there was a factor of 100, and that’s the point at which is was easy to use, because computers were just getting faster. AN: Over the past several decades, you’ve invented so many pieces of neural networks and deep learning. I’m actually curious, of all of the things you’ve invented, which of the ones you’re still most excited about today? So I think the most beautiful one is the work I do with Terry Sejnowski on Boltzmann machines. So we discovered there was this really, really simple learning algorithm that applied to great big density connected nets where you could only see a few of the nodes. So it would learn hidden representations and it was a very simple algorithm. And it looked like the kind of thing you should be able to get in a brain because each synapse only needed to know about the behavior of the two neurons it was directly connected to. And the information that was propagated was the same. There were two different phases, which we called wake and sleep. But in the two different phases, you’re propagating information in just the same way. Where as in something like back propagation, there’s a forward pass and a backward pass, and they work differently. They’re sending different kinds of signals. So I think that’s the most beautiful thing. And for many years it looked just like a curiosity, because it looked like it was much too slow. But then later on, I got rid of a little bit of the beauty, and it started letting me settle down and just use one iteration, in a somewhat simpler net. And that gave restricted Boltzmann machines, which actually worked effectively in practice. So in the Netflix competition, for example, restricted Boltzmann machines were one of the ingredients of the winning entry. AN: And in fact, a lot of the recent resurgence of neural net and deep learning, starting about 2007, was the restricted Boltzmann machine, and de-restricted Boltzmann machine work that you and your lab did. Yes so that’s another of the pieces of work I’m very happy with, the idea of that you could train your restricted Boltzmann machine, which just had one layer of hidden features and you could learn one layer of feature. And then you could treat those features as data and do it again, and then you could treat the new features you learned as data and do it again, as many times as you liked. So that was nice, it worked in practice. And then UY Tay realized that the whole thing could be treated as a single model, but it was a weird kind of model. It was a model where at the top you had a restricted Boltzmann machine, but below that you had a Sigmoid belief net which was something that invented many years early. So it was a directed model and what we’d managed to come up with by training these restricted Boltzmann machines was an efficient way of doing inferences in Sigmoid belief nets. So, around that time, there were people doing neural nets, who would use densely connected nets, but didn’t have any good ways of doing probabilistic imprints in them. And you had people doing graphical models, unlike my children, who could do inference properly, but only in sparsely connected nets. And what we managed to show was the way of learning these deep belief nets so that there’s an approximate form of inference that’s very fast, it’s just hands in a single forward pass and that was a very beautiful result. And you could guarantee that each time you learn that extra layer of features there was a band, each time you learned a new layer, you got a new band, and the new band was always better than the old band. AN: The variational bands, showing as you add layers. Yes, I remember that video. So that was the second thing that I was really excited about. And I guess the third thing was the work I did with on variational methods. It turns out people in statistics had done similar work earlier, but we didn’t know about that. So we managed to make EN work a whole lot better by showing you didn’t need to do a perfect E step. You could do an approximate E step. And EN was a big algorithm in statistics. And we’d showed a big generalization of it. And in particular, in 1993, I guess, with Van Camp. I did a paper, with I think, the first variational Bayes paper, where we showed that you could actually do a version of Bayesian learning that was far more tractable, by approximating the true posterior with a guessing. And you could do that in neural net. And I was very excited by that. AN: I see. Wow, right. Yep, I think I remember all of these papers. You and Hinton, approximate paper, spent many hours reading over that. And I think some of the algorithms you use today, or some of the algorithms that lots of people use almost every day, are what, things like dropouts, or I guess activations came from your group? Yes and no. So other people have thought about rectified linear units. And we actually did some work with restricted Boltzmann machines showing that a ReLU was almost exactly equivalent to a whole stack of logistic units. And that’s one of the things that helped ReLUs catch on. AN: I was really curious about that. The value paper had a lot of math showing that this function can be approximated with this really complicated formula. Did you do that math so your paper would get accepted into an academic conference, or did all that math really influence the development of max of 0 and x? That was one of the cases where actually the math was important to the development of the idea. So I knew about rectified linear units, obviously, and I knew about logistic units. And because of the work on Boltzmann machines, all of the basic work was done using logistic units. And so the question was, could the learning algorithm work in something with rectified linear units? And by showing the rectified linear units were almost exactly equivalent to a stack of logistic units, we showed that all the math would go through. AN： I see. And it provided the inspiration for today, tons of people use ReLU and it just works without without necessarily needing to understand the same motivation. Yeah, one thing I noticed later when I went to Google. I guess in 2014, I gave a talk at Google about using ReLUs and initializing with the identity matrix. because the nice thing about ReLUs is that if you keep replicating the hidden layers and you initialize with the identity, it just copies the pattern in the layer below. And so I was showing that you could train networks with 300 hidden layers and you could train them really efficiently if you initialize with their identity. But I didn’t pursue that any further and I really regret not pursuing that. We published one paper with showing you could initialize an active showing you could initialize recurringness like that. But I should have pursued it further because Later on these residual networks is really that kind of thing. AN: Over the years I’ve heard you talk a lot about the brain. I’ve heard you talk about relationship being back prop and the brain. What are your current thoughts on that? I’m actually working on a paper on that right now. I guess my main thought is this. If it turns out the back prop is a really good algorithm for doing learning. Then for sure evolution could’ve figured out how to prevent it. I mean you have cells that could turn into either eyeballs or teeth. Now, if cells can do that, they can for sure implement back propagation and presumably this huge selective pressure for it. So I think the neuroscientist idea that it doesn’t look plausible is just silly. There may be some subtle implementation of it. And I think the brain probably has something that may not be exactly be back propagation, but it’s quite close to it. And over the years, I’ve come up with a number of ideas about how this might work. So in 1987, working with Jay McClelland, I came up with the recirculation algorithm, where the idea is you send information round a loop. And you try to make it so that things don’t change as information goes around this loop. So the simplest version would be you have input units and hidden units, and you send information from the input to the hidden and then back to the input, and then back to the hidden and then back to the input and so on. And what you want, you want to train an auto-encoder, but you want to train it without having to do back propagation. So you just train it to try and get rid of all variation in the activities. So the idea is that the learning rule for synapse is change the weighting proportion to the pre-synaptic input and in proportion to the rate of change at the post synaptic input. But in recirculation, you’re trying to make the post synaptic input, you’re trying to make the old one be good and the new one be bad, so you’re changing in that direction. We invented this algorithm before neuroscientists come up with spike-timing-dependent plasticity. Spike-timing-dependent plasticity is actually the same algorithm but the other way round, where the new thing is good and the old thing is bad in the learning rule. So you’re changing the weighting proportions to the preset outlook activity times the new person outlook activity minus the old one. Later on I realized in 2007, that if you took a stack of Restricted Boltzmann machines and you trained it up. After it was trained, you then had exactly the right conditions for implementing back propagation by just trying to reconstruct. If you looked at the reconstruction era, that reconstruction era would actually tell you the derivative of the discriminative performance. And at the first deep learning workshop at in 2007, I gave a talk about that. That was almost completely ignored. Later on, Yoshua Bengio, took up the idea and that’s actually done quite a lot of more work on that. And I’ve been doing more work on it myself. And I think this idea that if you have a stack of auto-encoders, then you can get derivatives by sending activity backwards and locate reconstructionaires, is a really interesting idea and may well be how the brain does it. AN: One other topic that I know you follow about and that I hear you’re still working on is how to deal with multiple time skills in deep learning? So, can you share your thoughts on that? Yes, so actually, that goes back to my first years of graduate student. The first talk I ever gave was about using what I called fast weights. So weights that adapt rapidly, but decay rapidly. And therefore can hold short term memory. And I showed in a very simple system in 1973 that you could do true recursion with those weights. And what I mean by true recursion is that the neurons that is used in representing things get re-used for representing things in the recursive core. And the weights that is used for actually knowledge get re-used in the recursive core. And so that leads the question of when you pop out your recursive core, how do you remember what it was you were in the middle of doing? Where’s that memory? because you used the neurons for the recursive core. And the answer is you can put that memory into fast weights, and you can recover the activities neurons from those fast weights. And more recently working with Jimmy Ba, we actually got a paper in it by using fast weights for recursion like that. I see. So that was quite a big gap. The first model was unpublished in 1973 and then Jimmy Ba’s model was in 2015, I think, or 2016. So it’s about 40 years later. AN: And, I guess, one other idea of quite a few years now, over five years, I think is capsules, where are you with that? Okay, so I’m back to the state I’m used to being in. Which is I have this idea I really believe in and nobody else believes it. And I submit papers about it and they would get rejected. But I really believe in this idea and I’m just going to keep pushing it. So it hinges on, there’s a couple of key ideas. One is about how you represent multi dimensional entities, and you can represent multi-dimensional entities by just a little backdoor activities. As long as you know there’s any one of them. So the idea is in each region of the image, you’ll assume there’s at most, one of the particular kind of feature. And then you’ll use a bunch of neurons, and their activities will represent the different aspects to that feature, like within that region exactly what are its x and y coordinates? What orientation is it at? How fast is it moving? What color is it? How bright is it? And stuff like that. So you can use a whole bunch of neurons to represent different dimensions of the same thing. Provided there’s only one of them. That’s a very different way of doing representation from what we’re normally used to in neuronettes. Normally in neuronettes, we just have a great big layer, and all the units go off and do whatever they do. But you don’t think of bundling them up into little groups that represent different coordinates of the same thing. So I think we should beat this extra structure. And then the other idea that goes with that. AN: So this means in the truth of the representation, you partition the representation to different subsets, to represent, right, rather than I call each of those subsets a capsule. And the idea is a capsule is able to represent an instance of a feature, but only one. And it represents all the different properties of that feature. It’s a feature that has a lot of properties as opposed to a normal neuron and a normal neuronette, which has just one scale of property. And then what you can do if you’ve got that, is you can do something that normal neuronettes are very bad at, which is you can do what I call routine by agreement. So let’s suppose you want to do segmentation and you have something that might be a mouth and something else that might be a nose. And you want to know if you should put them together to make one thing. So the idea should have a capsule for a mouth that has the parameters of the mouth. And you have a capsule for a nose that has the parameters of the nose. And then to decipher whether to put them together or not, you get each of them to vote for what the parameters should be for a face. Now if the mouth and the nose are in the right spacial relationship, they will agree. So when you get two captures at one level voting for the same set of parameters at the next level up, you can assume they’re probably right, because agreement in a high dimensional space is very unlikely. And that’s a very different way of doing filtering, than what we normally use in neural nets. So I think this routing by agreement is going to be crucial for getting neural nets to generalize much better from limited data. I think it’d be very good at getting the changes in viewpoint, very good at doing segmentation. And I’m hoping it will be much more statistically efficient than what we currently do in neural nets. Which is, if you want to deal with changes in viewpoint, you just give it a whole bunch of changes in view point and training on them all. AN: I see, right, so rather than FIFO learning, supervised learning, you can learn this in some different way. Well, I still plan to do it with supervised learning, but the mechanics of the forward paths are very different. It’s not a pure forward path in the sense that there’s little bits of iteration going on, where you think you found a mouth and you think you found a nose. And use a little bit of iteration to decide whether they should really go together to make a face. And you can do back props from that iteration. So you can try and do it a little discriminatively, and we’re working on that now at my group in Toronto. So I now have a little Google team in Toronto, part of the Brain team. That’s what I’m excited about right now. AN: I see, great, yeah. Look forward to that paper when that comes out. You worked in deep learning for several decades. I’m actually really curious, how has your thinking, your understanding of AI changed over these years? So I guess a lot of my intellectual history has been around back propagation, and how to use back propagation, how to make use of its power. So to begin with, in the mid 80s, we were using it for discriminative learning and it was working well. I then decided, by the early 90s, that actually most human learning was going to be unsupervised learning. And I got much more interested in unsupervised learning, and that’s when I worked on things like the Wegstein algorithm. AN: And your comments at that time really influenced my thinking as well. So when I was leading Google Brain, our first project spent a lot of work in unsupervised learning because of your influence. Right, and I may have misled you. Because in the long run, I think unsupervised learning is going to be absolutely crucial. But you have to sort of face reality. And what’s worked over the last ten years or so is supervised learning. Discriminative training, where you have labels, or you’re trying to predict the next thing in the series, so that acts as the label. And that’s worked incredibly well. I still believe that unsupervised learning is going to be crucial, and things will work incredibly much better than they do now when we get that working properly, but we haven’t yet. AN: Yeah, I think many of the senior people in deep learning, including myself, remain very excited about it. It’s just none of us really have almost any idea how to do it yet. Maybe you do, I don’t feel like I do. Variational altering code is where you use the reparameterization tricks. Seemed to me like a really nice idea. And generative adversarial nets also seemed to me to be a really nice idea. I think generative adversarial nets are one of the sort of biggest ideas in deep learning that’s really new. I’m hoping I can make capsules that successful, but right now generative adversarial nets, I think, have been a big breakthrough. AN: What happened to sparsity and slow features, which were two of the other principles for building unsupervised models? I was never as big on sparsity as you were, buddy. But slow features, I think, is a mistake. You shouldn’t say slow. The basic idea is right, but you shouldn’t go for features that don’t change, you should go for features that change in predictable ways. So here’s a sort of basic principle about how you model anything. You take your measurements, and you’re applying nonlinear transformations to your measurements until you get to a representation as a state vector in which the action is linear. So you don’t just pretend it’s linear like you do with common filters. But you actually find a transformation from the observables to the underlying variables where linear operations, like matrix multipliers on the underlying variables, will do the work. So for example, if you want to change viewpoints. If you want to produce the image from another viewpoint, what you should do is go from the pixels to coordinates. And once you got to the coordinate representation, which is a kind of thing I’m hoping captures will find. You can then do a matrix multiplier to change viewpoint, and then you can map it back to pixels. AN: Right, that’s why you did all that. I think that’s a very, very general principle. AN: That’s why you did all that work on face synthesis, right? Where you take a face and compress it to very low dimensional vector, and so you can fiddle with that and get back other faces. I had a student who worked on that, I didn’t do much work on that myself. AN: Now I’m sure you still get asked all the time, if someone wants to break into deep learning, what should they do? So what advice would you have? I’m sure you’ve given a lot of advice to people in one on one settings, but for the global audience of people watching this video. What advice would you have for them to get into deep learning? Okay, so my advice is sort of read the literature, but don’t read too much of it. So this is advice I got from my advisor, which is very unlike what most people say. Most people say you should spend several years reading the literature and then you should start working on your own ideas. And that may be true for some researchers, but for creative researchers I think what you want to do is read a little bit of the literature. And notice something that you think everybody is doing wrong, I’m contrary in that sense. You look at it and it just doesn’t feel right. And then figure out how to do it right. And then when people tell you, that’s no good, just keep at it. And I have a very good principle for helping people keep at it, which is either your intuitions are good or they’re not. If your intuitions are good, you should follow them and you’ll eventually be successful. If your intuitions are not good, it doesn’t matter what you do. AN: Inspiring advice, might as well go for it. You might as well trust your intuitions. There’s no point not trusting them. AN: I usually advise people to not just read, but replicate published papers. And maybe that puts a natural limiter on how many you could do, because replicating results is pretty time consuming. Yes, it’s true that when you’re trying to replicate a published you discover all over little tricks necessary to make it work. The other advice I have is, never stop programming. Because if you give a student something to do, if they’re botching, they’ll come back and say, it didn’t work. And the reason it didn’t work would be some little decision they made, that they didn’t realize is crucial. And if you give it to a good student, like UY Tay for example. You can give him anything and he’ll come back and say, it worked. I remember doing this once, and I said, but wait a minute UY. Since we last talked, I realized it couldn’t possibly work for the following reason. And said, yeah, I realized that right away, so I assumed you didn’t mean that. AN: I see, yeah, that’s great, yeah. Let’s see, any other advice for people that want to break into AI and deep learning? I think that’s basically, read enough so you start developing intuitions. And then, trust your intuitions and go for it, don’t be too worried if everybody else says it’s nonsense. AN: And I guess there’s no way to know if others are right or wrong when they say it’s nonsense, but you just have to go for it, and then find out. Right, but there is one thing, which is, if you think it’s a really good idea, and other people tell you it’s complete nonsense, then you know you’re really on to something. So one example of that is when and I first came up with variational methods. I sent mail explaining it to a former student of mine called Peter Brown, who knew a lot about. And he showed it to people who worked with him, called the brothers, they were twins, I think. And he then told me later what they said, and they said, either this guy’s drunk, or he’s just stupid, so they really, really thought it was nonsense. Now, it could have been partly the way I explained it, because I explained it in intuitive terms. But when you have what you think is a good idea and other people think is complete rubbish, that’s the sign of a really good idea. AN: I see, and research topics, new grad students should work on capsules and maybe unsupervised learning, any other? One good piece of advice for new grad students is, see if you can find an advisor who has beliefs similar to yours. Because if you work on stuff that your advisor feels deeply about, you’ll get a lot of good advice and time from your advisor. If you work on stuff your advisor’s not interested in, all you’ll get is, you get some advice, but it won’t be nearly so useful. AN: I see, and last one on advice for learners, how do you feel about people entering a PhD program? Versus joining a top company, or a top research group? Yeah, it’s complicated, I think right now, what’s happening is, there aren’t enough academics trained in deep learning to educate all the people that we need educated in universities. There just isn’t the faculty bandwidth there, but I think that’s going to be temporary. I think what’s happened is, most departments have been very slow to understand the kind of revolution that’s going on. I kind of agree with you, that it’s not quite a second industrial revolution, but it’s something on nearly that scale. And there’s a huge sea change going on, basically because our relationship to computers has changed. Instead of programming them, we now show them, and they figure it out. That’s a completely different way of using computers, and computer science departments are built around the idea of programming computers. And they don’t understand that sort of, this showing computers is going to be as big as programming computers. Except they don’t understand that half the people in the department should be people who get computers to do things by showing them. So my department refuses to acknowledge that it should have lots and lots of people doing this. They think they got a couple, maybe a few more, but not too many. And in that situation, you have to remind the big companies to do quite a lot of the training. So Google is now training people, we call brain residence, I suspect the universities will eventually catch up. AN: In fact, maybe a lot of students have figured this out. A lot of top 50 programs, over half of the applicants are actually wanting to work on showing, rather than programming. Yeah, cool, AN: yeah, in fact, to give credit where it’s due, whereas a deep learning AI is creating a deep learning specialization. As far as I know, their first deep learning MOOC was actually yours taught on Coursera, back in 2012, as well. And somewhat strangely, that’s when you first published the RMS algorithm, which also is a rough. Right, yes, well, as you know, that was because you invited me to do the MOOC. And then when I was very dubious about doing, you kept pushing me to do it, so it was very good that I did, although it was a lot of work. AN: Yes, and thank you for doing that, I remember you complaining to me, how much work it was. And you staying out late at night, but I think many, many learners have benefited for your first MOOC, so I’m very grateful to you for it, so, over the years, I’ve seen you embroiled in debates about paradigms for AI, and whether there’s been a paradigm shift for AI. What are your, can you share your thoughts on that? Yes, happily, so I think that in the early days, back in the 50s, people like von Neumann and didn’t believe in symbolic AI, they were far more inspired by the brain. Unfortunately, they both died much too young, and their voice wasn’t heard. And in the early days of AI, people were completely convinced that the representations you need for intelligence were symbolic expressions of some kind. Sort of cleaned up logic, where you could do nomeratonic things, and not quite logic, but something like logic, and that the essence of intelligence was reasoning. What’s happened now is, there’s a completely different view, which is that what a thought is, is just a great big vector of neural activity, so contrast that with a thought being a symbolic expression. And I think the people who thought that thoughts were symbolic expressions just made a huge mistake. What comes in is a string of words, and what comes out is a string of words. And because of that, strings of words are the obvious way to represent things. So they thought what must be in between was a string of words, or something like a string of words. And I think what’s in between is nothing like a string of words. I think the idea that thoughts must be in some kind of language is as silly as the idea that understanding the layout of a spatial scene must be in pixels, pixels come in. And if we could, if we had a dot matrix printer attached to us, then pixels would come out, but what’s in between isn’t pixels. And so I think thoughts are just these great big vectors, and that big vectors have causal powers. They cause other big vectors, and that’s utterly unlike the standard AI view that thoughts are symbolic expressions. AN: I see, good, I guess AI is certainly coming round to this new point of view these days. Some of it, I think a lot of people in AI still think thoughts have to be symbolic expressions. AN: Thank you very much for doing this interview. It was fascinating to hear how deep learning has evolved over the years, as well as how you’re still helping drive it into the future, so thank you, Jeff. Well, thank you for giving me this opportunity. AN: Thank you. References Geoffrey Hinton interview, https://www.coursera.org/learn/neural-networks-deep-learning/lecture/dcm5r/geoffrey-hinton-interview","tags":[]},{"title":"画虾","date":"2017-09-03T06:31:32.000Z","path":"2017/09/03/画虾/","text":"今购得一画，甚喜。 构图严谨而不失大气，疏密有致且遥相呼应，笔墨灵动犹如活物跃然纸上。齐白石三字字迹遒劲有力，却又透露出儿童般的天真烂漫。 字，画相应得章，疏可走马密不透风，装裱更是别具一格，边上圆点疏密一致，深色背景与浅色画面相互映衬。 实为难得佳作，并经砖家鉴定真迹无疑。 此画还有一神奇之处，若放笼屉蒸 10 分钟，画面生彩，触觉肌理，意境更佳。","tags":[]},{"title":"为什么要读书","date":"2017-09-03T06:09:19.000Z","path":"2017/09/03/为什么要读书/","text":"父子二人饮茶，儿问：“为什么要我读书？” 父答：“我这么跟你说吧！你读了书，喝这茶时就会说：‘此茶汤色澄红透亮，气味幽香如兰，口感饱满纯正，圆润如诗，回味甘醇，齿颊留芳，韵味十足，顿觉如梦似幻，仿佛天上人间，真乃茶中极品！’” 而如果你没有读书，你就会说：：“‘卧操！这茶不赖’” 近日对《中国诗词大会》上瘾，于是思考人为什么要读书？又如何用好的词语来描述心情和感受呢？ 有人曾提出这样一个问题：大部分读过的书最后都会忘掉，那读书的意义何在？这是我见过最好的回答：“小的时候我吃了很多东西，其中的大部分我已记不清是什么，但我知道，他们已经成了我现在的骨和肉”。读书，也是如此。它在不知不觉中就已经影响了你的思想，你的言行，你的形象。 1，当你开心的时候你可以说：春风得意马蹄疾一日看尽长安花 而不是只会说：哈哈, 哈哈, 哈哈, 哈哈, 哈哈哈 2，当你伤心的时候你可以说：问君能有几多愁，恰似一江春水向东流 而不是只会说：我的心好痛 3，当你看到帅哥时你可以说：陌上人如玉公子世无双 而不是只会说：我靠，好帅！我靠靠靠，太帅了 4，当你看到美女时你可以说：北方有佳人，绝世而独立 而不是只会说：我去，她好美我去，她真美 5，当你遇见渣男时你可以说：遇人不淑 ，识人不善 而不是只会说：瞎了老子的狗眼 6，当你向一个人表达爱意时你可以说：山有木兮木有枝心悦君兮君不知 而不是只会说：我喜欢你，天荒地老，海枯石烂 7，当你思念一个人的时候你可以说：衣带渐宽终不悔为伊消得人憔悴 而不是只会说：我想死你啦 8，当你失恋的时候你可以说：人生若只如初见何事秋风悲画扇等闲变却故人心却道故人心易变 而不是只会千万遍的呼喊：蓝瘦，香菇 9，结婚的时候你可以说：春宵一刻值千金花有清香月有阴 而不是只会说：嘿嘿, 嘿嘿, 嘿嘿嘿 10，分手的时候你可以说：相濡以沫不如相忘于江湖 而不是只会说：我们不合适 11，看见大漠戈壁的时候你可以说：大漠孤烟直，长河落日圆 而不是只会说：唉呀妈呀，这全都是沙子 12，看见夕阳余晖的时候你可以说：落霞与孤鹜齐飞秋水共长天一色 而不是只会说：卧槽这么多鸟真好看真他妈太好看了","tags":[]},{"title":"C10k / C10M Challenge","date":"2017-09-03T04:38:45.000Z","path":"2017/09/03/C10k-C10M-Challenge/","text":"C10k &#x2F; C10M Challenge 挑战 HistoryThe term was coined in 1999 by Dan Kegel, citing the Simtel FTP host, cdrom.com, serving 10,000 clients at once over 1 gigabit per second Ethernet in that year. The term has since been used for the general issue of large number of clients, with similar numeronyms for larger number of connections, most recently C10M in the 2010s. By the early 2010s millions of connections on a single commodity 1U server became possible: over 2 million connections (WhatsApp, 24 cores, using Erlang on FreeBSD), 10–12 million connections (MigratoryData, 12 cores, using Java on Linux). C10k（concurrently handling 10k connections）是一个在 1999 年被提出来的技术挑战，如何在一颗 1GHz CPU，2G 内存，1Gbps 网络环境下，让单台服务器同时为 1 万个客户端提供 FTP 服务。而到了 2010 年后，随着硬件技术的发展，这个问题被延伸为 C10M，即如何利用 8 核心 CPU，64G 内存，在 10Gbps 的网络上保持 1000 万并发连接，或是每秒钟处理 100 万的连接。（两种类型的计算机资源在各自的时代都约为 1200 美元）。 C10k &#x2F; C10M 问题则是从技术角度出发挑战软硬件极限。C10k &#x2F; C10M 问题得解，成本问题和效率问题迎刃而解。 References C10k problem, https://en.wikipedia.org/wiki/C10k_problem The C10K problem, http://www.kegel.com/c10k.html The Secret To 10 Million Concurrent Connections - The Kernel Is The Problem Not The Solution, http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html Inside NGINX: How We Designed for Performance &amp; Scale, https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/ 架构师实践日｜从C10K到C10M高性能网络的探索与实践, http://blog.qiniu.com/archives/4941","tags":[]},{"title":"Plato and Socrates - 柏拉图麦穗问题","date":"2017-09-02T03:58:00.000Z","path":"2017/09/02/Plato-and-Socrates-柏拉图麦穗问题/","text":"One day, Plato asked his teacher Socrates, “What is love? How can I find it?” Socrates answered, “There is a vast wheat field in front. Walk forward without turning back, and pick only one stalk. If you find the most magnificent stalk, then you have found love.” Plato walked forward, and before long, he returned with empty hands, having picked nothing. His teacher asked, “Why did you not pick any stalk?” Plato answered, “Because I could only pick once, and yet I could not turn back. I did find the most magnificent stalk, but did not know if there were any better ones ahead, so I did not pick it. As I walked further, the stalks that I saw were not as good as the earlier one, so! I did not pick any in the end.” Socrates then said, “And that is LOVE.” On another day, Plato asked Socrates: “What is marriage? How can I find it?” His teacher answered, “There is a thriving forest in front. Walk forward without turning back, and chop down only one tree. If you find the tallest tree, then you have found marriage”. Plato walked forward, and before long, He returned with a tree. The tree was not bad but it was not tall, either. It was only an ordinary tree, not the best but just a good tree. His teacher asked, “Why did you chop down such an ordinary tree?” Plato answered, “Based on my previous experience, I had walked through the field, but returned with empty hands. This time, I saw this tree, and I felt that it was the first good tree I had seen, so I chopped it down and brought it back. I did not want to miss the chance.” Socrates then said, “And that is MARRIAGE. On another day, Plato asked his teacher, “What is life?” Socrates asked him to go to the forest again, allowed back and forth as well, and pluck the most beautiful flower. Plato walked forward. However he hadn’t come back for 3 days. His teacher went to find him. When he saw Plato’s camping in the forest, he asked:” Have you found the most beautiful flower?” Plato pointed a flower near to his camp and answered, “This is the most beautiful flower!” “Why didn’t you take it out?” Socrates asked. “Because if I pick it, it would be drooping. Even though I didn’t pick, it would die in a couple of days for sure. So I had been living by its side while it was blooming. When it’s drooped, I was up to find another one. This is the second most beautiful flower I have found!” Socrates then said, “You’ve got the truth of LIFE” “Love” is the most beautiful thing to happen to a person, it’s an opportunity you don’t realize its worth when you have it but only when it’s gone like the field of stalks. “Marriage” is like the tree you chopped, it’s a compromise; you pick the first best thing you see and learn to live a happy life with it. Having an affair is alluring. It’s like lightning - bright but disappeared so quickly that you cannot catch up with and keep it. “Life” is to follow and enjoy the every beautiful moment of living. That’s why you should enjoy your life wherever you live. 有一天，古希腊哲学家柏拉图问他的老师苏格拉底什么是爱情，他的老师就叫他先到麦田里，摘一棵全麦田里最大最金黄的的麦穗。期间只能摘一次，并且只可以向前走，不能回头。柏拉图于是照着老师的说话做。结果，他两手空空的走出麦田。老师问他为什么摘不到，他说：“因为只能摘一次，又不能走回头路，其间即使见到一棵又大又金黄的，因为不知前面是否有更好，所以没有摘；走到前面时，又发觉总不及之前见到的好，原来麦田里最大最金黄的麦穗，早就错过了；于是，我什么也没摘到。” 苏格拉底说：“这就是爱情。” 之后又有一天，柏拉图问他的老师什么是婚姻，他的老师就叫他先到树林里，砍下一棵全树林最大最茂盛、最适合放在家作圣诞树的树。其间同样只能摘一次，以及同样只可以向前走，不能回头。柏拉图于是照着老师的说话做。今次，他带了一棵普普通通，不是很茂盛，亦不算太差的树回来。老师问他，怎么带这棵普普通通的树回来。他说：“有了上一次经验，当我走到大半路程，已经感到累了却还两手空空时，我觉得虽然树林里还有很多树，但这棵树还是挺不错的，便砍下来，免得最后又什么也带不出来。” 苏格拉底说：“这就是婚姻。” 又有一天柏拉图又问老师苏格拉底什么是生活，苏格拉底还是叫他到树林走一次。要求是随便走，在途中要取一支最好看的花。柏拉图有了以前的教训又充满信心地出去过了三天三夜，他也没有回来。苏格拉底只好走进树林里去找他，最后发现柏拉图已在树林里安营扎寨。苏格拉底问他：“你找着最好看的花么？” 柏拉图指着边上的一朵花说：“这就是最好看的花。” 苏格拉底问：“为什么不把它带出去呢？” 柏拉图回答老师： “我如果把它摘下来，它马上就枯萎。即使我不摘它，它也迟早会枯。所以我就在它还盛开的时候，住在它边上。等它凋谢的时候，再找下一朵。这已经是我找着的第二朵最好看的花了。” 苏格拉底说： “你已经懂得生活的真谛了。” 爱情给人经历和回忆，之后，婚姻靠的是明智的决定和好好的把握，经过了这些考验，到最后才会明白生活是一种珍惜和守护。 柏拉图麦穗问题的数学解答 现在我们用数学的角度来讨论这个问题。 假设我们碰到的麦穗有 n 个，我们用这样的策略来选麦穗，前 k 个，记住一个最大的麦穗记为 d（可能是重量，也可能是体积），然后 k + 1 个开始，只要大于 d 的，就选择，否则就不选择。 对于某个固定的 k，如果最大的麦穗出现在了第 i 个位置（k &lt; i ≤ n），要想让他有幸正好被选中，就必须得满足前 i - 1 个麦穗中的最好的麦穗在前 k 个麦穗里，这有 k &#x2F; (i - 1) 的可能。考虑所有可能的 i，我们便得到了前 k 个麦穗作为参考，能选中最大麦穗的总概率 P(k)： 设 k &#x2F; n &#x3D; x，并且假设 n 充分大，则上述公式可以改为： 对 x·ln(x) 求导，并令这个导数为 0，可以解出 x 的最优值，它就是欧拉研究的神秘常数的倒数 1 &#x2F; e. 所以 k &#x3D; n &#x2F; e. 如果你想摘取最大的麦穗，假设有 n 个麦穗，你应该先将前 n &#x2F; e 个麦穗作为参考，然后再 k + 1 个麦穗开始选择比前面 k 个最大的麦穗即可。 e &#x3D; 2.718281828459 1 &#x2F; e &#x3D; 0.36787944117144 其他例子一、一楼到十楼的每层电梯门口都放着一颗钻石，钻石大小不一。你乘坐电梯从一楼到十楼，每层楼电梯门都会打开一次，只能拿一次钻石，问怎样才能拿到最大的一颗。 首先，这个题目说的，并不能完全拿到最大的钻石。但可以保证拿到最大钻石的概率最大。10 &#x2F; e &#x3D; 3.67，向上取整得 4。前四层皆不取，只记下最大的。后面遇到的，只要比前面最大的还大，取之即可。 二、秘书问题。在机率及博弈论上，秘书问题（类似名称有相亲问题、止步问题、见好就收问题、苏丹的嫁妆问题、挑剔的求婚者问题等) 内容是这样的： 要聘请一名秘书，有 n 人来面试。每次面试一人，面试过后便要即时决定聘不聘他，如果当时决定不聘他，他便不会回来。面试时总能清楚了解求职者的适合程度，并能和之前的每个人作比较。问凭什么策略，才使选得到最适合担任秘书的人的机率最大？ References 麦穗理论，又名“秘书问题” (1&#x2F;e 处为最优分割点), http://geek.csdn.net/news/detail/231497","tags":[]},{"title":"Why Functional Programming?","date":"2017-08-17T13:15:35.000Z","path":"2017/08/17/Why-functional-programming/","text":"Why Static Type? 性能 - 方法调用速度更快，因为不需要在运行时才来判断调用的是哪个方法。 可靠性 - 编译器验证了程序的正确性，因而运行时崩溃的概率更低。 可维护性 - 陌生代码更容易维护，因为你可以看到代码中用到的对象的类型。 工具支持 - 静态类型使 IDE 能提供可靠的重构、精确的代码补全以及其他特性。 Benefit of Functional Programming 头等函数 - 把函数（一小段行为）当作值使用，可以用变量保存它，把它当作参数传递，或者当作其他函数的返回值。 不可变性 - 使用不可变对象，这保证了它们的状态在其创建之后不能再变化。 无副作用 - 使用的是纯函数。此类函数在输入相同时会产生同样的结果，并且不会修改其他对象的状态，也不会和外面的世界交互。 简洁 函数式风格的代码 比相应的命令式风格的代码更优雅、更简练，因为把函数当作值可以让你获得更强大的抽象能力，从而避免重复代码。 假设你有两段类似的代码，实现相似的任务但具体细节略有不同，可以轻易地将这段逻辑中公共的部分提取到一个函数中，并将其他不同的部分作为参数传递给它。这些参数本身也是函数，但你可以使用一种简洁的语法来表示这些匿名函数，被称作 lambda 表达式。 多线程安全 多线程程序中最大的错误来源之一就是，在没有采用适当同步机制的情况下，在不同的线程上修改同一份数据。如果你使用的是不可变数据结构和纯函数，就能保证这样不安全的修改根本不会发生，也就不需要考虑为其设计复杂的同步方案。 测试更加容易 没有副作用的函数可以独立地进行测试，因为不需要写大量的设置代码来构造它们所依赖的整个环境。 Functional programming, views a program as a mathematical function which is evaluated to produce a result value. That function may call upon nested functions, which in turn may call upon more nested functions. A nested function evaluates to produce a result. From there, that result is passed on to the enclosing function, which uses the nested function values to calculate its own return value. To enable functions to easily pass data to and from other functions, functional programming languages typically define data structures in the most generic possible way, as a collection of (any) things. They also allow functions to be passed to other functions as if they were data parameters. A function in this paradigm is not allowed to produce any side effects such as modifying a global variable that maintains state information. Instead, it is only allowed to receive parameters and perform some operations on them in order to produce its return value. Executing a functional program involves evaluating the outermost function, which in turn causes evaluation of all the nested functions, recursively down to the most basic functions that have no nested functions. Why is functional programming a big deal? Clarity Programming without side effects creates code that is easier to follow - a function is completely described by what goes in and what comes out. A function that produces the right answer today will produce the right answer tomorrow. This creates code that is easier to debug, easier to test, and easier to re-use. Brevity In functional languages, data is implicitly passed from a nested function to its parent function, via a general-purpose collection data type. This makes functional programs much more compact than those of other paradigms, which require substantial “housekeeping” code to pass data from one function to the next. Efficiency Because functions do not have side effects, operations can be re-ordered or performed in parallel in order to optimize performance, or can be skipped entirely if their result is not used by any other function. References Kotlin 初体验：主要特征与应用, http://geek.csdn.net/news/detail/231497 The Rise and Fall of Scala, https://dzone.com/articles/the-rise-and-fall-of-scala","tags":[]},{"title":"股市投资三要素","date":"2017-08-04T12:16:44.000Z","path":"2017/08/04/股市投资三要素/","text":"不要借钱投资 只买有长期投资价值的公司 不要用短期的钱去买长线的股票","tags":[]},{"title":"Customisation of Filco Majestouch 2 Mechanical Keyboard","date":"2017-08-01T16:29:45.000Z","path":"2017/08/02/Customisation-of-Filco-Majestouch-2-Mechanical-Keyboard/","text":"A Filco Majestouch 2 Tenkeyless Mechanical Keyboard, with Cherry Brown switches. It has been replaced with GMK Honeywell keycaps from Originative Co. (https://originative.co/products/honeywell) soon after it bought. Replace Filco Majestouch 2’s with GMK Honeywell keycaps. After joined Massdrop Lambo 80% Anodized Aluminum Case for Filco 87 TKL campaign (https://www.massdrop.com/buy/lambo-80-anodized-aluminum-case-for-filco-87-tkl) After a few months waiting, case delivered in, shipped from USA. Get hands warmed up and dirty. Now, show time.","tags":[]},{"title":"极简之道 - The interface 人与机器的思想交流","date":"2017-07-16T07:50:43.000Z","path":"2017/07/16/极简之道-The-interface-人与机器的思想交流/","text":"Why 60%? GH60 GH60 可编程键盘 (http://blog.komar.be/projects/gh60-programmable-keyboard/), it’s Poker 2 键盘的 rip-off。开放式公板设计。电路板中国制造。三周前，在 AliExpress (https://www.aliexpress.com/item/Customized-DIY-GH60-Case-Shell-PCB-Plate-Switches-LED-Kit-60-Mechanical-Keyboard-Satan-Poker2-GH/32651474350.html) 下的订单。 Cherry MX Switch 德国 Cherry 工厂的茶轴。敲起键盘来极有段落，层次感。 iQunix Lambo 拆掉原装的塑料键盘壳。换装 iQunix Lambo (https://www.aliexpress.com/item/Iqunix-lambo-60-mechanical-keyboard-anode-alumina-shell-base-gh60-poker2/32677061753.html) 铝制外壳。 GMK 3Run Keycap Set 最后再装上德国 GMK 工厂造的 3Run ABS keycaps 后 (https://www.massdrop.com/buy/gmk-3run-keycap-set) ，一个拥有自己 signature，体现个性，品味的机械键盘诞生了。 极简之道 &#x2F; Simplicity","tags":[]},{"title":"IT 的终点是艺术 - The end of IT is Art","date":"2017-07-02T08:26:23.000Z","path":"2017/07/02/The-end-of-IT-is-Art/","text":"Modern, postmodern, and contemporary IT and its history.","tags":[]},{"title":"Ant Colony Optimization (ACO)","date":"2017-06-30T12:46:52.000Z","path":"2017/06/30/Ant-Colony-Optimization-ACO/","text":"Ant Colony Optimization (ACO) for the the Traveling Salesman Problem (TSP). In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs. 蚁群算法是一种用来寻找优化路径的概率型算法。它由 Marco Dorigo 于 1992 年在他的博士论文中提出，其灵感来源于蚂蚁在寻找食物过程中发现路径的行为。这种算法具有分布计算、信息正反馈和启发式搜索的特征，本质上是进化算法中的一种启发式全局优化算法。 蚁群系统 (Ant System 或 Ant Colony System) 是由意大利学者 Dorigo、Maniezzo 等人于 20 世纪 90 年代首先提出来的。他们在研究蚂蚁觅食的过程中，发现单个蚂蚁的行为比较简单，但是蚁群整体却可以体现一些智能的行为。例如蚁群可以在不同的环境下，寻找最短到达食物源的路径。这是因为蚁群内的蚂蚁可以通过某种信息机制实现信息的传递。后又经进一步研究发现，蚂蚁会在其经过的路径上释放一种可以称之为“信息素”的物质，蚁群内的蚂蚁对“信息素”具有感知能力，它们会沿着“信息素”浓度较高路径行走，而每只路过的蚂蚁都会在路上留下“信息素”，这就形成一种类似正反馈的机制，这样经过一段时间后，整个蚁群就会沿着最短路径到达食物源了。 将蚁群算法应用于解决优化问题的基本思路为：用蚂蚁的行走路径表示待优化问题的可行解，整个蚂蚁群体的所有路径构成待优化问题的解空间。路径较短的蚂蚁释放的信息素量较多，随着时间的推进，较短的路径上累积的信息素浓度逐渐增高，选择该路径的蚂蚁个数也愈来愈多。最终，整个蚂蚁会在正反馈的作用下集中到最佳的路径上，此时对应的便是待优化问题的最优解。 蚂蚁找到最短路径要归功于信息素和环境，假设有两条路可从蚁窝通向食物，开始时两条路上的蚂蚁数量差不多：当蚂蚁到达终点之后会立即返回，距离短的路上的蚂蚁往返一次时间短，重复频率快，在单位时间里往返蚂蚁的数目就多，留下的信息素也多，会吸引更多蚂蚁过来，会留下更多信息素。而距离长的路正相反，因此越来越多的蚂蚁聚集到最短路径上来。 蚂蚁具有的智能行为得益于其简单行为规则，该规则让其具有多样性和正反馈。在觅食时，多样性使蚂蚁不会走进死胡同而无限循环，是一种创新能力；正反馈使优良信息保存下来，是一种学习强化能力。两者的巧妙结合使智能行为涌现，如果多样性过剩，系统过于活跃，会导致过多的随机运动，陷入混沌状态；如果多样性不够，正反馈过强，会导致僵化，当环境变化时蚁群不能相应调整。 与其他优化算法相比，蚁群算法具有以下几个特点： (1) 采用正反馈机制，使得搜索过程不断收敛，最终逼近最优解。(2) 每个个体可以通过释放信息素来改变周围的环境，且每个个体能够感知周围环境的实时变化，个体间通过环境进行间接地通讯。(3) 搜索过程采用分布式计算方式，多个个体同时进行并行计算，大大提高了算法的计算能力和运行效率。(4) 启发式的概率搜索方式不容易陷入局部最优，易于寻找到全局最优解。 该算法应用于其他组合优化问题，如旅行商问题、指派问题、Job Shop 调度问题、车辆路由问题、图着色问题和网络路由问题等。最近几年，该算法在网络路由中的应用受到越来越多学者的关注，并提出了一些新的基于蚂蚁算法的路由算法。同传统的路由算法相比较，该算法在网络路由中具有信息分布式性、动态性、随机性和异步性等特点，而这些特点正好能满足网络路由的需要。 VisualizationA visual demo of Ant Colony Optimisation written in Javascript (ES6): Another visual demo of Ant Colony Optimisation: References Ant colony optimization algorithms, https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms Visualisation of Ant Colony Optimisation, http://poolik.github.io/visual-aco/#/visualisation A visual demo of Ant Colony Optimisation applied to TSP written in Javascript, http://gordyd.github.io/js-aco/ 百度百科 - 蚁群算法, https://wapbaike.baidu.com/item/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95","tags":[]},{"title":"Influence Without Authority","date":"2017-03-12T11:31:23.000Z","path":"2017/03/12/Influence-Without-Authority/","text":"The Psychology of Persuasion.","tags":[]},{"title":"Spring Data - powerful and succinct abstraction","date":"2017-03-03T23:05:30.000Z","path":"2017/03/04/Spring-Data-powerful-and-succinct-abstraction/","text":"Database tier definition Database tables, indexes and foreign keys defined in Liquibase configuration: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122databaseChangeLog: - changeSet: id: 1 author: Terrence Miao changes: - createTable: tableName: draft_order columns: - column: name: id type: int autoIncrement: true constraints: primaryKey: true nullable: false - column: name: c_number type: varchar(32) constraints: nullable: false - column: name: source_time_in_ms type: bigint constraints: nullable: false - column: name: source_item_id type: varchar(255) constraints: nullable: false - column: name: shipment type: json constraints: nullable: false - column: name: shipment_id type: varchar(255) constraints: nullable: true - column: name: quantity type: int constraints: nullable: false - column: name: source_system type: varchar(255) constraints: nullable: false - column: name: status type: varchar(32) constraints: nullable: false - createIndex: columns: - column: name: source_item_id indexName: idx_source_item_id tableName: draft_order unique: false - createIndex: columns: - column: name: c_number - column: name: source_item_id indexName: idx_c_number_source_item_id tableName: draft_order unique: true - createTable: tableName: draft_order_combined columns: - column: name: id type: int autoIncrement: true constraints: primaryKey: true nullable: false - column: name: combined_id type: varchar(64) constraints: nullable: false - column: name: draft_order_id type: int constraints: nullable: false - addForeignKeyConstraint: baseColumnNames: draft_order_id baseTableName: draft_order_combined constraintName: fk_draft_order_combined_draft_order onDelete: CASCADE onUpdate: RESTRICT referencedColumnNames: id referencedTableName: draft_order - changeSet: id: 2 author: Terrence Miao changes: - addColumn: columns: - column: # For MySQL 5.7.x above, the first TIMESTAMP column in the table gets current timestamp as the default value, likely. So # if an INSERT or UPDATE without supplying a value, the column will get the current timestamp. Any subsequent TIMESTAMP # columns should have a default value explicitly defined. If you have two TIMESTAMP columns and if you don&#x27;t specify a # default value for the second column, you will get this error while trying to create the table: # ERROR 1067 (42000): Invalid default value for &#x27;COLUMN_NAME&#x27; name: date_created type: timestamp(3) constraints: nullable: false - column: name: date_updated type: timestamp(3) defaultValueComputed: LOCALTIMESTAMP(3) constraints: nullable: false tableName: draft_order DAO definition Draft Order 1234567891011121314151617181920212223242526272829303132@Entity@Table(name = &quot;draft_order&quot;)public class DraftOrder implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Integer id; @Column(name = &quot;c_number&quot;) private String cNumber; @Column(name = &quot;source_time_in_ms&quot;) private Long sourceTimeInMs; @Column(name = &quot;source_item_id&quot;) private String sourceItemId; @Column(name = &quot;shipment&quot;, columnDefinition = &quot;json&quot;) private String shipment; @Column(name = &quot;shipment_id&quot;) private String shipmentId; @Column(name = &quot;quantity&quot;) private Integer quantity; @Column(name = &quot;source_system&quot;) private String sourceSystem; @Column(name = &quot;status&quot;) private String status;&#125; Draft Order Combined 123456789101112131415@Entity@Table(name = &quot;draft_order_combined&quot;)public class DraftOrderCombined implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Integer id; @Column(name = &quot;combined_id&quot;) private String combinedId; @OneToOne(cascade = CascadeType.ALL) @JoinColumn(name = &quot;draft_order_id&quot;) private DraftOrder draftOrder;&#125; An middle Aggregation Object 123456789101112public class CombinedIdSourceTimeInMs &#123; private Long counter; private String combinedId; private Long sourceTimeInMs; public CombinedIdSourceTimeInMs(Long counter, String combinedId, Long sourceTimeInMs) &#123; this.counter = counter; this.combinedId = combinedId; this.sourceTimeInMs = sourceTimeInMs; &#125;&#125; CRUD Repository definition DraftOrderRepository 1234567891011121314public interface DraftOrderRepository extends CrudRepository&lt;DraftOrder, Integer&gt; &#123; List&lt;DraftOrder&gt; findByCNumberAndStatusOrderBySourceTimeInMsDesc(String cNumber, String status, Pageable pageable); List&lt;DraftOrder&gt; findByCNumberAndSourceItemIdIn(String cNumber, List&lt;String&gt; sourceItemIds); DraftOrder findByCNumberAndSourceItemId(String cNumber, String sourceItemId); List&lt;DraftOrder&gt; findByShipmentIdInAndStatusAndSourceSystem(List&lt;String&gt; shipmentIds, String status, String sourceSystem); List&lt;DraftOrder&gt; findByCNumberAndId(String cNumber, Integer id); Long countByCNumberAndStatus(String cNumber, String status);&#125; DraftOrderCombinedRepository 12345678910111213141516171819public interface DraftOrderCombinedRepository extends CrudRepository&lt;DraftOrderCombined, Integer&gt; &#123; String FIND_QUERY = &quot;SELECT new org.paradise.data.dao.CombinedIdSourceTimeInMs&quot; + &quot;(count(doc) as counter, doc.combinedId as combinedId, min(doc.draftOrder.sourceTimeInMs) as sourceTimeInMs) &quot; + &quot; FROM DraftOrderCombined doc WHERE doc.draftOrder.cNumber = :cNumber AND doc.draftOrder.status = :status &quot; + &quot; GROUP BY combinedId &quot; + &quot; ORDER BY sourceTimeInMs DESC&quot;; String COUNT_QUERY = &quot;SELECT count(1) FROM &quot; + &quot;(SELECT count(1) FROM DraftOrderCombined doc WHERE doc.draftOrder.cNumber = :cNumber AND doc.draftOrder.status = :status&quot; + &quot; GROUP BY doc.combinedId)&quot;; @Query(value = FIND_QUERY, countQuery = COUNT_QUERY) List&lt;CombinedIdSourceTimeInMs&gt; countPerCombinedIdAndSourceTimeInMs(@Param(&quot;cNumber&quot;) String cNumber, @Param(&quot;status&quot;) String status, Pageable pageable); List&lt;DraftOrderCombined&gt; findByCombinedIdOrderByDraftOrderDaoSourceTimeInMsDesc(String combinedId);&#125; References Spring Data JPA Reference Documentation, https://docs.spring.io/spring-data/jpa/docs/current/reference/html/","tags":[]},{"title":"SQL script generates random data and insert into MySQL database","date":"2017-03-03T12:02:17.000Z","path":"2017/03/03/SQL-script-generates-random-data-and-insert-into-MySQL-database/","text":"1DROP PROCEDURE InsertRandomRecords; 12345678910111213141516DELIMITER $$CREATE PROCEDURE InsertRandomRecords(IN NumRows INT) BEGIN DECLARE i INT; SET i = 1; START TRANSACTION; WHILE i &lt;= NumRows DO INSERT INTO draftorders.draft_order (c_number, source_time_in_ms, source_item_id, shipment, shipment_id, quantity, source_system, status) VALUES (&#x27;C01234567890&#x27;, RAND()*1000000000, CONCAT(&#x27;randomSourceRef-&#x27;, UUID_SHORT()), &#x27;&#123;&quot;to&quot;: &#123;&quot;name&quot;: &quot;T T&quot;, &quot;lines&quot;: [&quot;Lvl 100&quot;, &quot;123 smith st&quot;], &quot;phone&quot;: &quot;0356567567&quot;, &quot;state&quot;: &quot;VIC&quot;, &quot;suburb&quot;: &quot;Greensborough&quot;, &quot;postcode&quot;: &quot;3088&quot;, &quot;business_name&quot;: &quot;In debt&quot;&#125;, &quot;from&quot;: &#123;&quot;name&quot;: &quot;Carl Block&quot;, &quot;lines&quot;: [&quot;1341 Dandenong Road&quot;], &quot;state&quot;: &quot;VIC&quot;, &quot;suburb&quot;: &quot;Geelong&quot;, &quot;postcode&quot;: &quot;3220&quot;&#125;, &quot;items&quot;: [&#123;&quot;width&quot;: &quot;10&quot;, &quot;height&quot;: &quot;10&quot;, &quot;length&quot;: &quot;10&quot;, &quot;weight&quot;: &quot;10&quot;, &quot;product_id&quot;: &quot;3D85&quot;, &quot;item_reference&quot;: &quot;blocked&quot;, &quot;authority_to_leave&quot;: true, &quot;allow_partial_delivery&quot;: true, &quot;contains_dangerous_goods&quot;: true&#125;], &quot;shipment_reference&quot;: &quot;My second shipment ref&quot;, &quot;customer_reference_1&quot;: &quot;cr1234&quot;, &quot;customer_reference_2&quot;: &quot;cr5678&quot;&#125;&#x27;, UUID(), 1, &#x27;EBAY&#x27;, ELT(1 + FLOOR(RAND()*3), &#x27;DRAFT&#x27;, &#x27;READY_TO_SHIP&#x27;, &#x27;SHIPPED&#x27;)); SET i = i + 1; END WHILE; COMMIT; END$$DELIMITER ; To generate 1,000,000 draft orders: 1CALL InsertRandomRecords(1000000);","tags":[]},{"title":"Set up and run AWS Lambda 'hello' function with serverless","date":"2017-02-26T08:00:26.000Z","path":"2017/02/26/Set-up-and-run-AWS-Lambda-hello-function-with-serverless/","text":"serverlessWith latest Node.js 6.x.x installed, then install serverless globally: 1$ npm install serverless -g AWS LambdaCreate a AWS Lambda skeleton project with serverless: 1234567891011121314$ mkdir serverless-example &amp;&amp; cd $_$ sls create -t aws-nodejsServerless: Generating boilerplate... _______ __| _ .-----.----.--.--.-----.----| .-----.-----.-----.| |___| -__| _| | | -__| _| | -__|__ --|__ --||____ |_____|__| \\___/|_____|__| |__|_____|_____|_____|| | | The Serverless Application Framework| | serverless.com, v1.7.0 -------&#x27;Serverless: Successfully generated boilerplate for template: &quot;aws-nodejs&quot;Serverless: NOTE: Please update the &quot;service&quot; property in serverless.yml with your service name Policies set up for Lambda function For AWS user “ec2-user”, now need to have some policies with permissions to let “serverless” create role, Lambda function and deployment it … Roles for Lambda function Lambda function role created after Lambda function added and deployed into AWS. DeploymentMake sure AWS environment has been set up, including access key, user, group, policies … Pack and deploy Lambda example into AWS: 123456789101112131415161718$ sls deploy -r ap-southeast-2 -s devServerless: Packaging service...Serverless: Uploading CloudFormation file to S3...Serverless: Uploading service .zip file to S3 (583 B)...Serverless: Updating Stack...Serverless: Checking Stack update progress.....................Serverless: Stack update finished...Service Informationsservice: serverless-examplestage: devregion: ap-southeast-2api keys: Noneendpoints: Nonefunctions: serverless-example-dev-hello Lambda “hello” function A “hello” Lambda function has been created in Lambda after it’s deployed into AWS by “serverless”. Events generated during Lambda function deployment Deployment events generated during Lambda “hello” function deployed into AWS. Add Lambda Trigger on AWS API Gateway Manually create a Lambda Trigger. This time we use AWS API Gateway to trigger &#x2F; invoke Lambda “hello” function. Exposed Lambda API Gateway After Lambda Trigger created, an exposed RESTful interface for Lambda “hello” function. Say “hello”Set up AWS API Gateway trigger for Lambda “hello” function. Go to url, e.g.: Function “hello” log: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; &quot;message&quot;: &quot;Go Serverless v1.0! Your function executed successfully!&quot;, &quot;input&quot;: &#123; &quot;resource&quot;: &quot;/serverless-example-dev-hello&quot;, &quot;path&quot;: &quot;/serverless-example-dev-hello&quot;, &quot;httpMethod&quot;: &quot;GET&quot;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate, sdch, br&quot;, &quot;Accept-Language&quot;: &quot;en-AU,en-GB;q=0.8,en-US;q=0.6,en;q=0.4&quot;, &quot;CloudFront-Forwarded-Proto&quot;: &quot;https&quot;, &quot;CloudFront-Is-Desktop-Viewer&quot;: &quot;true&quot;, &quot;CloudFront-Is-Mobile-Viewer&quot;: &quot;false&quot;, &quot;CloudFront-Is-SmartTV-Viewer&quot;: &quot;false&quot;, &quot;CloudFront-Is-Tablet-Viewer&quot;: &quot;false&quot;, &quot;CloudFront-Viewer-Country&quot;: &quot;AU&quot;, &quot;Host&quot;: &quot;b5dyhej16l.execute-api.ap-southeast-2.amazonaws.com&quot;, &quot;Referer&quot;: &quot;https://ap-southeast-2.console.aws.amazon.com/lambda/home?region=ap-southeast-2&quot;, &quot;upgrade-insecure-requests&quot;: &quot;1&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36&quot;, &quot;Via&quot;: &quot;2.0 6884828476070d32978b45d03c1cc437.cloudfront.net (CloudFront)&quot;, &quot;X-Amz-Cf-Id&quot;: &quot;mvToMffe1AsUJNcMJKUh-Rx26oBJsRBe2n9I1df3xqIAIENPR_ku3A==&quot;, &quot;X-Amzn-Trace-Id&quot;: &quot;Root=1-58aae2ff-0b0c5e4059cc97576211ba4a&quot;, &quot;X-Forwarded-For&quot;: &quot;101.181.175.227, 54.239.202.65&quot;, &quot;X-Forwarded-Port&quot;: &quot;443&quot;, &quot;X-Forwarded-Proto&quot;: &quot;https&quot; &#125;, &quot;queryStringParameters&quot;: null, &quot;pathParameters&quot;: null, &quot;stageVariables&quot;: null, &quot;requestContext&quot;: &#123; &quot;accountId&quot;: &quot;624388274630&quot;, &quot;resourceId&quot;: &quot;5jbqsp&quot;, &quot;stage&quot;: &quot;prod&quot;, &quot;requestId&quot;: &quot;51ba2876-f769-11e6-b507-4b10c8a6886a&quot;, &quot;identity&quot;: &#123; &quot;cognitoIdentityPoolId&quot;: null, &quot;accountId&quot;: null, &quot;cognitoIdentityId&quot;: null, &quot;caller&quot;: null, &quot;apiKey&quot;: null, &quot;sourceIp&quot;: &quot;101.181.175.227&quot;, &quot;accessKey&quot;: null, &quot;cognitoAuthenticationType&quot;: null, &quot;cognitoAuthenticationProvider&quot;: null, &quot;userArn&quot;: null, &quot;userAgent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36&quot;, &quot;user&quot;: null &#125;, &quot;resourcePath&quot;: &quot;/serverless-example-dev-hello&quot;, &quot;httpMethod&quot;: &quot;GET&quot;, &quot;apiId&quot;: &quot;b5dyhej16l&quot; &#125;, &quot;body&quot;: null, &quot;isBase64Encoded&quot;: false &#125;&#125; References serverless framework, https://serverless.com/ Example source code and artefact, https://github.com/TerrenceMiao/AWS/tree/master/serverless-example","tags":[]},{"title":"The State Quadrants - Functional vs. Object Oriented","date":"2017-02-18T12:19:56.000Z","path":"2017/02/18/The-State-Quadrants-Functional-vs-Object-Oriented/","text":"","tags":[]},{"title":"Factorial function implementation in Java 8","date":"2017-02-18T11:15:33.000Z","path":"2017/02/18/Factorial-function-implementation-in-Java-8/","text":"Implementation123456789101112131415161718192021222324252627package org.paradise.function;import java.util.HashMap;import java.util.Map;import java.util.function.Function;/** * Created by terrence on 12/12/2016. */public final class FactorialFunction &#123; public static final Map&lt;Integer, Long&gt; FACTORIAL_MAP = new HashMap&lt;&gt;(); public static final Function&lt;Integer, Long&gt; FACTORIAL = (x) -&gt; FACTORIAL_MAP.computeIfAbsent(x, n -&gt; n * FactorialFunction.FACTORIAL.apply(n - 1)); static &#123; FACTORIAL_MAP.put(1, 1L); // FACTORIAL(1) &#125; private FactorialFunction() &#123; &#125;&#125; Unit test123456789101112131415161718192021package org.paradise.function;import org.junit.Test;import static org.junit.Assert.assertEquals;/** * Created by terrence on 12/12/2016. */public class FactorialFunctionTest &#123; @Test public void testFactorialFunction() throws Exception &#123; assertEquals(&quot;Incorrect result&quot;, Long.valueOf(1), FactorialFunction.FACTORIAL.apply(1)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(2), FactorialFunction.FACTORIAL.apply(2)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(3628800), FactorialFunction.FACTORIAL.apply(10)); &#125;&#125;","tags":[]},{"title":"Fibonacci function implementation in Java 8","date":"2017-02-18T11:09:56.000Z","path":"2017/02/18/Fibonacci-function-implementation-in-Java-8/","text":"Implementation12345678910111213141516171819202122232425262728package org.paradise.function;import java.util.HashMap;import java.util.Map;import java.util.function.Function;/** * Created by terrence on 12/12/2016. */public final class FibonacciFunction &#123; public static final Map&lt;Integer, Long&gt; FIBONACCI_MAP = new HashMap&lt;&gt;(); public static final Function&lt;Integer, Long&gt; FIBONACCI = (x) -&gt; FIBONACCI_MAP.computeIfAbsent(x, n -&gt; FibonacciFunction.FIBONACCI.apply(n - 2) + FibonacciFunction.FIBONACCI.apply(n - 1)); static &#123; FIBONACCI_MAP.put(0, 0L); // FIBONACCI(0) FIBONACCI_MAP.put(1, 1L); // FIBONACCI(1) &#125; private FibonacciFunction() &#123; &#125;&#125; Unit test12345678910111213141516171819202122232425262728293031package org.paradise.function;import org.junit.Test;import static org.junit.Assert.assertEquals;/** * Created by terrence on 12/12/2016. */public class FibonacciFunctionTest &#123; @Test public void testFibonacciFunction() throws Exception &#123; assertEquals(&quot;Incorrect result&quot;, Long.valueOf(0), FibonacciFunction.FIBONACCI.apply(0)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(1), FibonacciFunction.FIBONACCI.apply(1)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(1), FibonacciFunction.FIBONACCI.apply(2)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(2), FibonacciFunction.FIBONACCI.apply(3)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(3), FibonacciFunction.FIBONACCI.apply(4)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(5), FibonacciFunction.FIBONACCI.apply(5)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(8), FibonacciFunction.FIBONACCI.apply(6)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(13), FibonacciFunction.FIBONACCI.apply(7)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(21), FibonacciFunction.FIBONACCI.apply(8)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(34), FibonacciFunction.FIBONACCI.apply(9)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(55), FibonacciFunction.FIBONACCI.apply(10)); assertEquals(&quot;Incorrect result&quot;, Long.valueOf(12586269025L), FibonacciFunction.FIBONACCI.apply(50)); &#125;&#125;","tags":[]},{"title":"Remote debugging Java applications run on Tomcat","date":"2017-02-11T02:52:26.000Z","path":"2017/02/11/Remote-debugging-Java-applications-run-on-Tomcat/","text":"Enable JVM option to attach a remote debugger: 1$ export JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 “suspend” set to “y” is to let remote debugger start loading the application. Now, start running Tomcat. JVM debugging port is bound on port 5005i on the machine runs on Tomcat. Next, set up SSH tunnel mirror remotei host (ip-10-213-79-77.ap-southeast-2.compute.internal) 5005 port to localhost on port 5005. For example: 1$ ssh -L 5005:ip-10-213-79-77.ap-southeast-2.compute.internal:5005 -l ec2-user ip-10-213-79-77.ap-southeast-2.compute.internal You can start remote debugging in IDE like IntelliJ and debug the code since.","tags":[]},{"title":"Web Components are coming","date":"2017-02-01T12:56:19.000Z","path":"2017/02/01/Web-Components-are-coming/","text":"AngularJS is going to continue to succeed for some time. But change is inevitable. Web Components are coming. WebComponents create the ability to do all the sorts of markup-driven programming like AngularJS, ReactJS, but less ecosystem dependent. Because DOM is integration point for all the kinds of JavaScript frameworks. Web Components make it MUCH easier to interoperate between components. The future isn’t here yet, but it will change fundamental assumptions about how a JavaScript framework should act and what it should be responsible for. Those shifts in assumptions frequently cause frameworks will drop out of the ecosystem quickly than expected. URL: https://www.webcomponents.org/","tags":[]},{"title":"Perfection","date":"2017-02-01T12:35:24.000Z","path":"2017/02/01/Perfection/","text":"Il semble que la perfection soit atteinte non quand il n’y a plus rien à ajouter, mais quand il n’y a plus rien à retrancher. 1- Antoine de saint Exupery It seems that perfection is attained not when there is nothing more to add, but when there is nothing more to remove.","tags":[]},{"title":"Setup Docker Private Registry in Nexus Repository OSS 3.x.x","date":"2017-01-28T03:37:50.000Z","path":"2017/01/28/Setup-Docker-Private-Registry-in-Nexus-Repository-OSS-3-x-x/","text":"Make sure Nexus Repository has been setup with Self Signed certificate, certificate for host&#x2F;server e.g. “silencer.bigpond”. The following instructions have been successfully tested in Nexus version 3.2.0-01. Create Docker Hub repository in Nexus Create Docker Internal repository in Nexus Create Docker Group repository in Nexus Run Docker with Docker NativeAdd Docker Private Registry in Insecure Registries Now this approach supports docker pull and docker push. Work around with “x509: certificate signed by unknown authority“ error by adding “–disable-content-trust” option on docker push command line if Docker doesn’t accept Self-Signed certificate. Add Docker Private Registry server’s certificate into Docker Virtual Machine CA list1234567891011121314151617181920𝜆 keytool -printcert -rfc -sslserver silencer.bigpond:8444 &gt; silencer.bigpond.pem-----BEGIN CERTIFICATE-----MIIDkDCCAnigAwIBAgIEAqo9kTANBgkqhkiG9w0BAQsFADBgMQswCQYDVQQGEwJBVTEUMBIGA1UECBMLVW5zcGVjaWZpZWQxFDASBgNVBAcTC1Vuc3BlY2lmaWVkMREwDwYDVQQKEwhTb25hdHlwZTESMBAGA1UEAwwJKi5iaWdwb25kMB4XDTE2MDYxMjExMzY1M1oXDTMwMDIxOTExMzY1M1owYDELMAkGA1UEBhMCQVUxFDASBgNVBAgTC1Vuc3BlY2lmaWVkMRQwEgYDVQQHEwtVbnNwZWNpZmllZDERMA8GA1UEChMIU29uYXR5cGUxEjAQBgNVBAMMCSouYmlncG9uZDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJNO5mDpBDQQ8n4t0P2z8ChWzIFQ3Pf+5U8x6P17O3WtKTfsbuRYobHYmas5tVdVdnLIqpb4JV9DWIoS+CNG6cRLy3GIWWT7CbjsrpDlOTArslvk3KuzQ0dsZNflRfdd+ughI2LczehEfhzPJzA+ZU8Am1CadM+VUa+T6MilFQMXpWfjND6BNnV+qr/MX1QQfSjiWt7oWBex0BB0VPv9ooBZUqO+8jk5fUY8wEIa/kqLUqIKGxIUx9BMQBwBJwDKZmK93DXSPvAFYbKQjj6/nbV9R1VWmR7fhkLG+Ixlx5ld2dxv4+xvXmS8s4NanBtKZWUfEYVPp7gUF9HZoW9A1jcCAwEAAaNSMFAwDAYDVR0TBAUwAwEB/zAhBgNVHREEGjAYghBzaWxlbmNlci5iaWdwb25khwQKAAAJMB0GA1UdDgQWBBQV3WTuC+GI8lHtH0uL+kYqTG+vczANBgkqhkiG9w0BAQsFAAOCAQEAUwL+qnKVT0ENZEZnDjB+cjPfvkeWOD05PrGUOn4YB4vllq2S6Cgfm0OaZ+vMt3KMXPf9pIgZ797jdPhOP/s5IVJItldky+u/Hk9gNtUwEjpgl0MjhSm/PqxR5XoJdkYlvUdtq+PTrU5RU3v3GImeOmlI4mM5PaZ6OT8HC5VMX5s9RawBr/5EbJHRM7EN8r3g4Y/2109YoHoiWAhnN6TC3RhmCoQqGOiPsS732KHUz3KqXVbq9VTRdA3dXqFj1cUSet1TXTPisaiehffvbqYm2vrJ5WYgqCwb8TadDg66TToj080qvA8cXAF7qlA8pOImrbVOs7tdANSAs+AOcqCkiA==-----END CERTIFICATE----- 1𝜆 screen ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/tty Inside Docker Virtual Machine, follow these steps: 123$ sudo cat silencer.bigpond.pem &gt;&gt; /etc/ssl/certs/ca-certificates.crt$ sudo /etc/init.d/docker restart$ tail -f /var/lib/boot2docker/docker.log With Docker ToolboxAdd Docker Private Registry server’s certificate into Docker Virtual Machine CA list12𝜆 keytool -printcert -rfc -sslserver silencer.bigpond:8444 &gt; silencer.bigpond.pem𝜆 docker-machine ssh default Inside Docker Virtual Machine, follow these steps: 123$ sudo cat silencer.bigpond.pem &gt;&gt; /etc/ssl/certs/ca-certificates.crt$ sudo /etc/init.d/docker restart$ tail -f /var/lib/boot2docker/docker.log Press Ctrl + D and Ctrl + D to exist SCREEN program. Type “screen -r” to re-entry SCREEN program. Additional CertificatesDownload 3rd party Repository server’s certificate. Use tool like KeyStore Explorer to add 3rd party server certificates into existing keystore.jks. Due to a certificate chain that does not exist within the existing Java truststore, Java does not trust the certificate and fails to connect to the application. Test1234𝜆 docker login silencer.bigpond:18443Username: adminPassword: admin123Login Succeeded 12𝜆 docker search silencer.bigpond:18443/tomcat𝜆 docker pull silencer.bigpond:18443/jtech/tomcat:latest 1234𝜆 docker login silencer.bigpond:18444Username: adminPassword: admin123Login Succeeded 123𝜆 docker build -t jtech/camel-spring:latest .𝜆 docker tag jtech/camel-spring silencer.bigpond:18444/jtech/camel-spring:latest𝜆 docker push silencer.bigpond:18444/jtech/camel-spring:latest 1𝜆 docker push --disable-content-trust silencer.bigpond:18444/jtech/camel-spring:latest Note Due to Docker Virtual Machine is immutable, the CA certificate added change made inside VM is ephemeral, and lost after VM is restarted Docker Private Registry ONLY supports HTTPS, NOT HTTP Nexus repository MUST register and use server certificate for host e.g. “silencer.bigpond”, NOT “localhost.bigpond” to run Docker Private Registry Try with “–disable-content-trust” if error like “Get https://silencer.gateway:18444/v1/_ping: x509: certificate signed by unknown authority” Reference Private Registry for Docker, https://books.sonatype.com/nexus-book/3.0/reference/docker.html Using Self-Signed Certificates with Nexus Repository Manager and Docker Daemon, https://support.sonatype.com/hc/en-us/articles/217542177 SSL Certificate Guide, https://support.sonatype.com/hc/en-us/articles/213465768 Setup HTTPS access Nexus Repository, https://github.com/TerrenceMiao/nexus/wiki/Setup-HTTPS-access-Nexus-Repository-Manager-OSS-3.xx Add Self Signed CA certificate into Docker’s CA list, https://github.com/klippx/inject-docker-certs Adding Self Signed certificate in Docker Native for Mac, https://forums.docker.com/t/adding-self-signed-certificates/9761","tags":[]},{"title":"Setup HTTPS access in Nexus Repository Manager OSS 3.x.x","date":"2017-01-28T02:20:47.000Z","path":"2017/01/28/Setup-HTTPS-access-in-Nexus-Repository-Manager-OSS-3-x-x/","text":"Generate Self Signed certificateOn a Mac at home, with Bigpond internet access. Full host name is silencer.bigpond and IP Address is 10.0.0.9. 12terrence@Silencer /Applications/nexus-3.0.0-03/etc/ssl00:13:05 𝜆 keytool -genkeypair -keystore keystore.jks -storepass changeit -keypass changeit -alias jetty -keyalg RSA -keysize 2048 -validity 5000 -dname &quot;CN=*.bigpond, O=Sonatype, L=Unspecified, ST=Unspecified, C=AU&quot; -ext &quot;SAN=DNS:silencer.bigpond,IP:10.0.0.9&quot; -ext &quot;BC=ca:true&quot; OR run “nslookup 127.0.0.1” return full domain hostname e.g. “localhost.bigpond”. 12terrence@Silencer /Applications/nexus-3.0.0-03/etc/ssl00:13:05 𝜆 keytool -genkeypair -keystore keystore.jks -storepass changeit -keypass changeit -alias jetty -keyalg RSA -keysize 2048 -validity 5000 -dname &quot;CN=*.bigpond, O=Sonatype, L=Unspecified, ST=Unspecified, C=AU&quot; -ext &quot;SAN=DNS:localhost.bigpond,IP:127.0.0.1&quot; -ext &quot;BC=ca:true&quot; Now, with latest Nexus (version 3.2.0-01) you can use self-signed server certificate without specifying IP address. 12terrence@Silencer /usr/local/nexus-3.2.0-01/etc/ssl00:13:05 𝜆 keytool -genkeypair -keystore keystore.jks -storepass changeit -keypass changeit -alias jetty -keyalg RSA -keysize 2048 -validity 5000 -dname &quot;CN=*.gateway, O=Sonatype, L=Unspecified, ST=Unspecified, C=AU&quot; -ext &quot;SAN=DNS:silencer.gateway&quot; -ext &quot;BC=ca:true&quot; Enable HTTPS accessChange jetty-https.xml file: 123456789101112terrence@Silencer /Applications/nexus-3.0.0-03/etc00:18:59 𝜆 diff jetty-https.xml jetty-https.xml.orig25,26c25,26&lt; &lt;Set name=&quot;KeyStorePassword&quot;&gt;changeit&lt;/Set&gt;&lt; &lt;Set name=&quot;KeyManagerPassword&quot;&gt;changeit&lt;/Set&gt;---&gt; &lt;Set name=&quot;KeyStorePassword&quot;&gt;OBF:1v2j1uum1xtv1zej1zer1xtn1uvk1v1v&lt;/Set&gt;&gt; &lt;Set name=&quot;KeyManagerPassword&quot;&gt;OBF:1v2j1uum1xtv1zej1zer1xtn1uvk1v1v&lt;/Set&gt;28c28&lt; &lt;Set name=&quot;TrustStorePassword&quot;&gt;changeit&lt;/Set&gt;---&gt; &lt;Set name=&quot;TrustStorePassword&quot;&gt;OBF:1v2j1uum1xtv1zej1zer1xtn1uvk1v1v&lt;/Set&gt; Add SSL port and include jetty-https.xml in file: 12345678terrence@Silencer /Applications/nexus-3.0.0-03/etc00:19:06 𝜆 diff org.sonatype.nexus.cfg org.sonatype.nexus.cfg.orig3d2&lt; application-port-ssl=84445c4&lt; nexus-args=$&#123;karaf.etc&#125;/jetty.xml,$&#123;karaf.etc&#125;/jetty-http.xml,$&#123;karaf.etc&#125;/jetty-https.xml,$&#123;karaf.etc&#125;/jetty-http-redirect-to-https.xml,$&#123;karaf.etc&#125;/jetty-requestlog.xml---&gt; nexus-args=$&#123;karaf.etc&#125;/jetty.xml,$&#123;karaf.etc&#125;/jetty-http.xml,$&#123;karaf.etc&#125;/jetty-requestlog.xml Retrieve server’s certificate 1234567891011121314151617181920𝜆 keytool -printcert -rfc -sslserver silencer.bigpond:8444 &gt; silencer.bigpond.pem-----BEGIN CERTIFICATE-----MIIDkDCCAnigAwIBAgIEAqo9kTANBgkqhkiG9w0BAQsFADBgMQswCQYDVQQGEwJBVTEUMBIGA1UECBMLVW5zcGVjaWZpZWQxFDASBgNVBAcTC1Vuc3BlY2lmaWVkMREwDwYDVQQKEwhTb25hdHlwZTESMBAGA1UEAwwJKi5iaWdwb25kMB4XDTE2MDYxMjExMzY1M1oXDTMwMDIxOTExMzY1M1owYDELMAkGA1UEBhMCQVUxFDASBgNVBAgTC1Vuc3BlY2lmaWVkMRQwEgYDVQQHEwtVbnNwZWNpZmllZDERMA8GA1UEChMIU29uYXR5cGUxEjAQBgNVBAMMCSouYmlncG9uZDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJNO5mDpBDQQ8n4t0P2z8ChWzIFQ3Pf+5U8x6P17O3WtKTfsbuRYobHYmas5tVdVdnLIqpb4JV9DWIoS+CNG6cRLy3GIWWT7CbjsrpDlOTArslvk3KuzQ0dsZNflRfdd+ughI2LczehEfhzPJzA+ZU8Am1CadM+VUa+T6MilFQMXpWfjND6BNnV+qr/MX1QQfSjiWt7oWBex0BB0VPv9ooBZUqO+8jk5fUY8wEIa/kqLUqIKGxIUx9BMQBwBJwDKZmK93DXSPvAFYbKQjj6/nbV9R1VWmR7fhkLG+Ixlx5ld2dxv4+xvXmS8s4NanBtKZWUfEYVPp7gUF9HZoW9A1jcCAwEAAaNSMFAwDAYDVR0TBAUwAwEB/zAhBgNVHREEGjAYghBzaWxlbmNlci5iaWdwb25khwQKAAAJMB0GA1UdDgQWBBQV3WTuC+GI8lHtH0uL+kYqTG+vczANBgkqhkiG9w0BAQsFAAOCAQEAUwL+qnKVT0ENZEZnDjB+cjPfvkeWOD05PrGUOn4YB4vllq2S6Cgfm0OaZ+vMt3KMXPf9pIgZ797jdPhOP/s5IVJItldky+u/Hk9gNtUwEjpgl0MjhSm/PqxR5XoJdkYlvUdtq+PTrU5RU3v3GImeOmlI4mM5PaZ6OT8HC5VMX5s9RawBr/5EbJHRM7EN8r3g4Y/2109YoHoiWAhnN6TC3RhmCoQqGOiPsS732KHUz3KqXVbq9VTRdA3dXqFj1cUSet1TXTPisaiehffvbqYm2vrJ5WYgqCwb8TadDg66TToj080qvA8cXAF7qlA8pOImrbVOs7tdANSAs+AOcqCkiA==-----END CERTIFICATE----- To get another Source Code Repository server’s certificate 1𝜆 keytool -printcert -rfc -sslserver bitbucket.cd.paradise.org:443 &gt; bitbucket.cd.paradise.org.pem TestRestart Nexus and access: https://localhost:8444 Note Use utility tool “KeyStore Explorer” add additional CA certificates into keystore.jks file, especially when Gradle &#x2F; Maven output error like: 1&gt; sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target","tags":[]}]