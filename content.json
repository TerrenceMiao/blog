{"meta":{"title":"Terrence Miao's Adventures","subtitle":"A journey of a thousand miles begins with a single step","description":"Veni, vidi, vici","author":"Terrence Miao","url":"https://terrencemiao.github.io/blog"},"posts":[{"title":"The A.I. Revolution Will Not Be Supervised","date":"2018-03-11T09:22:21.000Z","path":"2018/03/11/The-A-I-Revolution-Will-Not-Be-Supervised/","text":"â€œThe A.I. Revolution Will Not Be Supervised.â€â€ - Yann LeCun","tags":[]},{"title":"RESTful calls to create Index and Mappings in ElasticSearch","date":"2018-03-09T23:58:36.000Z","path":"2018/03/10/RESTful-calls-to-create-Index-and-Mappings-in-ElasticSearch/","text":"There is a simple way, using RESTful client to create Index and Mappings in ElasticSearch, for example, with ElasticSearch Head plugin. Create Index in ElasticSearch with Settings JSON file. URL: http://localhost:9200/orders/, Method: PUT 123456789101112131415161718192021222324252627&#123; \"settings\": &#123; \"index\": &#123; \"number_of_shards\": 1, \"number_of_replicas\": 0, \"analysis\": &#123; \"filter\": &#123; \"partial_matching_filter\": &#123; \"type\": \"edge_ngram\", \"min_gram\": 1, \"max_gram\": 40 &#125; &#125;, \"analyzer\": &#123; \"partial_matcher\": &#123; \"type\": \"custom\", \"tokenizer\": \"standard\", \"filter\": [ \"lowercase\", \"partial_matching_filter\" ] &#125; &#125; &#125; &#125; &#125;&#125; Create Mappings in ElasticSearch with Mappings JSON file. URL: http://localhost:9200/orders/_mapping/order, Method: PUT 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; \"dynamic\": \"false\", \"_all\": &#123; \"enabled\": false&#125;, \"properties\": &#123; \"customer_id\": &#123; \"type\": \"keyword\" &#125;, \"encryption_key_id\": &#123; \"type\": \"keyword\", \"index\": \"false\" &#125;, \"metadata_tags\": &#123; \"type\": \"text\", \"analyzer\": \"whitespace\" &#125;, \"category\": &#123; \"type\": \"keyword\" &#125;, \"order_creation_date\": &#123; \"type\": \"date\", \"format\": \"date_optional_time\" &#125;, \"order_reference\": &#123; \"type\": \"text\" &#125;, \"order_id\": &#123; \"type\": \"keyword\" &#125;, \"to_name\": &#123; \"type\": \"text\", \"analyzer\": \"partial_matcher\", \"search_analyzer\": \"standard\" &#125;, \"to_suburb\": &#123; \"type\": \"text\", \"analyzer\": \"whitespace\" &#125;, \"to_state\": &#123; \"type\": \"text\", \"analyzer\": \"whitespace\" &#125;, \"to_postcode\": &#123; \"type\": \"text\", \"analyzer\": \"whitespace\" &#125;, \"to_email\": &#123; \"type\": \"text\", \"analyzer\": \"whitespace\" &#125; &#125;&#125;","tags":[]},{"title":"AWS access key id and secret access key","date":"2018-03-09T23:28:06.000Z","path":"2018/03/10/AWS-access-key-id-and-secret-access-key/","text":"There are some secret of AWS credentials, i.e., its Access Key ID and Secret Access Key, which let you connect AWS services withouth authentication. In a quintessential project environment, you have dev, test, prod environment setup in AWS. Your local AWS_PROFILE is something like this so you can switch to different targeting end services while run your application on localhost: 12345678910111213141516ğœ† cat ~/.aws/credentials[default]aws_access_key_id=dev-aws_access_key_idaws_secret_access_key=dev-aws_secret_access_key[dev]aws_access_key_id=dev-aws_access_key_idaws_secret_access_key=dev-aws_secret_access_key[test]aws_access_key_id=test-aws_access_key_idaws_secret_access_key=test-aws_secret_access_key[prod]aws_access_key_id=prod-aws_access_key_idaws_secret_access_key=prod-aws_secret_access_key When test Jest client in Test environment, a AWS ElasticSearch client library, error thrown: 1232018-02-27 15:18:19 INFO org.paradise.search.routes.ElasticSearchRoute - Body: io.searchbox.core.Index@4804b850[uri=orders/order/4SMK1UmbtqIAAAFhmYAFt9V7,method=PUT] message: searchIndexRoute - updating search index2018-02-27 15:18:19 INFO org.apache.camel.processor.interceptor.Tracer - &gt;&gt;&gt; (searchIndexRoute) org.paradise.search.routes.ElasticSearchRoute$$Lambda$137/819330075@119b837 --&gt; org.paradise.search.routes.ElasticSearchRoute$$Lambda$138/2035788375@2acacdf &lt;&lt;&lt; Pattern:InOnly, BodyType:io.searchbox.core.Index2018-02-27 15:18:19 ERROR org.paradise.search.routes.ElasticSearchRoute - Error while updating search index. 403 Forbidden &#123;\"Message\":\"User: arn:aws:iam::123456789012:user/svcbamboo is not authorized to perform: es:ESHttpPut on resource: paradise-esv5-test-esd\"&#125; at 'orders/order/4SMK1UmbtqIAAAFhmYAFt9V7' It turns out that WRONG Dev AWS_PROFILE applied in Test environment. Replacing â€œaws_access_key_idâ€ and â€œaws_secret_access_keyâ€ from the default profile with â€œaws_access_key_idâ€ and â€œaws_secret_access_keyâ€ from test profile: 12345678910111213141516ğœ† cat ~/.aws/credentials[default]aws_access_key_id=test-aws_access_key_idaws_secret_access_key=test-aws_secret_access_key[dev]aws_access_key_id=dev-aws_access_key_idaws_secret_access_key=dev-aws_secret_access_key[test]aws_access_key_id=test-aws_access_key_idaws_secret_access_key=test-aws_secret_access_key[prod]aws_access_key_id=prod-aws_access_key_idaws_secret_access_key=prod-aws_secret_access_key Rerun the test and in log: 1232018-02-27 15:23:54 INFO org.paradise.search.routes.ElasticSearchRoute - Body: io.searchbox.core.Index@62b18125[uri=orders/order/RVwK1UIBXmoAAAFhdJkFo9WA,method=PUT] message: searchIndexRoute - updating search index2018-02-27 15:23:54 INFO org.apache.camel.processor.interceptor.Tracer - &gt;&gt;&gt; (searchIndexRoute) org.paradise.search.routes.ElasticSearchRoute$$Lambda$137/1716909005@a809a62 --&gt; org.paradise.search.routes.ElasticSearchRoute$$Lambda$138/612681832@4b2713b1 &lt;&lt;&lt; Pattern:InOnly, BodyType:io.searchbox.core.Index2018-02-27 15:23:55 INFO org.paradise.search.routes.ElasticSearchRoute - Finished updating search index at 'orders/order/RVwK1UIBXmoAAAFhdJkFo9WA'.","tags":[]},{"title":"AWS ElasticSearch and Kibana proxy setup","date":"2018-02-17T09:26:41.000Z","path":"2018/02/17/AWS-ElasticSearch-and-Kibana-proxy-setup/","text":"There are several ways to access Amazon AWS ElasticSearch and Kibana services, which are HTTP based, without inject into HTTP request headers with authentication key â€¦ AWS ES ProxyInstall a proxy application - AWS ES/Kibana Proxy. Download it from https://www.npmjs.com/package/aws-es-kibana and install: 1$ npm install -g aws-es-kibana Run AWS ES/Kibana Proxy then connect to ElasticSearch and Kibana services on AWS: 123456789101112131415161718192021222324252627$ cat ~/.aws/config[default]output = jsonregion = ap-southeast-2[profile sandy-test]output = jsonregion = ap-southeast-2$ cat ~/.aws/credentials[default]aws_access_key_id=Academyaws_secret_access_key=Dingly Ding[ap-test]aws_access_key_id=Abracadbraaws_secret_access_key=Somewhere Over The Rainbow$ AWS_PROFILE=ap-test aws-es-kibana search-paradise-esv5-test-01-esd-blah23dlaoed81nz890adle4.ap-southeast-2.es.amazonaws.com__________ _________ _________________ ________ _________ |_ | / /_ ___/ ___ ____/_ ___/ ___ __ \\________________ ______ ____ /__ /| |_ | /| / /_____ \\ __ __/ _____ \\ __ /_/ /_ ___/ __ \\_ |/_/_ / / /_ /_ ___ |_ |/ |/ / ____/ / _ /___ ____/ / _ ____/_ / / /_/ /_&gt; &lt; _ /_/ / /_//_/ |_|___/|__/ /____/ /_____/ /____/ /_/ /_/ \\____//_/|_| _\\__, / (_) /____/AWS ES cluster available at http://127.0.0.1:9200Kibana available at http://127.0.0.1:9200/_plugin/kibana/ SSH TunnelSet up SSH Tunnel for AWS ElasticSearch https://search-paradise-esv5-test-01-esd-blah23dlaoed81nz890adle4.ap-southeast-2.es.amazonaws.com from 443 port to localhost 9200: 1ğœ† ssh -L 9200:search-paradise-esv5-test-01-esd-blah23dlaoed81nz890adle4.ap-southeast-2.es.amazonaws.com:443 -l ec2-user aws-jump-box Then access AWS ElasticSearch at: https://localhost:9200, AWS Kibana at: https://localhost:9200/_plugin/kibana ElasticSearch HeadWith plugin ElasticSearch Head, to query ElasticSearch, using URL and index â€œorders-search-boxâ€ e.g. https://localhost:9200/orders-search-test/, and context path â€œ_searchâ€","tags":[]},{"title":"Good things come to those who could wait till the last","date":"2018-02-06T12:35:51.000Z","path":"2018/02/06/Good-things-come-to-those-who-could-wait-till-the-last/","text":"And good things come in two (å¥½äº‹æˆåŒ)ã€‚ æƒ³å½“åˆä¹°è€³æœºæ—¶å¯¹å¸‚åœºåšè¿‡çš„ä¸€ç•ªè°ƒç ”ï¼Œå¯¹æ¬§æ´²ï¼Œæ—¥æœ¬å¤§å‚çš„äº§å“ä¹Ÿç•¥çŸ¥ä¸€äºŒã€‚å½“åŒäº‹ä»‹ç» Bluedio (è“å¼¦)ï¼Œä¸€å®¶ä»æ²¡å¬è¯´è¿‡çš„ä¸­å›½å¹¿å·å…¬å¸ç”Ÿäº§çš„è€³æœºæ—¶ï¼Œä»¥ä¸ºä»…æ˜¯ä¸€å®¶ Made In Chinaï¼Œä»·å»‰ä½è´¨çš„å±±å¯¨äº§å“ã€‚ä½†è¢«å‘ŠçŸ¥ Bluedio è€³æœºåœ¨äºšé©¬é€Šé”€é‡æ’è¡Œç¬¬äºŒåï¼Œç€å®åœ°å¤§åƒä¸€æ–¤ã€‚ Si vous voyez un entrepreneur chinois sauter par la fenÃªtre, suivez-le, il y a srement de bonnes affaires Ã  faire (If you see a Chinese entrepreneur jumping out the window, follow him, there is certainly of good bargains to be done). äºšé©¬é€Šçš„ç”¨æˆ·å¤šéå‘†ç“œã€‚å½“åƒç“œç¾¤ä¼—äº‰å…ˆæååœ° jumping out of window æ—¶ï¼Œyou have to follow. There is certainly of good bargains to be done. è´§æ¯”ä¸‰å®¶ååœ¨ Aliexpress ä¸‹äº†å•ã€‚ Bluedio Ai ä»æ¾³æ´²æœ¬åœŸå‘è´§ã€‚ä¸‰å¤©åæ”¶åˆ°ã€‚ æˆ‘å¯¹è¿åŠ¨è€³æœºè¦æ±‚ä¸é«˜ã€‚è½»ä¾¿ï¼Œèˆ’é€‚ï¼Œé”»ç‚¼æ—¶ä¸è¦æ€»åˆ†å¿ƒå®ƒä¼šæ‰ä¸‹æ¥ã€‚ç”¨è¿‡ä» $5 çš„ cheapy åˆ°é«˜ç«¯ Apple EarPodsï¼Œæ²¡ä¸€ä¸ªä»¤äººæ»¡æ„ã€‚ ç»è¿‡ä¸€æ®µæ—¶é—´çš„ä½¿ç”¨ï¼Œå·²æ„Ÿè§‰ Ai æ˜¯æˆ‘æ‰€æ±‚ã€‚è·‘æ­¥æ—¶æˆ´ä¸Šæ„Ÿè§‰ä¸åˆ°å­˜åœ¨ã€‚è€³æœºï¼Œäººä¸è‡ªç„¶è¾¾æˆå’Œè°ï¼Œå·²èä¸ºä¸€ä½“äº†ã€‚ ä»å›½å†…ç»é˜¿é‡Œç‰¹å¿«ï¼ˆå…è´¹çš„ï¼‰ï¼ŒBluedio F2 ä¸¤å‘¨åä¹Ÿåˆ°äº†ã€‚â€Fâ€ ä»£è¡¨ Faith (å¿ è¯š)ã€‚F2 åœ¨å¤–è§‚è®¾è®¡ä¸Šä¹Ÿå¿ è¯šåœ°æ‹·è´äº†è‘—åçš„ Bose QC-35ã€‚One on one æ¯”è¾ƒï¼ŒF2 åœ¨é‡é‡ä¸Šæ¯” QC-35 ä¸Šäº†ä¸€ä¸ªçº§åˆ«ã€‚ä½†å…¨é‡‘å± frameï¼Œè®©å¼ºçƒˆåŒæ¶å¡‘æ–™æ„Ÿçš„ç”¨æˆ·ä½“ä¼šåˆ°å‚å®¶çš„æ¸©æš–ã€‚è€³æœºå„ä¸ªæ–¹é¢åšå·¥ä¼˜å¼‚ï¼Œç”¨æ–™å®è¶³ï¼Œå¤šå¤„ç»†èŠ‚è¡¨ç°å‡ºåŒ äººç”¨å¿ƒã€‚ è¯•å¬äº† a wild range ä¸åŒé£æ ¼çš„éŸ³ä¹ä½œå“åï¼Œyou could feel the rich bass, warmer tone, not overpowering highs, and very detailed clarity from F2. éŸ³è´¨ä¸é«˜ç«¯è€³æœºè¶³æœ‰ä¸€æ‹¼ã€‚ Active Noise Cancelation (ANC) ä¸å¦‚ QC-35. å¤–ç•Œçš„èœ‚é¸£å£°åœ¨ç»è¿‡ noise cancelation åç»Ÿç»Ÿæ·¹æ²¡åœ¨ä¸€ç‰‡ç™½å™ªå£°ä¸­ã€‚ è°ˆåˆ°éŸ³è´¨ï¼ŒF2 è¿˜ä¸æ˜¯ Bluedio çš„æœ€å¼ºäº§å“ã€‚Bluedio æœ‰ä¸€æ¬¾ Victory ç³»åˆ—çš„ headphonesï¼Œwith 12 driversï¼Œç›¸å½“äºæœ‰åäºŒä¸ª speakers çš„ä¸€å¯¹éŸ³ç®± built-in å°å°çš„ headphones é‡Œï¼Œæä¾›äº† studio level çš„éŸ³è´¨äº«å—ã€‚å¯¹é«˜å“è´¨ç»§ç»­ following. Final thought. $52 for a é«˜æ€§ä»·æ¯”çš„ wireless wheadphone plus ANC like this, nothing could go wrong. æƒŠå–œä¹‹ä½™ï¼Œä½ ä¼šè§‰å¾—å¯¹è¿™ä¸ªè€³æœºæ¯ä¸€åˆ†é’±çš„æŠ•èµ„éƒ½æ˜¯ç‰©è¶…æ‰€å€¼ã€‚ Top range products on same market now like Sennheiser PXC 550 (Best Audio Quality), Sony WH1000XM2 (Most Elegant Design) and Bose QC 35 II (Peopleâ€™s Choice), cost 10x what you pay for Bluedioã€‚Bluedio åœ¨æœ‰äº›æ–¹é¢ä¸é¡¶çº§äº§å“è¿˜æœ‰è·ç¦»ï¼Œbut spend $500 for one æœ‰é¢å¤–äº«å—çš„ headphone æ˜¯å¦çœŸæœ‰æ‰€å€¼? Furthermore, think about if you wear them cross the road, and you are run over by car, that will seriously damage these expensive headphones. é•¿åŸæ°¸ä¸å€’ï¼Œå›½è´§å½“è‡ªå¼ºã€‚","tags":[]},{"title":"OnePlus 5T - a smartphone full of all the right ideas","date":"2018-01-12T08:13:47.000Z","path":"2018/01/12/OnePlus-5T-a-smartphone-full-of-all-the-right-ideas/","text":"Bought one OnePlus 5T Lava Red (ç†”å²©çº¢) special edition from OnePlus China website - http://rush.oneplus.cn/ in December 2017, when this edition is first available and exclusively sale in mainland China, which Red Colour is very popular in China that means bringing you a good fortune. Un-boxing Dual SIMsDual SIMs support. Having two mobile network carriers ALL in active mode. Performance Testby AnTuTu Benchmark by Geekbench by SpeedTest - ADSL2 connection Shooting TestSupport Dual Cameras and Portrait mode OnePlus Family OnePlus Oneï¼ŒOnePlus 2ï¼ŒOnePlus 3ï¼ŒOnePlus 5, along with OnePlus 5Tï¼Œä¸€éƒ¨å®Œå…¨ä¸€åŠ æ‰‹æœºçš„ç¼–å¹´å²ï¼Œæ›´æ˜¾ç”¨æˆ·å¯¹é«˜è´¨é‡ï¼Œé«˜æ€§ä»·æ¯”çš„ä¸€åŠ äº§å“çš„å–œçˆ±ä¸å¿ è¯šã€‚ What OnePlus stands for is a no-gimmicks smartphone manufacturer build around a community. For a good example, OnePlus 5T, a phone is full of all the right ideas - hardware design, software responsiveness, and overall usability. æ‰“åŠ¨ç”¨æˆ·çš„å¥½äº§å“è‡ªå·±ä¼šçƒ­é”€ï¼Œä¸åœ¨ä¹å–åˆ°å“ªé‡Œã€‚ä¸€åŠ æ‰€åšçš„ä¸€åˆ‡ï¼Œä¸è¿‡æ˜¯åŠªåŠ›æŠŠè¯¥åšçš„åšå¾—æ›´å¥½ã€‚ é•¿åŸæ°¸ä¸å€’ï¼Œå›½è´§å½“è‡ªå¼ºã€‚ Never Settle.","tags":[]},{"title":"First ever half marathon trial","date":"2017-10-28T23:18:10.000Z","path":"2017/10/29/First-ever-half-marathon-trial/","text":"Try the first ever half marathon today - Yarra Boulevard run. New experience such routine, Yarra Boulevard from suburb Kew to Hawthorn, either by walking or cycling. Terrible weather condition today, windy and hot. Have to stop several time to get water supplied in case of dehydrated. After 17km, left leg calf muscles start cramping, force me to reduce the pace. After 19km, right leg calf muscles also start cramping, but I keep moving forward. New new personal best of Performance Level - 55 after run. Meanwhile, remind me there is limits inside me, but I know I can finish it. Quotes from Johnnie Walker commercial: â€œWhen the sun came shining and I was strolling and the wheat fields waving and the dust clouds rolling. As the fog was lifting, a voice was chanting, this land was made for you and me.â€ â€œAs I was walking that ribbon of highway, I saw above me that endless sky way. I saw below me that golden valley, Esta tierra fue hecha para ti y para mÃ­.â€ â€œIâ€™ve roamed and rambled and Iâ€™ve followed my footsteps to the sparkling sands over diamond deserts, and all around me a voice was sounding, this land was made for you and me.â€ Keep running!","tags":[]},{"title":"æµ…è°ˆç¨‹åºå‘˜çš„æ•°å­¦ä¿®å…»","date":"2017-09-23T13:46:22.000Z","path":"2017/09/23/æµ…è°ˆç¨‹åºå‘˜çš„æ•°å­¦ä¿®å…»/","text":"å¯èƒ½æœ‰å¾ˆå¤šæœ‹å‹åœ¨ç½‘ä¸Šçœ‹è¿‡ Google å…¬å¸æ—©å‡ å¹´çš„æ‹›è˜å¹¿å‘Šï¼Œå®ƒçš„ç¬¬ä¸€é¢˜å¦‚ä¸‹äº†: {first 10-digit prime found in consecutive digits e}.com e ä¸­å‡ºç°çš„è¿ç»­çš„ç¬¬ä¸€ä¸ª10ä¸ªæ•°å­—ç»„æˆçš„è´¨æ•°ã€‚ æ®è¯´å½“æ—¶è¿™ä¸ªè¯•é¢˜åœ¨ç¾å›½å¾ˆå¤šåœ°é“çš„å‡ºç«™å£éƒ½æœ‰å¤§å¹…å¹¿å‘Šï¼Œåªè¦æ­£ç¡®è§£ç­”äº†è¿™é“é¢˜ï¼Œåœ¨æµè§ˆå™¨çš„åœ°å€æ ä¸­è¾“å…¥è¿™ä¸ªç­”æ¡ˆï¼Œå°±å¯ä»¥è¿›å…¥ä¸‹ä¸€è½®çš„æµ‹è¯•ï¼Œæ•´ä¸ªæµ‹è¯•è¿‡ç¨‹å¦‚åŒä¸€ä¸ªæ•°å­¦è¿·å®«ï¼Œç›´åˆ°ä½ æˆä¸º Google çš„ä¸€å‘˜ã€‚ åˆå¦‚ Intel æŸå¹´çš„ä¸€é“é¢è¯•é¢˜ç›®: å·´æ‹¿èµ«ç—…æ•…äº 1945å¹´8æœˆ31æ—¥ã€‚ä»–çš„å‡ºç”Ÿå¹´ä»½æ°å¥½æ˜¯ä»–åœ¨ä¸–æ—¶æŸå¹´å¹´é¾„çš„å¹³æ–¹ã€‚ é—®:ä»–æ˜¯å“ªå¹´å‡ºç”Ÿçš„? è¿™é“çœ‹ä¼¼å¾ˆç®€å•çš„æ•°å­¦é—®é¢˜ï¼Œä½ èƒ½ä¸èƒ½èƒ½å¿«åœ°è§£ç­”å‘¢? ä¸‹é¢åˆ™æ˜¯ä¸€é“ä¸–ç•Œç¬¬ä¸€å¤§è½¯ä»¶å…¬å¸å¾®è½¯çš„æ‹›è˜æµ‹è¯•é¢˜: ä¸­é—´åªéš”ä¸€ä¸ªæ•°å­—çš„ä¸¤ä¸ªç´ æ•°è¢«ç§°ä¸ºç´ æ•°å¯¹ï¼Œæ¯”å¦‚ 5 å’Œ 7ï¼Œ17 å’Œ 19ï¼Œè¯æ˜ç´ æ•°å¯¹ä¹‹é—´çš„æ•°å­—æ€»èƒ½è¢« 6 æ•´é™¤ (å‡è®¾è¿™ä¸¤ä¸ªç´ æ•°éƒ½å¤§äº 6)ï¼Œç°åœ¨è¯æ˜æ²¡æœ‰ç”±ä¸‰ä¸ªç´ æ•°ç»„æˆçš„ç´ æ•°å¯¹ã€‚ è¿™æ ·çš„è¯•é¢˜è¿˜æœ‰å¾ˆå¤šå¾ˆå¤šï¼Œè¿™äº›é¢˜ç›®ä¹åˆçœ‹ä¸Šå»éƒ½æ˜¯ä¸€äº›æ•°å­¦é—®é¢˜ã€‚ä½†æ˜¯ä¸–ç•Œä¸Šä¸€äº›è‘—åçš„å…¬å¸éƒ½æŠŠå®ƒä»¬ç”¨äºæ‹›è˜æµ‹è¯•ï¼Œå¯è§å®ƒä»¬å¯¹æ–°å‘˜å·¥æ•°å­¦åŸºç¡€çš„é‡è§†ã€‚æ•°å­¦è¯•é¢˜ä¸åº”ç”¨ç¨‹åºè¯•é¢˜æ˜¯è®¸å¤šå¤§å‹è½¯ä»¶å…¬å¸é¢è¯•ä¸­æŒ‡å‘æ€§æœ€æ˜æ˜¾çš„ä¸€ç±»è¯•é¢˜ï¼Œè¿™äº›è¯•é¢˜å°±æ˜¯è€ƒå¯Ÿåº”è˜è€…çš„æ•°å­¦èƒ½åŠ›ä¸è®¡ç®—æœºèƒ½åŠ›ã€‚æŸå’¨è¯¢å…¬å¸çš„ä¸€åé«˜çº§é¡¾é—®æ›¾è¯´: å¾®è½¯æ˜¯ä¸€å®¶ç”µè„‘è½¯ä»¶å…¬å¸ï¼Œå½“ç„¶è¦æ±‚å…¶å‘˜å·¥æœ‰ä¸€å®šçš„è®¡ç®—æœºå’Œæ•°å­¦èƒ½åŠ›ï¼Œé¢è¯•ä¸­è‡ªç„¶å°±ä¼šè€ƒå¯Ÿè¿™ç±»èƒ½åŠ›ã€‚å¾®è½¯çš„é¢è¯•é¢˜ç›®å°±è€ƒå¯Ÿäº†åº”è˜äººå‘˜å¯¹åŸºç¡€çŸ¥è¯†çš„æŒæ¡ç¨‹åº¦ã€å¯¹åŸºç¡€çŸ¥è¯†çš„åº”ç”¨èƒ½åŠ›ï¼Œç”šè‡³æš—å«äº†å¯¹è®¡ç®—æœºåŸºæœ¬åŸç†çš„è€ƒå¯Ÿã€‚æ‰€ä»¥ï¼Œè¿™æ ·çš„é¢è¯•é¢˜ç›®çš„ç¡®å¾ˆâ€œæ¯’è¾£â€ï¼Œè¶³ä»¥ç­›é€‰åˆ°åˆé€‚çš„äººã€‚ å››å·å¤§å­¦æ•°å­¦å­¦é™¢çš„æ›¹å¹¿ç¦æ•™æˆæ›¾è¯´è¿‡: â€œä¸€ä¸ªå¤§å­¦ç”Ÿå°†æ¥çš„ä½œä¸ºä¸ä»–çš„æ•°å­¦ä¿®å…»æœ‰å¾ˆå¤§çš„å…³ç³»â€ã€‚ å¤§å­¦è®¡ç®—æœºä¸“ä¸šå­¦ç”Ÿéƒ½æœ‰æ„Ÿè§¦ï¼Œè®¡ç®—æœºä¸“ä¸šè¯¾ç¨‹ä¸­æœ€éš¾çš„å‡ é—¨è¯¾ç¨‹è«è¿‡äºç¦»æ•£æ•°å­¦ã€ç¼–è¯‘åŸç†ã€æ•°æ®ç»“æ„ï¼Œå½“ç„¶åƒç»„åˆæ•°å­¦ã€å¯†ç å­¦ã€è®¡ç®—æœºå›¾å½¢å­¦ç­‰è¯¾ç¨‹ä¹Ÿä»¤è®¸å¤šäººå­¦èµ·æ¥ç›¸å½“åƒåŠ›ï¼Œå¾ˆå¤šè‡ªè®¤ä¸ºæ•°æ®åº“å­¦å¾—å¾ˆå¥½çš„å­¦ç”Ÿåœ¨èŒƒå¼ã€å‡½æ•°ä¾èµ–ã€ä¼ é€’ä¾èµ–ç­‰æ•°å­¦æ€§æ¯”è¾ƒå¼ºçš„æ¦‚å¿µé¢å‰æ„Ÿåˆ°åŠ›ä¸ä»å¿ƒï¼Œè¿™äº›éƒ½æ˜¯å› ä¸ºæ•°å­¦åŸºç¡€æˆ–è€…è¯´æ•°å­¦çŸ¥è¯†çš„ç¼ºä¹æ‰€é€ æˆçš„ã€‚æ•°å­¦æ˜¯è®¡ç®—æœºçš„åŸºç¡€ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆè€ƒè®¡ç®—æœºä¸“ä¸šç ”ç©¶ç”Ÿæ•°å­¦éƒ½é‡‡ç”¨æœ€éš¾è¯•é¢˜ (æ•°å­¦ä¸€) çš„åŸå› ï¼Œå½“ç„¶è¿™ä¹Ÿèƒ½ä¿ƒä½¿ä¸€äº›æ–°çš„äº¤å‰å­¦ç§‘å¦‚æ•°å­¦ä¸åº”ç”¨è½¯ä»¶ã€ä¿¡æ¯ä¸è®¡ç®—ç§‘å­¦ä¸“ä¸šç­‰é£é€Ÿå‘å±•ã€‚ è®¸å¤šå¤©æ‰ç¨‹åºå‘˜æœ¬èº«å°±æ˜¯æ•°å­¦å°–å­ï¼Œä¼—æ‰€å‘¨çŸ¥ï¼ŒBill Gates çš„æ•°å­¦æˆç»©ä¸€ç›´éƒ½å¾ˆæ£’ï¼Œä»–ç”šè‡³æ›¾ç»æœŸæœ›å½“ä¸€åæ•°å­¦æ•™æˆï¼Œä»–çš„æ¯æ ¡ â€” æ¹–æ»¨ä¸­å­¦çš„æ•°å­¦ç³»ä¸»ä»»å¼—é›·ç¦Â·èµ–ç‰¹æ›¾è¿™æ ·è°ˆèµ·è¿‡ä»–çš„å­¦ç”Ÿ: â€œä»–èƒ½ç”¨ä¸€ç§æœ€ç®€å•çš„æ–¹æ³•æ¥è§£å†³æŸä¸ªä»£æ•°æˆ–è®¡ç®—æœºé—®é¢˜ï¼Œä»–å¯ä»¥ç”¨æ•°å­¦çš„æ–¹æ³•æ¥æ‰¾åˆ°ä¸€æ¡å¤„ç†é—®é¢˜çš„æ·å¾„ï¼Œæˆ‘æ•™äº†è¿™ä¹ˆå¤šå¹´çš„ä¹¦ï¼Œæ²¡è§è¿‡åƒä»–è¿™æ ·å¤©åˆ†çš„æ•°å­¦å¥‡æ‰ã€‚ä»–ç”šè‡³å¯ä»¥å’Œæˆ‘å·¥ä½œè¿‡å¤šå¹´çš„é‚£äº›ä¼˜ç§€æ•°å­¦å®¶åª²ç¾ã€‚å½“ç„¶ï¼Œæ¯”å°”ä¹Ÿå„æ–¹é¢è¡¨ç°å¾—éƒ½å¾ˆä¼˜ç§€ï¼Œä¸ä»…ä»…æ˜¯æ•°å­¦ï¼Œä»–çš„çŸ¥è¯†é¢éå¸¸å¹¿æ³›ï¼Œæ•°å­¦ä»…æ˜¯ä»–ä¼—å¤šç‰¹é•¿ä¹‹ä¸€ã€‚â€ å½±å“ä¸€ä»£ä¸­å›½ç¨‹åºäººçš„é‡‘å±±è½¯ä»¶è‚¡ä»½æœ‰é™å…¬å¸è‘£äº‹é•¿æ±‚ä¼¯å›å½“å¹´é«˜è€ƒæ•°å­¦æˆç»©æ»¡åˆ†è¿›ä¸€æ­¥è¯´æ˜äº†é—®é¢˜ã€‚å¾ˆå¤šæ•°å­¦åŸºç¡€å¾ˆå¥½çš„äººï¼Œä¸€æ—¦ç†Ÿæ‚‰äº†æŸç§è®¡ç®—æœºè¯­è¨€ï¼Œä»–å¯ä»¥å¾ˆå¿«åœ°ç†è§£ä¸€äº›ç®—æ³•çš„ç²¾é«“ï¼Œä½¿ä¹‹èƒ½å¤Ÿè¿ç”¨è‡ªå¦‚ï¼Œå¹¶å¯èƒ½å†™å‡ºæ—¶é—´ä¸ç©ºé—´å¤æ‚åº¦éƒ½æœ‰æ˜æ˜¾æ”¹å–„çš„ç®—æ³•ã€‚ ç¨‹åºè®¾è®¡å½“ä¸­è§£å†³çš„ç›¸å½“ä¸€éƒ¨åˆ†é—®é¢˜éƒ½ä¼šæ¶‰åŠå„ç§å„æ ·çš„ç§‘å­¦è®¡ç®—ï¼Œè¿™éœ€è¦ç¨‹åºå‘˜å…·æœ‰ä»€ä¹ˆæ ·çš„åŸºç¡€å‘¢? å®é™…é—®é¢˜è½¬æ¢ä¸ºç¨‹åºï¼Œè¦ç»è¿‡ä¸€ä¸ªå¯¹é—®é¢˜æŠ½è±¡çš„è¿‡ç¨‹ï¼Œå»ºç«‹èµ·å®Œå–„çš„æ•°å­¦æ¨¡å‹ï¼Œåªæœ‰è¿™æ ·ï¼Œæˆ‘ä»¬æ‰èƒ½å»ºç«‹ä¸€ä¸ªè®¾è®¡è‰¯å¥½çš„ç¨‹åºã€‚ä»ä¸­æˆ‘ä»¬ä¸éš¾çœ‹å‡ºæ•°å­¦åœ¨ç¨‹åºè®¾è®¡é¢†åŸŸçš„é‡è¦æ€§ã€‚ç®—æ³•ä¸è®¡ç®—ç†è®ºæ˜¯è®¡ç®—æœºç¨‹åºè®¾è®¡é¢†åŸŸçš„çµé­‚æ‰€åœ¨ï¼Œæ˜¯å‘æŒ¥ç¨‹åºè®¾è®¡è€…ä¸¥è°¨ï¼Œæ•é”æ€ç»´çš„æœ‰æ•ˆå·¥å…·ï¼Œä»»ä½•çš„ç¨‹åºè®¾è®¡è¯­è¨€éƒ½è¯•å›¾å°†ä¹‹å‘æŒ¥å¾—æ·‹æ¼“å°½è‡´ã€‚ç¨‹åºå‘˜éœ€è¦ä¸€å®šçš„æ•°å­¦ä¿®å…»ï¼Œä¸ä½†æ˜¯ç¼–ç¨‹æœ¬èº«çš„éœ€è¦ï¼ŒåŒæ—¶ä¹Ÿæ˜¯åŸ¹å…»é€»è¾‘æ€ç»´ä»¥åŠä¸¥è°¨çš„ç¼–ç¨‹ä½œé£çš„éœ€è¦ã€‚æ•°å­¦å¯ä»¥é”»ç‚¼æˆ‘ä»¬çš„æ€ç»´èƒ½åŠ›ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³ç°å®ä¸­çš„é—®é¢˜ã€‚å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´é«˜çš„å­¦ä¹ å“²å­¦ã€‚ä¸ºä»€ä¹ˆç»å¸¸æœ‰äººå¯¹ä¸€äº›ç§‘å­¦è®¡ç®—ç¨‹åºä¸€ç­¹è«å±•ï¼Œä»–å¯ä»¥è¯»æ‡‚æ¯ä¸€è¡Œä»£ç ï¼Œä½†æ˜¯å´æ— æ³•é¢„æµ‹ç¨‹åºçš„é¢„æµ‹ç»“æœï¼Œç”šè‡³å¯¹ç¨‹åºçš„ç»“æ„ä¸åŠŸèƒ½ä¹Ÿä¸€çŸ¥åŠè§£ï¼Œç»™ä»–ä¸€ä¸ªç¨å¾®å¤æ‚ç‚¹çš„æ•°å­¦å…¬å¼ï¼Œä»–å¯èƒ½å°±ä¸çŸ¥é“æ€ä¹ˆæŠŠå®ƒå˜æˆè®¡ç®—æœºç¨‹åºã€‚å¾ˆå¤šç¨‹åºå‘˜è¿˜åœç•™åœ¨åšåšç®€å•çš„ MISï¼Œè®¾è®¡ä¸€ä¸‹ MDIï¼Œå†™å†™ç®€å•çš„ Class æˆ–ç”¨ SQL è¯­å¥å®ç°æŸ¥è¯¢ç­‰åŸºç¡€çš„ç¼–ç¨‹å·¥ä½œä¸Šï¼Œå¯¹äºä¸€äº›éœ€è¦ç”¨åˆ°æ•°å­¦çŸ¥è¯†çš„ç¼–ç¨‹å·¥ä½œå°±é¿è€Œè¿œä¹‹ï¼Œå½“ç„¶å®ç°ä¸€ä¸ªç´¯åŠ ç¨‹åºæˆ–è€…ä¸€ä¸ªç¨ç‡çš„æ¢ç®—ç¨‹åºè¿˜æ˜¯å¾ˆå®¹æ˜“çš„ï¼Œå› ä¸ºå®ƒä»¬å¹¶ä¸éœ€è¦ä»€ä¹ˆé«˜æ·±çš„æ•°å­¦çŸ¥è¯†ã€‚ ä¸€åæœ‰è¿‡ 10 å¤šå¹´å¼€å‘ç»éªŒçš„è€ç¨‹åºå‘˜æ›¾è¯´è¿‡: â€œæ‰€æœ‰ç¨‹åºçš„æœ¬è´¨å°±æ˜¯é€»è¾‘ã€‚æŠ€æœ¯ä½ å·²ç»è¾ƒå¥½åœ°æŒæ¡äº†ï¼Œä½†åªæœ‰å®Œæˆé€»è¾‘èƒ½åŠ›çš„æé«˜ï¼Œä½ æ‰èƒ½æˆä¸ºä¸€åèŒä¸šç¨‹åºå‘˜ã€‚æ‰“ä¸€ä¸ªæ¯”æ–¹å§ï¼Œä½ ä¼šåå…«èˆ¬æ­¦è‰ºï¼Œåˆ€æªæ£æ£’éƒ½å¾ˆç²¾é€šï¼Œä½†å°±æ˜¯åŠ›æ°”ä¸å¤Ÿï¼Œæ‰€ä»¥æ°¸è¿œéƒ½ä¸Šä¸äº†æˆ˜åœºï¼Œè¿™ä¸ªåŠ›æ°”å¯¹ç¨‹åºå‘˜è€Œè¨€å°±æ˜¯é€»è¾‘èƒ½åŠ› (å…¶æœ¬è´¨æ˜¯ä¸€ä¸ªäººçš„æ•°å­¦ä¿®å…»ï¼Œæ³¨æ„ï¼Œä¸æ˜¯æ•°å­¦çŸ¥è¯†)ã€‚â€ ç¨‹åºå‘˜çš„æ•°å­¦ä¿®å…»ä¸æ˜¯ä¸€æœä¸€å¤•å°±å¯ä»¥åŸ¹å…»çš„ã€‚æ•°å­¦ä¿®å…»ä¸æ•°å­¦çŸ¥è¯†ä¸ä¸€æ ·ï¼Œä¿®å…»éœ€è¦ä¸€ä¸ªé•¿æœŸçš„è¿‡ç¨‹ï¼Œè€ŒçŸ¥è¯†çš„å­¦ä¹ å¯èƒ½åªæ˜¯ä¸€æ®µçŸ­æš‚çš„æ—¶é—´ã€‚ä¸‹é¢æ˜¯ä¸€äº›æˆ‘ä¸ªäººå¯¹äºç¨‹åºå‘˜å¦‚ä½•æé«˜ä¸åŸ¹å…»è‡ªå·±çš„æ•°å­¦ä¿®å…»çš„åŸºæœ¬çœ‹æ³•ã€‚ é¦–å…ˆï¼Œåº”è¯¥æ„è¯†åˆ°æ•°å­¦ä¿®å…»çš„é‡è¦æ€§ã€‚ä½œä¸ºä¸€ä¸ªä¼˜ç§€çš„ç¨‹åºå‘˜ï¼Œä¸€å®šçš„æ•°å­¦ä¿®å…»æ˜¯ååˆ†é‡è¦ä¹Ÿæ˜¯å¿…è¦çš„ã€‚æ•°å­¦æ˜¯è‡ªç„¶ç§‘å­¦çš„åŸºç¡€ï¼Œè®¡ç®—æœºç§‘å­¦å®é™…ä¸Šæ˜¯æ•°å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚è®¡ç®—æœºç†è®ºå…¶å®æ˜¯å¾ˆå¤šæ•°å­¦çŸ¥è¯†çš„èåˆï¼Œè½¯ä»¶å·¥ç¨‹éœ€è¦å›¾è®ºï¼Œå¯†ç å­¦éœ€è¦æ•°è®ºï¼Œè½¯ä»¶æµ‹è¯•éœ€è¦ç»„åˆæ•°å­¦ï¼Œè®¡ç®—æœºç¨‹åºçš„ç¼–åˆ¶æ›´éœ€è¦å¾ˆå¤šçš„æ•°å­¦çŸ¥è¯†ï¼Œå¦‚é›†åˆè®ºã€æ’é˜Ÿè®ºã€ç¦»æ•£æ•°å­¦ã€ç»Ÿè®¡å­¦ï¼Œå½“ç„¶è¿˜æœ‰å¾®ç§¯åˆ†ã€‚è®¡ç®—æœºç§‘å­¦ä¸€ä¸ªæœ€å¤§çš„ç‰¹å¾æ˜¯ä¿¡æ¯ä¸çŸ¥è¯†æ›´æ–°é€Ÿåº¦å¾ˆå¿«ï¼Œéšç€æ•°å­¦çŸ¥è¯†ä¸è®¡ç®—æœºç†è®ºçš„è¿›ä¸€æ­¥ç»“åˆï¼Œæ•°æ®æŒ–æ˜ã€æ¨¡å¼è¯†åˆ«ã€ç¥ç»ç½‘ç»œç­‰åˆ†æ”¯ç§‘å­¦å¾—åˆ°äº†è¿…é€Ÿå‘å±•ï¼Œæ§åˆ¶è®ºã€æ¨¡ç³Šæ•°å­¦ã€è€—æ•£ç†è®ºã€åˆ†å½¢ç§‘å­¦éƒ½ä¿ƒè¿›äº†è®¡ç®—æœºè½¯ä»¶ç†è®ºã€ä¿¡æ¯ç®¡ç†æŠ€æœ¯çš„å‘å±•ã€‚ä¸¥æ ¼çš„è¯´ï¼Œä¸€ä¸ªæ•°å­¦åŸºç¡€ä¸æ‰å®çš„ç¨‹åºä¸èƒ½ç®—ä¸€ä¸ªåˆæ ¼çš„ç¨‹åºå‘˜ï¼Œå¾ˆå¤šä»‹ç»è®¡ç®—æœºç®—æ³•çš„ä¹¦ç±æœ¬èº«ä¹Ÿå°±æ˜¯æ•°å­¦çŸ¥è¯†çš„åº”ç”¨ä¸è®¡ç®—æœºå®ç°æ‰‹å†Œã€‚ å…¶æ¬¡ï¼Œè‡ªèº«æ•°å­¦çŸ¥è¯†çš„ç§¯ç´¯ï¼ŒåŸ¹å…»è‡ªå·±çš„ç©ºé—´æ€ç»´èƒ½åŠ›å’Œé€»è¾‘åˆ¤æ–­èƒ½åŠ›ã€‚æ•°å­¦æ˜¯ä¸€é—¨åˆ†æ”¯ä¼—å¤šçš„å­¦ç§‘ï¼Œæˆ‘ä»¬æ— æ³•åœ¨çŸ­æš‚çš„ä¸€ç”Ÿä¸­å­¦ä¼šæ‰€æœ‰çš„æ•°å­¦çŸ¥è¯†ï¼Œåƒæ³›å‡½ç†è®ºã€æ··æ²Œç†è®ºä»¥åŠä¸€äº›éçº¿æ€§æ•°å­¦é—®é¢˜ä¸æ˜¯ä¸‰äº”å‡ å¤©å°±å¯ä»¥æŒæ¡çš„ã€‚æ•°å­¦ä¿®å…»çš„åŸ¹å…»å¹¶ä¸åœ¨ä¸æ•°å­¦çŸ¥è¯†çš„å¤šå°‘ï¼Œä½†è¦æ±‚ç¨‹åºå‘˜æœ‰è‰¯å¥½çš„æ•°å­¦å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¾ˆå¿«åœ°æŠŠä¸€äº›æ•°å­¦çŸ¥è¯†å’Œè‡ªå·±æ­£åœ¨è§£å†³çš„é—®é¢˜è”ç³»èµ·æ¥ï¼Œå¾ˆå¤šç†å­¦å¤§å¸ˆè™½ç„¶ä¸æ˜¯æ•°å­¦å‡ºèº«ï¼Œä½†æ˜¯ä»–ä»¬å¯¹æ•°å­¦æœ‰å¾ˆå¼ºçš„ç†è§£èƒ½åŠ›å’Œæ•é”çš„è§‚å¯ŸåŠ›ï¼Œäºæ˜¯ä¸€ç³»åˆ—æ–°çš„å­¦ç§‘è¯ç”Ÿäº†ï¼Œå¦‚è®¡ç®—åŒ–å­¦ã€è®¡ç®—ç”Ÿç‰©å­¦ã€ç”Ÿç‰©ä¿¡æ¯å­¦ã€åŒ–å­¦ä¿¡æ¯å­¦ã€è®¡ç®—ç‰©ç†å­¦ï¼Œè®¡ç®—ææ–™å­¦ç­‰ç­‰ã€‚æ•°å­¦æ˜¯è‡ªç„¶å­¦ç§‘çš„åŸºç¡€ï¼Œè®¡ç®—æœºæŠ€æœ¯ä½œä¸ºç†è®ºä¸å®è·µçš„ç»“åˆï¼Œæ›´éœ€è¦æŠŠæ•°å­¦çš„ä¸€äº›ç²¾é«“èå…¥å…¶ä¸­ã€‚ä»è®¡ç®—æœºçš„è¯ç”Ÿæ¥çœ‹å®ƒå°±æ˜¯åœ¨æ•°å­¦çš„åŸºç¡€ä¸Šäº§ç”Ÿçš„ï¼Œæœ€ç®€å•çš„ 0ã€1 è¿›åˆ¶å°±æ˜¯ä¸€ä¸ªå¤è€çš„æ•°å­¦é—®é¢˜ã€‚ç¨‹åºè®¾è®¡ä½œä¸ºä¸€é¡¹åˆ›é€ æ€§å¾ˆå¼ºçš„èŒä¸šï¼Œå®ƒéœ€è¦ç¨‹åºå‘˜æœ‰ä¸€å®šçš„æ•°å­¦ä¿®å…»ï¼Œä¹Ÿå…·æœ‰ä¸€å®šçš„æ•°å­¦çŸ¥è¯†çš„ç§¯ç´¯ï¼Œå¯ä»¥æ›´å¥½åœ°æŠŠä¸€äº›æ•°å­¦åŸç†ä¸æ€æƒ³åº”ç”¨äºå®é™…çš„ç¼–ç¨‹å·¥ä½œä¸­å»ã€‚å­¦æ— æ­¢å¢ƒï¼Œä¸æ–­çš„å­¦ä¹ æ˜¯æé«˜ä¿®å…»çš„å¿…ç»ä¹‹è·¯ã€‚ ç¬¬ä¸‰ï¼Œå¤šåœ¨å®è·µä¸­è¿ç”¨æ•°å­¦ã€‚æœ‰äº›é«˜ç­‰å­¦æ ¡å¼€è®¾äº†ä¸€é—¨è¿™æ ·çš„è¯¾ç¨‹ â€”ã€Šæ•°å­¦å»ºæ¨¡ã€‹ã€‚æˆ‘åœ¨å¤§å­¦æ—¶æœŸä¹Ÿæ›¾å­¦è¿‡ï¼Œè¿™æ˜¯ä¸€é—¨å†…å®¹å¾ˆä¸°å¯Œçš„è¯¾ç¨‹ã€‚å®ƒæŠŠå¾ˆå¤šç›¸å…³çš„å­¦ç§‘ä¸æ•°å­¦éƒ½è”ç³»åœ¨ä¸€èµ·ï¼Œé€šè¿‡å¾ˆå¤šæ•°å­¦æ¨¡å‹æ¥è§£å†³å®é™…çš„ç”Ÿäº§ç”Ÿæ´»é—®é¢˜ï¼Œå¾ˆå¤šé—®é¢˜çš„è§£å†³éœ€è¦è®¡ç®—æœºç¨‹åºæ¥å®ç°ã€‚æˆ‘åœ¨å¤§å­¦å’Œç ”ç©¶ç”Ÿé˜¶æ®µéƒ½å‚åŠ è¿‡æ•°å­¦å»ºæ¨¡ç«èµ›ï¼Œè·å¾—äº†ä¸å°‘çš„ç»éªŒï¼ŒåŒæ—¶ä¹Ÿè¿›ä¸€æ­¥æé«˜äº†è‡ªå·±çš„æ•°å­¦ä¿®å…»ã€‚å®é™…ä¸Šï¼Œç°åœ¨çš„ç¨‹åºè®¾è®¡ä»æŸäº›è§’åº¦æ¥çœ‹å°±æ˜¯ä¸€ä¸ªæ•°å­¦å»ºæ¨¡çš„è¿‡ç¨‹ï¼Œæ¨¡å‹çš„å¥½åå…³ç³»åˆ°ç³»ç»Ÿçš„æˆè´¥ï¼Œç°åœ¨æ•°å­¦å»ºæ¨¡çš„æ€æƒ³å·²ç»ç”¨äºè®¡ç®—æœºçš„è®¸å¤šç›¸å…³å­¦ç§‘ä¸­ï¼Œä¸å•åªæ˜¯è®¡ç®—æœºç¨‹åºè®¾è®¡ä¸ç®—æ³•åˆ†æã€‚åº”è¯¥çŸ¥é“ï¼Œæ•°å­¦æ˜¯ä¸€é—¨éœ€è¦åœ¨å®è·µä¸­å±•ç¤ºå…¶é­…åŠ›çš„ç§‘å­¦ï¼Œè€Œè®¡ç®—æœºç¨‹åºä¹Ÿæ˜¯ä¸ºå¸®åŠ©è§£å†³å®é™…é—®é¢˜è€Œç¼–åˆ¶çš„ï¼Œå› æ­¤ï¼Œåº”è¯¥å°½é‡ä½¿å®ƒä»¬ç»“åˆèµ·æ¥ï¼Œåœ¨è¿™ä¸ªæ–¹é¢ï¼Œè®¡ç®—æœºå¯†ç å­¦æ˜¯æˆ‘è®¤ä¸ºè¿ç”¨æ•°å­¦çŸ¥è¯†æœ€æ·±æœ€å¹¿æ³›çš„ï¼Œæ¯ä¸€ä¸ªå¥½çš„åŠ å¯†ç®—æ³•åé¢éƒ½æœ‰ä¸€ä¸ªæ•°å­¦ç†è®ºçš„æ”¯æŒï¼Œå¦‚æ¤­åœ†æ›²çº¿ã€èƒŒåŒ…é—®é¢˜ã€ç´ æ•°ç†è®ºç­‰ã€‚ä½œä¸ºä¸€åä¼˜ç§€çš„ç¨‹åºå‘˜ï¼Œåº”è¯¥åœ¨å®é™…å·¥ä½œä¸­æ ¹æ®éœ€è¦çµæ´»è¿ç”¨æ•°å­¦çŸ¥è¯†ï¼ŒåŸ¹å…»ä¸€å®šçš„æ•°å­¦å»ºæ¨¡èƒ½åŠ›ï¼Œå–„äºå½’çº³æ€»ç»“ï¼Œæ…¢æ…¢ä½¿è‡ªå·±çš„æ•°å­¦çŸ¥è¯†æ›´åŠ å…¨é¢ï¼Œæ•°å­¦ä¿®å…»å¾—åˆ°è¿›ä¸€æ­¥æé«˜ã€‚ ç¬¬å››ï¼Œç¨‹åºå‘˜åŸ¹å…»åˆ¶åº¦ä¸æ•™å­¦çš„æ”¹é©ã€‚è®¸å¤šç¨‹åºå‘˜åŸ¹å…»ä½“åˆ¶å­˜åœ¨å¾ˆå¤šç¼ºé™·ï¼Œä¸€å¼€å§‹å°±è¦æ±‚å­¦å‘˜èƒ½å¤Ÿå¿«é€Ÿç²¾é€šæŸç§è¯­è¨€ï¼Œä»¥è¯­è¨€ä¸ºä¸­å¿ƒï¼Œå¯¹ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³ä¸ç›¸å…³çš„æ•°å­¦çŸ¥è¯†éƒ½ä¸€ç¬”å¸¦è¿‡ï¼Œè®²å¾—å¾ˆå°‘ï¼Œè¿™é€ æˆå¾ˆå¤šç¨‹åºå‘˜æˆä¸ºèƒŒç¨‹åºçš„æœºå™¨ï¼Œè¿™æ ·ä¸åˆ©äºç¨‹åºå‘˜è‡ªèº«çš„å¿«é€Ÿæˆé•¿ï¼Œä¹Ÿä¸åˆ©äºç¨‹åºå‘˜è§£å†³æ–°é—®é¢˜ã€‚æˆ‘åœ¨é•¿æœŸçš„ç¨‹åºå‘˜åŸ¹è®­ä¸è®¡ç®—æœºæ•™å­¦å·¥ä½œé‡‡ç”¨äº†ä¸€äº›ä¸ä¼ ç»Ÿæ–¹å¼ä¸ä¸€è‡´çš„æ–¹æ³•ï¼Œæ”¶åˆ°äº†ä¸€å®šçš„æ•ˆæœã€‚å¾ˆå¤šåˆå­¦ç¨‹åºçš„äººå¾€å¾€å†™ç¨‹åºæ—¶æœ‰æ—¶å€™ä¼šæœ‰æ€ç»´ä¸­æ–­ï¼Œæˆ–è€…å¯¹ä¸€äº›ç¨éš¾çš„ç¨‹åºè§‰å¾—æ— æ³•ä¸‹æ‰‹ï¼Œæˆ‘é‡‡ç”¨äº†ä¸€äº›è¯¾å‰è§£å†³æ•°å­¦å°é—®é¢˜çš„æ–¹æ³•æ¥æ¿€åŠ±å¤§å®¶çš„å­¦ä¹ å…´è¶£ï¼Œè¿™äº›å°é—®é¢˜ä¸å•å•æ˜¯è„‘ç­‹æ€¥è½¬å¼¯ï¼Œå…¶ä¸­ä¸å°‘æ˜¯å¾ˆæœ‰ä»£è¡¨æ„ä¹‰çš„æ•°å­¦æ€è€ƒé¢˜ã€‚é€šè¿‡æ•°å­¦é—®é¢˜æ¥åšç¼–ç¨‹çš„çƒ­èº«è¿åŠ¨ï¼Œè®©å­¦å‘˜åœ¨æ•°å­¦è¯•é¢˜ä¸­æ¿€å‘è‡ªå·±çš„æ€ç»´èƒ½åŠ›ï¼Œè®°å¾—æœ‰ä½ä¸“å®¶æ›¾ç»è¯´è¿‡ï¼Œç»å¸¸åšåšæ•°å­¦é¢˜ç›®ä¼šä½¿è‡ªå·±å˜èªæ˜ï¼Œå¾ˆé•¿æ—¶é—´ä¸å»æ¥è§¦æ•°å­¦é—®é¢˜ä¼šä½¿è‡ªå·±æ€ç»´è¿Ÿé’ã€‚é€šè¿‡ä¸€äº›ç»å…¸çš„æ•°å­¦é—®é¢˜æ¥åŸ¹å…»å­¦å‘˜çš„æ€ç»´çš„ä¸¥è°¨æ€§å’Œè·³è·ƒæ€§ã€‚å¾ˆå¤šäººå¯èƒ½ä¸ä»¥ä¸ºç„¶ï¼Œå…¶å®æœ‰äº›çœ‹ä¼¼ç®€å•çš„é—®é¢˜å¹¶ä¸ä¸€å®šèƒ½å¤Ÿå¿«é€Ÿç»™å‡ºç­”æ¡ˆï¼Œå¤§è„‘ä¹Ÿæ˜¯åœ¨ä¸æ–­çš„è¿ç”¨ä¸­å˜æ›´åŠ çµæ´»çš„ã€‚ä¸ä¿¡å—? å¤§å®¶æœ‰å…´è¶£å¯ä»¥åšåšä¸‹é¢è¿™é“é¢˜ç›®ï¼Œçœ‹çœ‹èƒ½ä¸èƒ½åœ¨1åˆ†é’Ÿä¹‹å†…æƒ³åˆ°ç­”æ¡ˆï¼Œè¿™åªæ˜¯ä¸€é“å°å­¦æ•°å­¦è¯¾åä¹ é¢˜ã€‚å¾ˆå¤šäººè®¤ä¸ºè‡ªå·±çš„æ•°å­¦åŸºç¡€å¾ˆå¥½ï¼Œä½†æ˜¯æ®è¯´è¿™é“é¢˜ç›® 90% ä»¥ä¸Šçš„äººä¸èƒ½åœ¨ä¸€ä¸ªå°æ—¶å†…ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚è¯•è¯•ï¼Œå¦‚æœä½ è§‰å¾—æˆ‘è¯´çš„æ˜¯é”™çš„ã€‚ è¯æ˜: AB + AC &gt; DB + DC (D ä¸ºä¸‰è§’å½¢ ABC çš„ä¸€ä¸ªå†…ç‚¹) æœ€åï¼Œå¤šå­¦å¤šé—®ï¼Œå¤šçœ‹å¥½ä¹¦ï¼Œçœ‹ç»å…¸ã€‚æˆ‘åœ¨è¿™é‡Œå‘å¤§å®¶æ¨èä¸¤éƒ¨å¯èƒ½å¤§å®¶å·²ç»å¾ˆç†Ÿæ‚‰çš„ç»å…¸çš„è®¡ç®—æœºç®—æ³•æ•™æï¼Œå®ƒä»¬ä¸­é—´å¾ˆå¤šå†…å®¹å…¶å®å°±æ˜¯æ•°å­¦çŸ¥è¯†çš„ä»‹ç»ã€‚ ç¬¬ä¸€éƒ¨æ˜¯ã€Šç®—æ³•å¯¼è®ºã€‹ï¼Œè‹±æ–‡åç§°: Introduction to Algorithmsï¼Œä½œè€…: Thomas H. Cormen ï¼ŒCharles E. Leiserson ï¼ŒRonald L. Rivest ï¼ŒClifford Steinã€‚æœ¬ä¹¦çš„ä¸»è¦ä½œè€…æ¥è‡ªéº»çœç†å·¥å¤§å­¦è®¡ç®—æœºï¼Œä½œè€…ä¹‹ä¸€ Ronald L. Rivest ç”±äºå…¶åœ¨å…¬å¼€ç§˜é’¥å¯†ç ç®—æ³• RSA ä¸Šçš„è´¡çŒ®è·å¾—äº†å›¾çµå¥–ã€‚è¿™æœ¬ä¹¦ç›®å‰æ˜¯ç®—æ³•çš„æ ‡å‡†æ•™æï¼Œç¾å›½è®¸å¤šåæ ¡çš„è®¡ç®—æœºç³»éƒ½ä½¿ç”¨å®ƒï¼Œå›½å†…æœ‰äº›é™¢æ ¡ä¹Ÿå°†æœ¬ä¹¦ä½œä¸ºç®—æ³•è¯¾ç¨‹çš„æ•™æã€‚å¦å¤–è®¸å¤šä¸“ä¸šäººå‘˜ä¹Ÿç»å¸¸å¼•ç”¨å®ƒã€‚æœ¬ä¹¦åŸºæœ¬åŒ…å«äº†æ‰€æœ‰çš„ç»å…¸ç®—æ³•ï¼Œç¨‹åºå…¨éƒ¨ç”±ä¼ªä»£ç å®ç°ï¼Œè¿™æ›´å¢æ·»äº†æœ¬ä¹¦çš„é€šç”¨æ€§ï¼Œä½¿å¾—åˆ©ç”¨å„ç§ç¨‹åºè®¾è®¡è¯­è¨€è¿›è¡Œç¨‹åºå¼€å‘çš„ç¨‹åºå‘˜éƒ½å¯ä»¥ä½œä¸ºå‚è€ƒã€‚è¯­è¨€æ–¹é¢é€šä¿—ï¼Œå¾ˆé€‚åˆä½œä¸ºç®—æ³•æ•™æå’Œè‡ªå­¦ç®—æ³•ä¹‹ç”¨ã€‚ å¦ä¸€éƒ¨æ˜¯å¾ˆå¤šäººéƒ½åº”è¯¥çŸ¥é“çš„ Donald E. Knuth æ‰€è‘—ã€Šè®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯ã€‹ï¼Œè‹±æ–‡åç§°: The Art of Computer Programmingã€‚ Donald E. Knuth äººç”Ÿæœ€è¾‰ç…Œçš„æ—¶åˆ»åœ¨æ–¯å¦ç¦å¤§å­¦è®¡ç®—æœºç³»æ¸¡è¿‡ï¼Œç¾å›½è®¡ç®—æœºåä¼šå›¾çµå¥–çš„è·å¾—è€…ï¼Œæ˜¯æœ¬é¢†åŸŸå†…å½“ä¹‹æ— æ„§çš„æ³°æ–—ã€‚æœ‰æˆè¨€ç§°æè®¡ç®—æœºç¨‹åºè®¾è®¡çš„ä¸è®¤è¯† Knuth å°±ç­‰äºæç‰©ç†çš„ä¸çŸ¥é“çˆ±å› æ–¯å¦ï¼Œææ•°å­¦çš„ä¸çŸ¥é“æ¬§æ‹‰ï¼ŒæåŒ–å­¦çš„ä¸çŸ¥é“é“å°”é¡¿ã€‚è¢«ç®€ç§°ä¸º TAOCP çš„è¿™æœ¬å·¨è‘—å†…å®¹åšå¤§ç²¾æ·±ï¼Œå‡ ä¹æ¶µç›–äº†è®¡ç®—æœºç¨‹åºè®¾è®¡ç®—æ³•ä¸ç†è®ºæœ€é‡è¦çš„å†…å®¹ã€‚ç°åœ¨å‘è¡Œçš„åªæœ‰ä¸‰å·ï¼Œåˆ†åˆ«ä¸ºåŸºç¡€è¿ç®—æ³•åˆ™ï¼ŒåŠæ•°å€¼ç®—æ³•ï¼Œä»¥åŠæ’åºå’Œæœç´¢ (åœ¨å†™æœ¬æ–‡ä¹‹é™…ï¼Œç¬¬å››å·å·²ç»å‡ºæ¥äº†ï¼Œæˆ‘ä¹Ÿåœ¨ç¬¬ä¸€æ—¶é—´æŠ¢è´­äº†ä¸€æœ¬)ã€‚æœ¬ä¹¦ç»“åˆå¤§é‡æ•°å­¦çŸ¥è¯†ï¼Œåˆ†æä¸åŒåº”ç”¨é¢†åŸŸä¸­çš„å„ç§ç®—æ³•ï¼Œç ”ç©¶ç®—æ³•çš„å¤æ‚æ€§ï¼Œå³ç®—æ³•çš„æ—¶é—´ã€ç©ºé—´æ•ˆç‡ï¼Œæ¢è®¨å„ç§é€‚ç”¨ç®—æ³•ç­‰ï¼Œå…¶ç†è®ºå’Œå®è·µä»·å€¼å¾—åˆ°äº†å…¨ä¸–ç•Œè®¡ç®—æœºå·¥ä½œè€…çš„å…¬è®¤ã€‚ä¹¦ä¸­å¼•å…¥çš„è®¸å¤šæœ¯è¯­ã€å¾—åˆ°çš„è®¸å¤šç»“è®ºéƒ½å˜æˆäº†è®¡ç®—æœºé¢†åŸŸçš„æ ‡å‡†æœ¯è¯­å’Œè¢«å¹¿æ³›å¼•ç”¨çš„ç»“æœã€‚å¦å¤–ï¼Œä½œè€…å¯¹æœ‰å…³é¢†åŸŸçš„ç§‘å­¦å‘å±•å²ä¹Ÿæœ‰æ·±å…¥ç ”ç©¶ï¼Œå› æ­¤æœ¬ä¹¦ä»‹ç»ä¼—å¤šç ”ç©¶æˆæœçš„åŒæ—¶ï¼Œä¹Ÿå¯¹å…¶å†å²æ¸Šæºå’Œå‘å±•è¿‡ç¨‹åšäº†å¾ˆå¥½çš„ä»‹ç»ï¼Œè¿™ç§ç‰¹è‰²åœ¨å…¨çƒç§‘å­¦è‘—ä½œä¸­æ˜¯ä¸å¤šè§çš„ã€‚è‡³äºæœ¬ä¹¦çš„ä»·å€¼æˆ‘è§‰å¾— Bill Gates å…ˆç”Ÿçš„è¯è¶³ä»¥è¯´æ˜é—®é¢˜: â€œå¦‚æœä½ è®¤ä¸ºä½ æ˜¯ä¸€åçœŸæ­£ä¼˜ç§€çš„ç¨‹åºå‘˜è¯» Knuth çš„ã€Šè®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯ã€‹ï¼Œå¦‚æœä½ èƒ½è¯»æ‡‚æ•´å¥—ä¹¦çš„è¯ï¼Œè¯·ç»™æˆ‘å‘ä¸€ä»½ä½ çš„ç®€å†â€ã€‚ä½œè€…æ•°å­¦æ–¹é¢çš„åŠŸåº•é€ å°±äº†æœ¬ä¹¦ä¸¥è°¨çš„é£æ ¼ï¼Œè™½ç„¶æœ¬ä¹¦ä¸æ˜¯ç”¨å½“ä»Šæµè¡Œçš„ç¨‹åºè®¾è®¡è¯­è¨€æè¿°çš„ï¼Œä½†è¿™ä¸æ¯«ä¸æŸä¼¤å®ƒ â€œç¨‹åºè®¾è®¡å²è¯—â€ çš„åœ°ä½ã€‚é“ç†å¾ˆç®€å•ï¼Œå®ƒå†…æ¶µçš„è®¾è®¡æ€æƒ³æ˜¯æ°¸è¿œä¸ä¼šè¿‡æ—¶çš„ã€‚é™¤éè‹±è¯­å®åœ¨æœ‰å›°éš¾ï¼Œå¦åˆ™å»ºè®®è¯»è€…é€‰ç”¨è‹±æ–‡ç‰ˆã€‚æˆ‘ä¸ªäººå°±æ˜¯é˜…è¯»çš„è¯¥ä¹¦çš„è‹±æ–‡ç‰ˆï¼Œè™½ç„¶èŠ±äº†ä¸å°‘ money å’Œæ—¶é—´ï¼Œä½†æ˜¯æ”¶è·é¢‡ä¸°ï¼Œå€¼å¾—ã€‚ æ€»ä¹‹ï¼Œè¦æƒ³æˆä¸ºä¸€åæœ‰æ½œåŠ›æœ‰å‘å±•å‰é€”çš„ç¨‹åºå‘˜ï¼Œæˆ–è€…æƒ³æˆä¸ºç¨‹åºå‘˜ä¸­çš„ä½¼ä½¼è€…ï¼Œä½ ä¸€å®šè¦åŸ¹å…»è‰¯å¥½çš„æ•°å­¦ä¿®å…»ã€‚åˆ‡è®°: å¯¹äºä¸€åèƒ½å¤Ÿçµæ´»è‡ªå¦‚ç¼–å†™å„ç§ç¨‹åºçš„äººï¼Œ æ•°å­¦æ˜¯ç¨‹åºçš„çµé­‚ã€‚ Reference http://www.cs.xmu.edu.cn/cs/node/213","tags":[]},{"title":"Geoffrey Hinton interview","date":"2017-09-20T13:29:03.000Z","path":"2017/09/20/Geoffrey-Hinton-interview/","text":"About this courseIf you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new â€œsuperpowerâ€ that will let you build AI systems that just were not possible a few years ago. In this course, you will learn the foundations of deep learning. When you finish this class, you will: Understand the major technology trends driving Deep Learning Be able to build, train and apply fully connected deep neural networks Know how to implement efficient (vectorized) neural networks Understand the key parameters in a neural networkâ€™s architecture This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions. This is the first course of the Deep Learning Specialization. Lecture transcriptAs part of this course by deeplearning.ai, hope to not just teach you the technical ideas in deep learning, but also introduce you to some of the people, some of the heroes in deep learning. The people that invented so many of these ideas that you learn about in this course or in this specialization. In these videos, I hope to also ask these leaders of deep learning to give you career advice for how you can break into deep learning, for how you can do research or find a job in deep learning. As the first of this interview series, I am delighted to present to you an interview with Geoffrey Hinton. AN: I think that at this point you more than anyone else on this planet has invented so many of the ideas behind deep learning. And a lot of people have been calling you the godfather of deep learning. Although it was not until we were chatting a few minutes ago, until I realized you think Iâ€™m the first one to call you that, which Iâ€™m quite happy to have done. But what I want to ask is, many people know you as a legend, I want to ask about your personal story behind the legend. So how did you get involved in, going way back, how did you get involved in AI and machine learning and neural networks? So when I was at high school, I had a classmate who was always better than me at everything, he was a brilliant mathematician. And he came into school one day and said, did you know the brain uses holograms? And I guess that was about 1966, and I said, sort of whatâ€™s a hologram? And he explained that in a hologram you can chop off half of it, and you still get the whole picture. And that memories in the brain might be distributed over the whole brain. And so I guess heâ€™d read about Lashleyâ€™s experiments, where you chop off bits of a ratâ€™s brain and discover that itâ€™s very hard to find one bit where it stores one particular memory. So thatâ€™s what first got me interested in how does the brain store memories. And then when I went to university, I started off studying physiology and physics. I think when I was at Cambridge, I was the only undergraduate doing physiology and physics. And then I gave up on that and tried to do philosophy, because I thought that might give me more insight. But that seemed to me actually lacking in ways of distinguishing when they said something false. And so then I switched to psychology. And in psychology they had very, very simple theories, and it seemed to me it was sort of hopelessly inadequate to explaining what the brain was doing. So then I took some time off and became a carpenter. And then I decided that Iâ€™d try AI, and went of to Edinburgh, to study AI with Langer Higgins. And he had done very nice work on neural networks, and heâ€™d just given up on neural networks, and been very impressed by Winogradâ€™s thesis. So when I arrived he thought I was kind of doing this old fashioned stuff, and I ought to start on symbolic AI. And we had a lot of fights about that, but I just kept on doing what I believed in. AN: And then what? I eventually got a PhD in AI, and then I couldnâ€™t get a job in Britain. But I saw this very nice advertisement for Sloan Fellowships in California, and I managed to get one of those. And I went to California, and everything was different there. So in Britain, neural nets was regarded as kind of silly, and in California, Don Norman and David Rumelhart were very open to ideas about neural nets. It was the first time Iâ€™d been somewhere where thinking about how the brain works, and thinking about how that might relate to psychology, was seen as a very positive thing. And it was a lot of fun there, in particular collaborating with David Rumelhart was great. AN: I see, great. So this was when you were at UCSD, and you and Rumelhart around what, 1982, wound up writing the seminal back prop paper, right? Actually, it was more complicated than that. In, I think, early 1982, David Rumelhart and me, and Ron Williams, between us developed the back prop algorithm, it was mainly David Rumelhartâ€™s idea. We discovered later that many other people had invented it. David Parker had invented, it probably after us, but before weâ€™d published. Paul Werbos had published it already quite a few years earlier, but nobody paid it much attention. And there were other people whoâ€™d developed very similar algorithms, itâ€™s not clear whatâ€™s meant by back prop. But using the chain rule to get derivatives was not a novel idea. AN: I see, why do you think it was your paper that helped so much the community latch on to back prop? It feels like your paper marked an infection in the acceptance of this algorithm, whoever accepted it. So we managed to get a paper into Nature in 1986. And I did quite a lot of political work to get the paper accepted. I figured out that one of the referees was probably going to be Stuart Sutherland, who was a well known psychologist in Britain. And I went to talk to him for a long time, and explained to him exactly what was going on. And he was very impressed by the fact that we showed that back prop could learn representations for words. And you could look at those representations, which are little vectors, and you could understand the meaning of the individual features. So we actually trained it on little triples of words about family trees, like Mary has mother Victoria. And youâ€™d give it the first two words, and it would have to predict the last word. And after you trained it, you could see all sorts of features in the representations of the individual words. Like the nationality of the person there, what generation they were, which branch of the family tree they were in, and so on. That was what made Stuart Sutherland really impressed with it, and I think thatâ€™s why the paper got accepted. AN: Very early word embeddings, and youâ€™re already seeing learned features of semantic meanings emerge from the training algorithm. Yes, so from a psychologistâ€™s point of view, what was interesting was it unified two completely different strands of ideas about what knowledge was like. So there was the old psychologistâ€™s view that a concept is just a big bundle of features, and thereâ€™s lots of evidence for that. And then there was the AI view of the time, which is a formal structurist view. Which was that a concept is how it relates to other concepts. And to capture a concept, youâ€™d have to do something like a graph structure or maybe a semantic net. And what this back propagation example showed was, you could give it the information that would go into a graph structure, or in this case a family tree. And it could convert that information into features in such a way that it could then use the features to derive new consistent information, ie generalize. But the crucial thing was this to and fro between the graphical representation or the tree structured representation of the family tree, and a representation of the people as big feature vectors. And in fact that from the graph-like representation you could get feature vectors. And from the feature vectors, you could get more of the graph-like representation. AN: So this is 1986? In the early 90s, Bengio showed that you can actually take real data, you could take English text, and apply the same techniques there, and get embeddings for real words from English text, and that impressed people a lot. AN: I guess recently weâ€™ve been talking a lot about how fast computers like GPUs and supercomputers thatâ€™s driving deep learning. I didnâ€™t realize that back between 1986 and the early 90â€™s, it sounds like between you and Bengio there was already the beginnings of this trend. Yes, it was a huge advance. In 1986, I was using a list machine which was less than a tenth of a mega flop. And by about 1993 or thereabouts, people were seeing ten mega flops. So there was a factor of 100, and thatâ€™s the point at which is was easy to use, because computers were just getting faster. AN: Over the past several decades, youâ€™ve invented so many pieces of neural networks and deep learning. Iâ€™m actually curious, of all of the things youâ€™ve invented, which of the ones youâ€™re still most excited about today? So I think the most beautiful one is the work I do with Terry Sejnowski on Boltzmann machines. So we discovered there was this really, really simple learning algorithm that applied to great big density connected nets where you could only see a few of the nodes. So it would learn hidden representations and it was a very simple algorithm. And it looked like the kind of thing you should be able to get in a brain because each synapse only needed to know about the behavior of the two neurons it was directly connected to. And the information that was propagated was the same. There were two different phases, which we called wake and sleep. But in the two different phases, youâ€™re propagating information in just the same way. Where as in something like back propagation, thereâ€™s a forward pass and a backward pass, and they work differently. Theyâ€™re sending different kinds of signals. So I think thatâ€™s the most beautiful thing. And for many years it looked just like a curiosity, because it looked like it was much too slow. But then later on, I got rid of a little bit of the beauty, and it started letting me settle down and just use one iteration, in a somewhat simpler net. And that gave restricted Boltzmann machines, which actually worked effectively in practice. So in the Netflix competition, for example, restricted Boltzmann machines were one of the ingredients of the winning entry. AN: And in fact, a lot of the recent resurgence of neural net and deep learning, starting about 2007, was the restricted Boltzmann machine, and de-restricted Boltzmann machine work that you and your lab did. Yes so thatâ€™s another of the pieces of work Iâ€™m very happy with, the idea of that you could train your restricted Boltzmann machine, which just had one layer of hidden features and you could learn one layer of feature. And then you could treat those features as data and do it again, and then you could treat the new features you learned as data and do it again, as many times as you liked. So that was nice, it worked in practice. And then UY Tay realized that the whole thing could be treated as a single model, but it was a weird kind of model. It was a model where at the top you had a restricted Boltzmann machine, but below that you had a Sigmoid belief net which was something that invented many years early. So it was a directed model and what weâ€™d managed to come up with by training these restricted Boltzmann machines was an efficient way of doing inferences in Sigmoid belief nets. So, around that time, there were people doing neural nets, who would use densely connected nets, but didnâ€™t have any good ways of doing probabilistic imprints in them. And you had people doing graphical models, unlike my children, who could do inference properly, but only in sparsely connected nets. And what we managed to show was the way of learning these deep belief nets so that thereâ€™s an approximate form of inference thatâ€™s very fast, itâ€™s just hands in a single forward pass and that was a very beautiful result. And you could guarantee that each time you learn that extra layer of features there was a band, each time you learned a new layer, you got a new band, and the new band was always better than the old band. AN: The variational bands, showing as you add layers. Yes, I remember that video. So that was the second thing that I was really excited about. And I guess the third thing was the work I did with on variational methods. It turns out people in statistics had done similar work earlier, but we didnâ€™t know about that. So we managed to make EN work a whole lot better by showing you didnâ€™t need to do a perfect E step. You could do an approximate E step. And EN was a big algorithm in statistics. And weâ€™d showed a big generalization of it. And in particular, in 1993, I guess, with Van Camp. I did a paper, with I think, the first variational Bayes paper, where we showed that you could actually do a version of Bayesian learning that was far more tractable, by approximating the true posterior with a guessing. And you could do that in neural net. And I was very excited by that. AN: I see. Wow, right. Yep, I think I remember all of these papers. You and Hinton, approximate paper, spent many hours reading over that. And I think some of the algorithms you use today, or some of the algorithms that lots of people use almost every day, are what, things like dropouts, or I guess activations came from your group? Yes and no. So other people have thought about rectified linear units. And we actually did some work with restricted Boltzmann machines showing that a ReLU was almost exactly equivalent to a whole stack of logistic units. And thatâ€™s one of the things that helped ReLUs catch on. AN: I was really curious about that. The value paper had a lot of math showing that this function can be approximated with this really complicated formula. Did you do that math so your paper would get accepted into an academic conference, or did all that math really influence the development of max of 0 and x? That was one of the cases where actually the math was important to the development of the idea. So I knew about rectified linear units, obviously, and I knew about logistic units. And because of the work on Boltzmann machines, all of the basic work was done using logistic units. And so the question was, could the learning algorithm work in something with rectified linear units? And by showing the rectified linear units were almost exactly equivalent to a stack of logistic units, we showed that all the math would go through. ANï¼š I see. And it provided the inspiration for today, tons of people use ReLU and it just works without without necessarily needing to understand the same motivation. Yeah, one thing I noticed later when I went to Google. I guess in 2014, I gave a talk at Google about using ReLUs and initializing with the identity matrix. because the nice thing about ReLUs is that if you keep replicating the hidden layers and you initialize with the identity, it just copies the pattern in the layer below. And so I was showing that you could train networks with 300 hidden layers and you could train them really efficiently if you initialize with their identity. But I didnâ€™t pursue that any further and I really regret not pursuing that. We published one paper with showing you could initialize an active showing you could initialize recurringness like that. But I should have pursued it further because Later on these residual networks is really that kind of thing. AN: Over the years Iâ€™ve heard you talk a lot about the brain. Iâ€™ve heard you talk about relationship being back prop and the brain. What are your current thoughts on that? Iâ€™m actually working on a paper on that right now. I guess my main thought is this. If it turns out the back prop is a really good algorithm for doing learning. Then for sure evolution couldâ€™ve figured out how to prevent it. I mean you have cells that could turn into either eyeballs or teeth. Now, if cells can do that, they can for sure implement back propagation and presumably this huge selective pressure for it. So I think the neuroscientist idea that it doesnâ€™t look plausible is just silly. There may be some subtle implementation of it. And I think the brain probably has something that may not be exactly be back propagation, but itâ€™s quite close to it. And over the years, Iâ€™ve come up with a number of ideas about how this might work. So in 1987, working with Jay McClelland, I came up with the recirculation algorithm, where the idea is you send information round a loop. And you try to make it so that things donâ€™t change as information goes around this loop. So the simplest version would be you have input units and hidden units, and you send information from the input to the hidden and then back to the input, and then back to the hidden and then back to the input and so on. And what you want, you want to train an auto-encoder, but you want to train it without having to do back propagation. So you just train it to try and get rid of all variation in the activities. So the idea is that the learning rule for synapse is change the weighting proportion to the pre-synaptic input and in proportion to the rate of change at the post synaptic input. But in recirculation, youâ€™re trying to make the post synaptic input, youâ€™re trying to make the old one be good and the new one be bad, so youâ€™re changing in that direction. We invented this algorithm before neuroscientists come up with spike-timing-dependent plasticity. Spike-timing-dependent plasticity is actually the same algorithm but the other way round, where the new thing is good and the old thing is bad in the learning rule. So youâ€™re changing the weighting proportions to the preset outlook activity times the new person outlook activity minus the old one. Later on I realized in 2007, that if you took a stack of Restricted Boltzmann machines and you trained it up. After it was trained, you then had exactly the right conditions for implementing back propagation by just trying to reconstruct. If you looked at the reconstruction era, that reconstruction era would actually tell you the derivative of the discriminative performance. And at the first deep learning workshop at in 2007, I gave a talk about that. That was almost completely ignored. Later on, Yoshua Bengio, took up the idea and thatâ€™s actually done quite a lot of more work on that. And Iâ€™ve been doing more work on it myself. And I think this idea that if you have a stack of auto-encoders, then you can get derivatives by sending activity backwards and locate reconstructionaires, is a really interesting idea and may well be how the brain does it. AN: One other topic that I know you follow about and that I hear youâ€™re still working on is how to deal with multiple time skills in deep learning? So, can you share your thoughts on that? Yes, so actually, that goes back to my first years of graduate student. The first talk I ever gave was about using what I called fast weights. So weights that adapt rapidly, but decay rapidly. And therefore can hold short term memory. And I showed in a very simple system in 1973 that you could do true recursion with those weights. And what I mean by true recursion is that the neurons that is used in representing things get re-used for representing things in the recursive core. And the weights that is used for actually knowledge get re-used in the recursive core. And so that leads the question of when you pop out your recursive core, how do you remember what it was you were in the middle of doing? Whereâ€™s that memory? because you used the neurons for the recursive core. And the answer is you can put that memory into fast weights, and you can recover the activities neurons from those fast weights. And more recently working with Jimmy Ba, we actually got a paper in it by using fast weights for recursion like that. I see. So that was quite a big gap. The first model was unpublished in 1973 and then Jimmy Baâ€™s model was in 2015, I think, or 2016. So itâ€™s about 40 years later. AN: And, I guess, one other idea of quite a few years now, over five years, I think is capsules, where are you with that? Okay, so Iâ€™m back to the state Iâ€™m used to being in. Which is I have this idea I really believe in and nobody else believes it. And I submit papers about it and they would get rejected. But I really believe in this idea and Iâ€™m just going to keep pushing it. So it hinges on, thereâ€™s a couple of key ideas. One is about how you represent multi dimensional entities, and you can represent multi-dimensional entities by just a little backdoor activities. As long as you know thereâ€™s any one of them. So the idea is in each region of the image, youâ€™ll assume thereâ€™s at most, one of the particular kind of feature. And then youâ€™ll use a bunch of neurons, and their activities will represent the different aspects to that feature, like within that region exactly what are its x and y coordinates? What orientation is it at? How fast is it moving? What color is it? How bright is it? And stuff like that. So you can use a whole bunch of neurons to represent different dimensions of the same thing. Provided thereâ€™s only one of them. Thatâ€™s a very different way of doing representation from what weâ€™re normally used to in neuronettes. Normally in neuronettes, we just have a great big layer, and all the units go off and do whatever they do. But you donâ€™t think of bundling them up into little groups that represent different coordinates of the same thing. So I think we should beat this extra structure. And then the other idea that goes with that. AN: So this means in the truth of the representation, you partition the representation to different subsets, to represent, right, rather than I call each of those subsets a capsule. And the idea is a capsule is able to represent an instance of a feature, but only one. And it represents all the different properties of that feature. Itâ€™s a feature that has a lot of properties as opposed to a normal neuron and a normal neuronette, which has just one scale of property. And then what you can do if youâ€™ve got that, is you can do something that normal neuronettes are very bad at, which is you can do what I call routine by agreement. So letâ€™s suppose you want to do segmentation and you have something that might be a mouth and something else that might be a nose. And you want to know if you should put them together to make one thing. So the idea should have a capsule for a mouth that has the parameters of the mouth. And you have a capsule for a nose that has the parameters of the nose. And then to decipher whether to put them together or not, you get each of them to vote for what the parameters should be for a face. Now if the mouth and the nose are in the right spacial relationship, they will agree. So when you get two captures at one level voting for the same set of parameters at the next level up, you can assume theyâ€™re probably right, because agreement in a high dimensional space is very unlikely. And thatâ€™s a very different way of doing filtering, than what we normally use in neural nets. So I think this routing by agreement is going to be crucial for getting neural nets to generalize much better from limited data. I think itâ€™d be very good at getting the changes in viewpoint, very good at doing segmentation. And Iâ€™m hoping it will be much more statistically efficient than what we currently do in neural nets. Which is, if you want to deal with changes in viewpoint, you just give it a whole bunch of changes in view point and training on them all. AN: I see, right, so rather than FIFO learning, supervised learning, you can learn this in some different way. Well, I still plan to do it with supervised learning, but the mechanics of the forward paths are very different. Itâ€™s not a pure forward path in the sense that thereâ€™s little bits of iteration going on, where you think you found a mouth and you think you found a nose. And use a little bit of iteration to decide whether they should really go together to make a face. And you can do back props from that iteration. So you can try and do it a little discriminatively, and weâ€™re working on that now at my group in Toronto. So I now have a little Google team in Toronto, part of the Brain team. Thatâ€™s what Iâ€™m excited about right now. AN: I see, great, yeah. Look forward to that paper when that comes out. You worked in deep learning for several decades. Iâ€™m actually really curious, how has your thinking, your understanding of AI changed over these years? So I guess a lot of my intellectual history has been around back propagation, and how to use back propagation, how to make use of its power. So to begin with, in the mid 80s, we were using it for discriminative learning and it was working well. I then decided, by the early 90s, that actually most human learning was going to be unsupervised learning. And I got much more interested in unsupervised learning, and thatâ€™s when I worked on things like the Wegstein algorithm. AN: And your comments at that time really influenced my thinking as well. So when I was leading Google Brain, our first project spent a lot of work in unsupervised learning because of your influence. Right, and I may have misled you. Because in the long run, I think unsupervised learning is going to be absolutely crucial. But you have to sort of face reality. And whatâ€™s worked over the last ten years or so is supervised learning. Discriminative training, where you have labels, or youâ€™re trying to predict the next thing in the series, so that acts as the label. And thatâ€™s worked incredibly well. I still believe that unsupervised learning is going to be crucial, and things will work incredibly much better than they do now when we get that working properly, but we havenâ€™t yet. AN: Yeah, I think many of the senior people in deep learning, including myself, remain very excited about it. Itâ€™s just none of us really have almost any idea how to do it yet. Maybe you do, I donâ€™t feel like I do. Variational altering code is where you use the reparameterization tricks. Seemed to me like a really nice idea. And generative adversarial nets also seemed to me to be a really nice idea. I think generative adversarial nets are one of the sort of biggest ideas in deep learning thatâ€™s really new. Iâ€™m hoping I can make capsules that successful, but right now generative adversarial nets, I think, have been a big breakthrough. AN: What happened to sparsity and slow features, which were two of the other principles for building unsupervised models? I was never as big on sparsity as you were, buddy. But slow features, I think, is a mistake. You shouldnâ€™t say slow. The basic idea is right, but you shouldnâ€™t go for features that donâ€™t change, you should go for features that change in predictable ways. So hereâ€™s a sort of basic principle about how you model anything. You take your measurements, and youâ€™re applying nonlinear transformations to your measurements until you get to a representation as a state vector in which the action is linear. So you donâ€™t just pretend itâ€™s linear like you do with common filters. But you actually find a transformation from the observables to the underlying variables where linear operations, like matrix multipliers on the underlying variables, will do the work. So for example, if you want to change viewpoints. If you want to produce the image from another viewpoint, what you should do is go from the pixels to coordinates. And once you got to the coordinate representation, which is a kind of thing Iâ€™m hoping captures will find. You can then do a matrix multiplier to change viewpoint, and then you can map it back to pixels. AN: Right, thatâ€™s why you did all that. I think thatâ€™s a very, very general principle. AN: Thatâ€™s why you did all that work on face synthesis, right? Where you take a face and compress it to very low dimensional vector, and so you can fiddle with that and get back other faces. I had a student who worked on that, I didnâ€™t do much work on that myself. AN: Now Iâ€™m sure you still get asked all the time, if someone wants to break into deep learning, what should they do? So what advice would you have? Iâ€™m sure youâ€™ve given a lot of advice to people in one on one settings, but for the global audience of people watching this video. What advice would you have for them to get into deep learning? Okay, so my advice is sort of read the literature, but donâ€™t read too much of it. So this is advice I got from my advisor, which is very unlike what most people say. Most people say you should spend several years reading the literature and then you should start working on your own ideas. And that may be true for some researchers, but for creative researchers I think what you want to do is read a little bit of the literature. And notice something that you think everybody is doing wrong, Iâ€™m contrary in that sense. You look at it and it just doesnâ€™t feel right. And then figure out how to do it right. And then when people tell you, thatâ€™s no good, just keep at it. And I have a very good principle for helping people keep at it, which is either your intuitions are good or theyâ€™re not. If your intuitions are good, you should follow them and youâ€™ll eventually be successful. If your intuitions are not good, it doesnâ€™t matter what you do. AN: Inspiring advice, might as well go for it. You might as well trust your intuitions. Thereâ€™s no point not trusting them. AN: I usually advise people to not just read, but replicate published papers. And maybe that puts a natural limiter on how many you could do, because replicating results is pretty time consuming. Yes, itâ€™s true that when youâ€™re trying to replicate a published you discover all over little tricks necessary to make it work. The other advice I have is, never stop programming. Because if you give a student something to do, if theyâ€™re botching, theyâ€™ll come back and say, it didnâ€™t work. And the reason it didnâ€™t work would be some little decision they made, that they didnâ€™t realize is crucial. And if you give it to a good student, like UY Tay for example. You can give him anything and heâ€™ll come back and say, it worked. I remember doing this once, and I said, but wait a minute UY. Since we last talked, I realized it couldnâ€™t possibly work for the following reason. And said, yeah, I realized that right away, so I assumed you didnâ€™t mean that. AN: I see, yeah, thatâ€™s great, yeah. Letâ€™s see, any other advice for people that want to break into AI and deep learning? I think thatâ€™s basically, read enough so you start developing intuitions. And then, trust your intuitions and go for it, donâ€™t be too worried if everybody else says itâ€™s nonsense. AN: And I guess thereâ€™s no way to know if others are right or wrong when they say itâ€™s nonsense, but you just have to go for it, and then find out. Right, but there is one thing, which is, if you think itâ€™s a really good idea, and other people tell you itâ€™s complete nonsense, then you know youâ€™re really on to something. So one example of that is when and I first came up with variational methods. I sent mail explaining it to a former student of mine called Peter Brown, who knew a lot about. And he showed it to people who worked with him, called the brothers, they were twins, I think. And he then told me later what they said, and they said, either this guyâ€™s drunk, or heâ€™s just stupid, so they really, really thought it was nonsense. Now, it could have been partly the way I explained it, because I explained it in intuitive terms. But when you have what you think is a good idea and other people think is complete rubbish, thatâ€™s the sign of a really good idea. AN: I see, and research topics, new grad students should work on capsules and maybe unsupervised learning, any other? One good piece of advice for new grad students is, see if you can find an advisor who has beliefs similar to yours. Because if you work on stuff that your advisor feels deeply about, youâ€™ll get a lot of good advice and time from your advisor. If you work on stuff your advisorâ€™s not interested in, all youâ€™ll get is, you get some advice, but it wonâ€™t be nearly so useful. AN: I see, and last one on advice for learners, how do you feel about people entering a PhD program? Versus joining a top company, or a top research group? Yeah, itâ€™s complicated, I think right now, whatâ€™s happening is, there arenâ€™t enough academics trained in deep learning to educate all the people that we need educated in universities. There just isnâ€™t the faculty bandwidth there, but I think thatâ€™s going to be temporary. I think whatâ€™s happened is, most departments have been very slow to understand the kind of revolution thatâ€™s going on. I kind of agree with you, that itâ€™s not quite a second industrial revolution, but itâ€™s something on nearly that scale. And thereâ€™s a huge sea change going on, basically because our relationship to computers has changed. Instead of programming them, we now show them, and they figure it out. Thatâ€™s a completely different way of using computers, and computer science departments are built around the idea of programming computers. And they donâ€™t understand that sort of, this showing computers is going to be as big as programming computers. Except they donâ€™t understand that half the people in the department should be people who get computers to do things by showing them. So my department refuses to acknowledge that it should have lots and lots of people doing this. They think they got a couple, maybe a few more, but not too many. And in that situation, you have to remind the big companies to do quite a lot of the training. So Google is now training people, we call brain residence, I suspect the universities will eventually catch up. AN: In fact, maybe a lot of students have figured this out. A lot of top 50 programs, over half of the applicants are actually wanting to work on showing, rather than programming. Yeah, cool, AN: yeah, in fact, to give credit where itâ€™s due, whereas a deep learning AI is creating a deep learning specialization. As far as I know, their first deep learning MOOC was actually yours taught on Coursera, back in 2012, as well. And somewhat strangely, thatâ€™s when you first published the RMS algorithm, which also is a rough. Right, yes, well, as you know, that was because you invited me to do the MOOC. And then when I was very dubious about doing, you kept pushing me to do it, so it was very good that I did, although it was a lot of work. AN: Yes, and thank you for doing that, I remember you complaining to me, how much work it was. And you staying out late at night, but I think many, many learners have benefited for your first MOOC, so Iâ€™m very grateful to you for it, so, over the years, Iâ€™ve seen you embroiled in debates about paradigms for AI, and whether thereâ€™s been a paradigm shift for AI. What are your, can you share your thoughts on that? Yes, happily, so I think that in the early days, back in the 50s, people like von Neumann and didnâ€™t believe in symbolic AI, they were far more inspired by the brain. Unfortunately, they both died much too young, and their voice wasnâ€™t heard. And in the early days of AI, people were completely convinced that the representations you need for intelligence were symbolic expressions of some kind. Sort of cleaned up logic, where you could do nomeratonic things, and not quite logic, but something like logic, and that the essence of intelligence was reasoning. Whatâ€™s happened now is, thereâ€™s a completely different view, which is that what a thought is, is just a great big vector of neural activity, so contrast that with a thought being a symbolic expression. And I think the people who thought that thoughts were symbolic expressions just made a huge mistake. What comes in is a string of words, and what comes out is a string of words. And because of that, strings of words are the obvious way to represent things. So they thought what must be in between was a string of words, or something like a string of words. And I think whatâ€™s in between is nothing like a string of words. I think the idea that thoughts must be in some kind of language is as silly as the idea that understanding the layout of a spatial scene must be in pixels, pixels come in. And if we could, if we had a dot matrix printer attached to us, then pixels would come out, but whatâ€™s in between isnâ€™t pixels. And so I think thoughts are just these great big vectors, and that big vectors have causal powers. They cause other big vectors, and thatâ€™s utterly unlike the standard AI view that thoughts are symbolic expressions. AN: I see, good, I guess AI is certainly coming round to this new point of view these days. Some of it, I think a lot of people in AI still think thoughts have to be symbolic expressions. AN: Thank you very much for doing this interview. It was fascinating to hear how deep learning has evolved over the years, as well as how youâ€™re still helping drive it into the future, so thank you, Jeff. Well, thank you for giving me this opportunity. AN: Thank you. References Geoffrey Hinton interview, https://www.coursera.org/learn/neural-networks-deep-learning/lecture/dcm5r/geoffrey-hinton-interview","tags":[]},{"title":"ç”»è™¾","date":"2017-09-03T06:31:32.000Z","path":"2017/09/03/ç”»è™¾/","text":"ä»Šè´­å¾—ä¸€ç”»ï¼Œç”šå–œã€‚ æ„å›¾ä¸¥è°¨è€Œä¸å¤±å¤§æ°”ï¼Œç–å¯†æœ‰è‡´ä¸”é¥ç›¸å‘¼åº”ï¼Œç¬”å¢¨çµåŠ¨çŠ¹å¦‚æ´»ç‰©è·ƒç„¶çº¸ä¸Šã€‚é½ç™½çŸ³ä¸‰å­—å­—è¿¹é’åŠ²æœ‰åŠ›ï¼Œå´åˆé€éœ²å‡ºå„¿ç«¥èˆ¬çš„å¤©çœŸçƒ‚æ¼«ã€‚ å­—ï¼Œç”»ç›¸åº”å¾—ç« ï¼Œç–å¯èµ°é©¬å¯†ä¸é€é£ï¼Œè£…è£±æ›´æ˜¯åˆ«å…·ä¸€æ ¼ï¼Œè¾¹ä¸Šåœ†ç‚¹ç–å¯†ä¸€è‡´ï¼Œæ·±è‰²èƒŒæ™¯ä¸æµ…è‰²ç”»é¢ç›¸äº’æ˜ è¡¬ã€‚ å®ä¸ºéš¾å¾—ä½³ä½œï¼Œå¹¶ç»ç –å®¶é‰´å®šçœŸè¿¹æ— ç–‘ã€‚ æ­¤ç”»è¿˜æœ‰ä¸€ç¥å¥‡ä¹‹å¤„ï¼Œè‹¥æ”¾ç¬¼å±‰è’¸ 10 åˆ†é’Ÿï¼Œç”»é¢ç”Ÿå½©ï¼Œè§¦è§‰è‚Œç†ï¼Œæ„å¢ƒæ›´ä½³ã€‚","tags":[]},{"title":"ä¸ºä»€ä¹ˆè¦è¯»ä¹¦","date":"2017-09-03T06:09:19.000Z","path":"2017/09/03/ä¸ºä»€ä¹ˆè¦è¯»ä¹¦/","text":"çˆ¶å­äºŒäººé¥®èŒ¶ï¼Œå„¿é—®ï¼šâ€œä¸ºä»€ä¹ˆè¦æˆ‘è¯»ä¹¦ï¼Ÿâ€ çˆ¶ç­”ï¼šâ€œæˆ‘è¿™ä¹ˆè·Ÿä½ è¯´å§ï¼ä½ è¯»äº†ä¹¦ï¼Œå–è¿™èŒ¶æ—¶å°±ä¼šè¯´ï¼šâ€˜æ­¤èŒ¶æ±¤è‰²æ¾„çº¢é€äº®ï¼Œæ°”å‘³å¹½é¦™å¦‚å…°ï¼Œå£æ„Ÿé¥±æ»¡çº¯æ­£ï¼Œåœ†æ¶¦å¦‚è¯—ï¼Œå›å‘³ç”˜é†‡ï¼Œé½¿é¢Šç•™èŠ³ï¼ŒéŸµå‘³åè¶³ï¼Œé¡¿è§‰å¦‚æ¢¦ä¼¼å¹»ï¼Œä»¿ä½›å¤©ä¸Šäººé—´ï¼ŒçœŸä¹ƒèŒ¶ä¸­æå“ï¼â€™â€ è€Œå¦‚æœä½ æ²¡æœ‰è¯»ä¹¦ï¼Œä½ å°±ä¼šè¯´ï¼šï¼šâ€œâ€˜å§æ“ï¼è¿™èŒ¶ä¸èµ–â€™â€ è¿‘æ—¥å¯¹ã€Šä¸­å›½è¯—è¯å¤§ä¼šã€‹ä¸Šç˜¾ï¼Œäºæ˜¯æ€è€ƒäººä¸ºä»€ä¹ˆè¦è¯»ä¹¦ï¼Ÿåˆå¦‚ä½•ç”¨å¥½çš„è¯è¯­æ¥æè¿°å¿ƒæƒ…å’Œæ„Ÿå—å‘¢ï¼Ÿ æœ‰äººæ›¾æå‡ºè¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šå¤§éƒ¨åˆ†è¯»è¿‡çš„ä¹¦æœ€åéƒ½ä¼šå¿˜æ‰ï¼Œé‚£è¯»ä¹¦çš„æ„ä¹‰ä½•åœ¨ï¼Ÿè¿™æ˜¯æˆ‘è§è¿‡æœ€å¥½çš„å›ç­”ï¼šâ€œå°çš„æ—¶å€™æˆ‘åƒäº†å¾ˆå¤šä¸œè¥¿ï¼Œå…¶ä¸­çš„å¤§éƒ¨åˆ†æˆ‘å·²è®°ä¸æ¸…æ˜¯ä»€ä¹ˆï¼Œä½†æˆ‘çŸ¥é“ï¼Œä»–ä»¬å·²ç»æˆäº†æˆ‘ç°åœ¨çš„éª¨å’Œè‚‰â€ã€‚è¯»ä¹¦ï¼Œä¹Ÿæ˜¯å¦‚æ­¤ã€‚å®ƒåœ¨ä¸çŸ¥ä¸è§‰ä¸­å°±å·²ç»å½±å“äº†ä½ çš„æ€æƒ³ï¼Œä½ çš„è¨€è¡Œï¼Œä½ çš„å½¢è±¡ã€‚ 1ï¼Œå½“ä½ å¼€å¿ƒçš„æ—¶å€™ä½ å¯ä»¥è¯´ï¼šæ˜¥é£å¾—æ„é©¬è¹„ç–¾ä¸€æ—¥çœ‹å°½é•¿å®‰èŠ± è€Œä¸æ˜¯åªä¼šè¯´ï¼šå“ˆå“ˆ, å“ˆå“ˆ, å“ˆå“ˆ, å“ˆå“ˆ, å“ˆå“ˆå“ˆ 2ï¼Œå½“ä½ ä¼¤å¿ƒçš„æ—¶å€™ä½ å¯ä»¥è¯´ï¼šé—®å›èƒ½æœ‰å‡ å¤šæ„ï¼Œæ°ä¼¼ä¸€æ±Ÿæ˜¥æ°´å‘ä¸œæµ è€Œä¸æ˜¯åªä¼šè¯´ï¼šæˆ‘çš„å¿ƒå¥½ç—› 3ï¼Œå½“ä½ çœ‹åˆ°å¸…å“¥æ—¶ä½ å¯ä»¥è¯´ï¼šé™Œä¸Šäººå¦‚ç‰å…¬å­ä¸–æ— åŒ è€Œä¸æ˜¯åªä¼šè¯´ï¼šæˆ‘é ï¼Œå¥½å¸…ï¼æˆ‘é é é ï¼Œå¤ªå¸…äº† 4ï¼Œå½“ä½ çœ‹åˆ°ç¾å¥³æ—¶ä½ å¯ä»¥è¯´ï¼šåŒ—æ–¹æœ‰ä½³äººï¼Œç»ä¸–è€Œç‹¬ç«‹ è€Œä¸æ˜¯åªä¼šè¯´ï¼šæˆ‘å»ï¼Œå¥¹å¥½ç¾æˆ‘å»ï¼Œå¥¹çœŸç¾ 5ï¼Œå½“ä½ é‡è§æ¸£ç”·æ—¶ä½ å¯ä»¥è¯´ï¼šé‡äººä¸æ·‘ ï¼Œè¯†äººä¸å–„ è€Œä¸æ˜¯åªä¼šè¯´ï¼šçäº†è€å­çš„ç‹—çœ¼ 6ï¼Œå½“ä½ å‘ä¸€ä¸ªäººè¡¨è¾¾çˆ±æ„æ—¶ä½ å¯ä»¥è¯´ï¼šå±±æœ‰æœ¨å…®æœ¨æœ‰æå¿ƒæ‚¦å›å…®å›ä¸çŸ¥ è€Œä¸æ˜¯åªä¼šè¯´ï¼šæˆ‘å–œæ¬¢ä½ ï¼Œå¤©è’åœ°è€ï¼Œæµ·æ¯çŸ³çƒ‚ 7ï¼Œå½“ä½ æ€å¿µä¸€ä¸ªäººçš„æ—¶å€™ä½ å¯ä»¥è¯´ï¼šè¡£å¸¦æ¸å®½ç»ˆä¸æ‚”ä¸ºä¼Šæ¶ˆå¾—äººæ†”æ‚´ è€Œä¸æ˜¯åªä¼šè¯´ï¼šæˆ‘æƒ³æ­»ä½ å•¦ 8ï¼Œå½“ä½ å¤±æ‹çš„æ—¶å€™ä½ å¯ä»¥è¯´ï¼šäººç”Ÿè‹¥åªå¦‚åˆè§ä½•äº‹ç§‹é£æ‚²ç”»æ‰‡ç­‰é—²å˜å´æ•…äººå¿ƒå´é“æ•…äººå¿ƒæ˜“å˜ è€Œä¸æ˜¯åªä¼šåƒä¸‡éçš„å‘¼å–Šï¼šè“ç˜¦ï¼Œé¦™è‡ 9ï¼Œç»“å©šçš„æ—¶å€™ä½ å¯ä»¥è¯´ï¼šæ˜¥å®µä¸€åˆ»å€¼åƒé‡‘èŠ±æœ‰æ¸…é¦™æœˆæœ‰é˜´ è€Œä¸æ˜¯åªä¼šè¯´ï¼šå˜¿å˜¿, å˜¿å˜¿, å˜¿å˜¿å˜¿ 10ï¼Œåˆ†æ‰‹çš„æ—¶å€™ä½ å¯ä»¥è¯´ï¼šç›¸æ¿¡ä»¥æ²«ä¸å¦‚ç›¸å¿˜äºæ±Ÿæ¹– è€Œä¸æ˜¯åªä¼šè¯´ï¼šæˆ‘ä»¬ä¸åˆé€‚ 11ï¼Œçœ‹è§å¤§æ¼ æˆˆå£çš„æ—¶å€™ä½ å¯ä»¥è¯´ï¼šå¤§æ¼ å­¤çƒŸç›´ï¼Œé•¿æ²³è½æ—¥åœ† è€Œä¸æ˜¯åªä¼šè¯´ï¼šå”‰å‘€å¦ˆå‘€ï¼Œè¿™å…¨éƒ½æ˜¯æ²™å­ 12ï¼Œçœ‹è§å¤•é˜³ä½™æ™–çš„æ—¶å€™ä½ å¯ä»¥è¯´ï¼šè½éœä¸å­¤é¹œé½é£ç§‹æ°´å…±é•¿å¤©ä¸€è‰² è€Œä¸æ˜¯åªä¼šè¯´ï¼šå§æ§½è¿™ä¹ˆå¤šé¸ŸçœŸå¥½çœ‹çœŸä»–å¦ˆå¤ªå¥½çœ‹äº†","tags":[]},{"title":"C10k / C10M Challenge","date":"2017-09-03T04:38:45.000Z","path":"2017/09/03/C10k-C10M-Challenge/","text":"C10k / C10M Challenge æŒ‘æˆ˜ HistoryThe term was coined in 1999 by Dan Kegel, citing the Simtel FTP host, cdrom.com, serving 10,000 clients at once over 1 gigabit per second Ethernet in that year. The term has since been used for the general issue of large number of clients, with similar numeronyms for larger number of connections, most recently C10M in the 2010s. By the early 2010s millions of connections on a single commodity 1U server became possible: over 2 million connections (WhatsApp, 24 cores, using Erlang on FreeBSD), 10â€“12 million connections (MigratoryData, 12 cores, using Java on Linux). C10kï¼ˆconcurrently handling 10k connectionsï¼‰æ˜¯ä¸€ä¸ªåœ¨ 1999 å¹´è¢«æå‡ºæ¥çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œå¦‚ä½•åœ¨ä¸€é¢— 1GHz CPUï¼Œ2G å†…å­˜ï¼Œ1Gbps ç½‘ç»œç¯å¢ƒä¸‹ï¼Œè®©å•å°æœåŠ¡å™¨åŒæ—¶ä¸º 1 ä¸‡ä¸ªå®¢æˆ·ç«¯æä¾› FTP æœåŠ¡ã€‚è€Œåˆ°äº† 2010 å¹´åï¼Œéšç€ç¡¬ä»¶æŠ€æœ¯çš„å‘å±•ï¼Œè¿™ä¸ªé—®é¢˜è¢«å»¶ä¼¸ä¸º C10Mï¼Œå³å¦‚ä½•åˆ©ç”¨ 8 æ ¸å¿ƒ CPUï¼Œ64G å†…å­˜ï¼Œåœ¨ 10Gbps çš„ç½‘ç»œä¸Šä¿æŒ 1000 ä¸‡å¹¶å‘è¿æ¥ï¼Œæˆ–æ˜¯æ¯ç§’é’Ÿå¤„ç† 100 ä¸‡çš„è¿æ¥ã€‚ï¼ˆä¸¤ç§ç±»å‹çš„è®¡ç®—æœºèµ„æºåœ¨å„è‡ªçš„æ—¶ä»£éƒ½çº¦ä¸º 1200 ç¾å…ƒï¼‰ã€‚ C10k / C10M é—®é¢˜åˆ™æ˜¯ä»æŠ€æœ¯è§’åº¦å‡ºå‘æŒ‘æˆ˜è½¯ç¡¬ä»¶æé™ã€‚C10k / C10M é—®é¢˜å¾—è§£ï¼Œæˆæœ¬é—®é¢˜å’Œæ•ˆç‡é—®é¢˜è¿åˆƒè€Œè§£ã€‚ References C10k problem, https://en.wikipedia.org/wiki/C10k_problem The C10K problem, http://www.kegel.com/c10k.html The Secret To 10 Million Concurrent Connections - The Kernel Is The Problem Not The Solution, http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html Inside NGINX: How We Designed for Performance &amp; Scale, https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/ æ¶æ„å¸ˆå®è·µæ—¥ï½œä»C10Kåˆ°C10Mé«˜æ€§èƒ½ç½‘ç»œçš„æ¢ç´¢ä¸å®è·µ, http://blog.qiniu.com/archives/4941","tags":[]},{"title":"Plato and Socrates - æŸæ‹‰å›¾éº¦ç©—é—®é¢˜","date":"2017-09-02T03:58:00.000Z","path":"2017/09/02/Plato-and-Socrates-æŸæ‹‰å›¾éº¦ç©—é—®é¢˜/","text":"One day, Plato asked his teacher Socrates, â€œWhat is love? How can I find it?â€ Socrates answered, â€œThere is a vast wheat field in front. Walk forward without turning back, and pick only one stalk. If you find the most magnificent stalk, then you have found love.â€ Plato walked forward, and before long, he returned with empty hands, having picked nothing. His teacher asked, â€œWhy did you not pick any stalk?â€ Plato answered, â€œBecause I could only pick once, and yet I could not turn back. I did find the most magnificent stalk, but did not know if there were any better ones ahead, so I did not pick it. As I walked further, the stalks that I saw were not as good as the earlier one, so! I did not pick any in the end.â€ Socrates then said, â€œAnd that is LOVE.â€ On another day, Plato asked Socrates: â€œWhat is marriage? How can I find it?â€ His teacher answered, â€œThere is a thriving forest in front. Walk forward without turning back, and chop down only one tree. If you find the tallest tree, then you have found marriageâ€. Plato walked forward, and before long, He returned with a tree. The tree was not bad but it was not tall, either. It was only an ordinary tree, not the best but just a good tree. His teacher asked, â€œWhy did you chop down such an ordinary tree?â€ Plato answered, â€œBased on my previous experience, I had walked through the field, but returned with empty hands. This time, I saw this tree, and I felt that it was the first good tree I had seen, so I chopped it down and brought it back. I did not want to miss the chance.â€ Socrates then said, â€œAnd that is MARRIAGE. On another day, Plato asked his teacher, â€œWhat is life?â€ Socrates asked him to go to the forest again, allowed back and forth as well, and pluck the most beautiful flower. Plato walked forward. However he hadnâ€™t come back for 3 days. His teacher went to find him. When he saw Platoâ€™s camping in the forest, he asked:â€ Have you found the most beautiful flower?â€ Plato pointed a flower near to his camp and answered, â€œThis is the most beautiful flower!â€ â€œWhy didnâ€™t you take it out?â€ Socrates asked. â€œBecause if I pick it, it would be drooping. Even though I didnâ€™t pick, it would die in a couple of days for sure. So I had been living by its side while it was blooming. When itâ€™s drooped, I was up to find another one. This is the second most beautiful flower I have found!â€ Socrates then said, â€œYouâ€™ve got the truth of LIFEâ€ â€œLoveâ€ is the most beautiful thing to happen to a person, itâ€™s an opportunity you donâ€™t realize its worth when you have it but only when itâ€™s gone like the field of stalks. â€œMarriageâ€ is like the tree you chopped, itâ€™s a compromise; you pick the first best thing you see and learn to live a happy life with it. Having an affair is alluring. Itâ€™s like lightning - bright but disappeared so quickly that you cannot catch up with and keep it. â€œLifeâ€ is to follow and enjoy the every beautiful moment of living. Thatâ€™s why you should enjoy your life wherever you live. æœ‰ä¸€å¤©ï¼Œå¤å¸Œè…Šå“²å­¦å®¶æŸæ‹‰å›¾é—®ä»–çš„è€å¸ˆè‹æ ¼æ‹‰åº•ä»€ä¹ˆæ˜¯çˆ±æƒ…ï¼Œä»–çš„è€å¸ˆå°±å«ä»–å…ˆåˆ°éº¦ç”°é‡Œï¼Œæ‘˜ä¸€æ£µå…¨éº¦ç”°é‡Œæœ€å¤§æœ€é‡‘é»„çš„çš„éº¦ç©—ã€‚æœŸé—´åªèƒ½æ‘˜ä¸€æ¬¡ï¼Œå¹¶ä¸”åªå¯ä»¥å‘å‰èµ°ï¼Œä¸èƒ½å›å¤´ã€‚æŸæ‹‰å›¾äºæ˜¯ç…§ç€è€å¸ˆçš„è¯´è¯åšã€‚ç»“æœï¼Œä»–ä¸¤æ‰‹ç©ºç©ºçš„èµ°å‡ºéº¦ç”°ã€‚è€å¸ˆé—®ä»–ä¸ºä»€ä¹ˆæ‘˜ä¸åˆ°ï¼Œä»–è¯´ï¼šâ€œå› ä¸ºåªèƒ½æ‘˜ä¸€æ¬¡ï¼Œåˆä¸èƒ½èµ°å›å¤´è·¯ï¼Œå…¶é—´å³ä½¿è§åˆ°ä¸€æ£µåˆå¤§åˆé‡‘é»„çš„ï¼Œå› ä¸ºä¸çŸ¥å‰é¢æ˜¯å¦æœ‰æ›´å¥½ï¼Œæ‰€ä»¥æ²¡æœ‰æ‘˜ï¼›èµ°åˆ°å‰é¢æ—¶ï¼Œåˆå‘è§‰æ€»ä¸åŠä¹‹å‰è§åˆ°çš„å¥½ï¼ŒåŸæ¥éº¦ç”°é‡Œæœ€å¤§æœ€é‡‘é»„çš„éº¦ç©—ï¼Œæ—©å°±é”™è¿‡äº†ï¼›äºæ˜¯ï¼Œæˆ‘ä»€ä¹ˆä¹Ÿæ²¡æ‘˜åˆ°ã€‚â€ è‹æ ¼æ‹‰åº•è¯´ï¼šâ€œè¿™å°±æ˜¯çˆ±æƒ…ã€‚â€ ä¹‹ååˆæœ‰ä¸€å¤©ï¼ŒæŸæ‹‰å›¾é—®ä»–çš„è€å¸ˆä»€ä¹ˆæ˜¯å©šå§»ï¼Œä»–çš„è€å¸ˆå°±å«ä»–å…ˆåˆ°æ ‘æ—é‡Œï¼Œç ä¸‹ä¸€æ£µå…¨æ ‘æ—æœ€å¤§æœ€èŒ‚ç››ã€æœ€é€‚åˆæ”¾åœ¨å®¶ä½œåœ£è¯æ ‘çš„æ ‘ã€‚å…¶é—´åŒæ ·åªèƒ½æ‘˜ä¸€æ¬¡ï¼Œä»¥åŠåŒæ ·åªå¯ä»¥å‘å‰èµ°ï¼Œä¸èƒ½å›å¤´ã€‚æŸæ‹‰å›¾äºæ˜¯ç…§ç€è€å¸ˆçš„è¯´è¯åšã€‚ä»Šæ¬¡ï¼Œä»–å¸¦äº†ä¸€æ£µæ™®æ™®é€šé€šï¼Œä¸æ˜¯å¾ˆèŒ‚ç››ï¼Œäº¦ä¸ç®—å¤ªå·®çš„æ ‘å›æ¥ã€‚è€å¸ˆé—®ä»–ï¼Œæ€ä¹ˆå¸¦è¿™æ£µæ™®æ™®é€šé€šçš„æ ‘å›æ¥ã€‚ä»–è¯´ï¼šâ€œæœ‰äº†ä¸Šä¸€æ¬¡ç»éªŒï¼Œå½“æˆ‘èµ°åˆ°å¤§åŠè·¯ç¨‹ï¼Œå·²ç»æ„Ÿåˆ°ç´¯äº†å´è¿˜ä¸¤æ‰‹ç©ºç©ºæ—¶ï¼Œæˆ‘è§‰å¾—è™½ç„¶æ ‘æ—é‡Œè¿˜æœ‰å¾ˆå¤šæ ‘ï¼Œä½†è¿™æ£µæ ‘è¿˜æ˜¯æŒºä¸é”™çš„ï¼Œä¾¿ç ä¸‹æ¥ï¼Œå…å¾—æœ€ååˆä»€ä¹ˆä¹Ÿå¸¦ä¸å‡ºæ¥ã€‚â€ è‹æ ¼æ‹‰åº•è¯´ï¼šâ€œè¿™å°±æ˜¯å©šå§»ã€‚â€ åˆæœ‰ä¸€å¤©æŸæ‹‰å›¾åˆé—®è€å¸ˆè‹æ ¼æ‹‰åº•ä»€ä¹ˆæ˜¯ç”Ÿæ´»ï¼Œè‹æ ¼æ‹‰åº•è¿˜æ˜¯å«ä»–åˆ°æ ‘æ—èµ°ä¸€æ¬¡ã€‚è¦æ±‚æ˜¯éšä¾¿èµ°ï¼Œåœ¨é€”ä¸­è¦å–ä¸€æ”¯æœ€å¥½çœ‹çš„èŠ±ã€‚æŸæ‹‰å›¾æœ‰äº†ä»¥å‰çš„æ•™è®­åˆå……æ»¡ä¿¡å¿ƒåœ°å‡ºå»è¿‡äº†ä¸‰å¤©ä¸‰å¤œï¼Œä»–ä¹Ÿæ²¡æœ‰å›æ¥ã€‚è‹æ ¼æ‹‰åº•åªå¥½èµ°è¿›æ ‘æ—é‡Œå»æ‰¾ä»–ï¼Œæœ€åå‘ç°æŸæ‹‰å›¾å·²åœ¨æ ‘æ—é‡Œå®‰è¥æ‰å¯¨ã€‚è‹æ ¼æ‹‰åº•é—®ä»–ï¼šâ€œä½ æ‰¾ç€æœ€å¥½çœ‹çš„èŠ±ä¹ˆï¼Ÿâ€ æŸæ‹‰å›¾æŒ‡ç€è¾¹ä¸Šçš„ä¸€æœµèŠ±è¯´ï¼šâ€œè¿™å°±æ˜¯æœ€å¥½çœ‹çš„èŠ±ã€‚â€ è‹æ ¼æ‹‰åº•é—®ï¼šâ€œä¸ºä»€ä¹ˆä¸æŠŠå®ƒå¸¦å‡ºå»å‘¢ï¼Ÿâ€ æŸæ‹‰å›¾å›ç­”è€å¸ˆï¼š â€œæˆ‘å¦‚æœæŠŠå®ƒæ‘˜ä¸‹æ¥ï¼Œå®ƒé©¬ä¸Šå°±æ¯èã€‚å³ä½¿æˆ‘ä¸æ‘˜å®ƒï¼Œå®ƒä¹Ÿè¿Ÿæ—©ä¼šæ¯ã€‚æ‰€ä»¥æˆ‘å°±åœ¨å®ƒè¿˜ç››å¼€çš„æ—¶å€™ï¼Œä½åœ¨å®ƒè¾¹ä¸Šã€‚ç­‰å®ƒå‡‹è°¢çš„æ—¶å€™ï¼Œå†æ‰¾ä¸‹ä¸€æœµã€‚è¿™å·²ç»æ˜¯æˆ‘æ‰¾ç€çš„ç¬¬äºŒæœµæœ€å¥½çœ‹çš„èŠ±äº†ã€‚â€ è‹æ ¼æ‹‰åº•è¯´ï¼š â€œä½ å·²ç»æ‡‚å¾—ç”Ÿæ´»çš„çœŸè°›äº†ã€‚â€ çˆ±æƒ…ç»™äººç»å†å’Œå›å¿†ï¼Œä¹‹åï¼Œå©šå§»é çš„æ˜¯æ˜æ™ºçš„å†³å®šå’Œå¥½å¥½çš„æŠŠæ¡ï¼Œç»è¿‡äº†è¿™äº›è€ƒéªŒï¼Œåˆ°æœ€åæ‰ä¼šæ˜ç™½ç”Ÿæ´»æ˜¯ä¸€ç§çæƒœå’Œå®ˆæŠ¤ã€‚ æŸæ‹‰å›¾éº¦ç©—é—®é¢˜çš„æ•°å­¦è§£ç­” ç°åœ¨æˆ‘ä»¬ç”¨æ•°å­¦çš„è§’åº¦æ¥è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚ å‡è®¾æˆ‘ä»¬ç¢°åˆ°çš„éº¦ç©—æœ‰ n ä¸ªï¼Œæˆ‘ä»¬ç”¨è¿™æ ·çš„ç­–ç•¥æ¥é€‰éº¦ç©—ï¼Œå‰ k ä¸ªï¼Œè®°ä½ä¸€ä¸ªæœ€å¤§çš„éº¦ç©—è®°ä¸º dï¼ˆå¯èƒ½æ˜¯é‡é‡ï¼Œä¹Ÿå¯èƒ½æ˜¯ä½“ç§¯ï¼‰ï¼Œç„¶å k + 1 ä¸ªå¼€å§‹ï¼Œåªè¦å¤§äº d çš„ï¼Œå°±é€‰æ‹©ï¼Œå¦åˆ™å°±ä¸é€‰æ‹©ã€‚ å¯¹äºæŸä¸ªå›ºå®šçš„ kï¼Œå¦‚æœæœ€å¤§çš„éº¦ç©—å‡ºç°åœ¨äº†ç¬¬ i ä¸ªä½ç½®ï¼ˆk &lt; i â‰¤ nï¼‰ï¼Œè¦æƒ³è®©ä»–æœ‰å¹¸æ­£å¥½è¢«é€‰ä¸­ï¼Œå°±å¿…é¡»å¾—æ»¡è¶³å‰ i - 1 ä¸ªéº¦ç©—ä¸­çš„æœ€å¥½çš„éº¦ç©—åœ¨å‰ k ä¸ªéº¦ç©—é‡Œï¼Œè¿™æœ‰ k / (i - 1) çš„å¯èƒ½ã€‚è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„ iï¼Œæˆ‘ä»¬ä¾¿å¾—åˆ°äº†å‰ k ä¸ªéº¦ç©—ä½œä¸ºå‚è€ƒï¼Œèƒ½é€‰ä¸­æœ€å¤§éº¦ç©—çš„æ€»æ¦‚ç‡ P(k)ï¼š è®¾ k / n = xï¼Œå¹¶ä¸”å‡è®¾ n å……åˆ†å¤§ï¼Œåˆ™ä¸Šè¿°å…¬å¼å¯ä»¥æ”¹ä¸ºï¼š å¯¹ xÂ·ln(x) æ±‚å¯¼ï¼Œå¹¶ä»¤è¿™ä¸ªå¯¼æ•°ä¸º 0ï¼Œå¯ä»¥è§£å‡º x çš„æœ€ä¼˜å€¼ï¼Œå®ƒå°±æ˜¯æ¬§æ‹‰ç ”ç©¶çš„ç¥ç§˜å¸¸æ•°çš„å€’æ•° 1 / e. æ‰€ä»¥ k = n / e. å¦‚æœä½ æƒ³æ‘˜å–æœ€å¤§çš„éº¦ç©—ï¼Œå‡è®¾æœ‰ n ä¸ªéº¦ç©—ï¼Œä½ åº”è¯¥å…ˆå°†å‰ n / e ä¸ªéº¦ç©—ä½œä¸ºå‚è€ƒï¼Œç„¶åå† k + 1 ä¸ªéº¦ç©—å¼€å§‹é€‰æ‹©æ¯”å‰é¢ k ä¸ªæœ€å¤§çš„éº¦ç©—å³å¯ã€‚ e = 2.718281828459 1 / e = 0.36787944117144 å…¶ä»–ä¾‹å­ä¸€ã€ä¸€æ¥¼åˆ°åæ¥¼çš„æ¯å±‚ç”µæ¢¯é—¨å£éƒ½æ”¾ç€ä¸€é¢—é’»çŸ³ï¼Œé’»çŸ³å¤§å°ä¸ä¸€ã€‚ä½ ä¹˜åç”µæ¢¯ä»ä¸€æ¥¼åˆ°åæ¥¼ï¼Œæ¯å±‚æ¥¼ç”µæ¢¯é—¨éƒ½ä¼šæ‰“å¼€ä¸€æ¬¡ï¼Œåªèƒ½æ‹¿ä¸€æ¬¡é’»çŸ³ï¼Œé—®æ€æ ·æ‰èƒ½æ‹¿åˆ°æœ€å¤§çš„ä¸€é¢—ã€‚ é¦–å…ˆï¼Œè¿™ä¸ªé¢˜ç›®è¯´çš„ï¼Œå¹¶ä¸èƒ½å®Œå…¨æ‹¿åˆ°æœ€å¤§çš„é’»çŸ³ã€‚ä½†å¯ä»¥ä¿è¯æ‹¿åˆ°æœ€å¤§é’»çŸ³çš„æ¦‚ç‡æœ€å¤§ã€‚10 / e = 3.67ï¼Œå‘ä¸Šå–æ•´å¾— 4ã€‚å‰å››å±‚çš†ä¸å–ï¼Œåªè®°ä¸‹æœ€å¤§çš„ã€‚åé¢é‡åˆ°çš„ï¼Œåªè¦æ¯”å‰é¢æœ€å¤§çš„è¿˜å¤§ï¼Œå–ä¹‹å³å¯ã€‚ äºŒã€ç§˜ä¹¦é—®é¢˜ã€‚åœ¨æœºç‡åŠåšå¼ˆè®ºä¸Šï¼Œç§˜ä¹¦é—®é¢˜ï¼ˆç±»ä¼¼åç§°æœ‰ç›¸äº²é—®é¢˜ã€æ­¢æ­¥é—®é¢˜ã€è§å¥½å°±æ”¶é—®é¢˜ã€è‹ä¸¹çš„å«å¦†é—®é¢˜ã€æŒ‘å‰”çš„æ±‚å©šè€…é—®é¢˜ç­‰) å†…å®¹æ˜¯è¿™æ ·çš„ï¼š è¦è˜è¯·ä¸€åç§˜ä¹¦ï¼Œæœ‰ n äººæ¥é¢è¯•ã€‚æ¯æ¬¡é¢è¯•ä¸€äººï¼Œé¢è¯•è¿‡åä¾¿è¦å³æ—¶å†³å®šè˜ä¸è˜ä»–ï¼Œå¦‚æœå½“æ—¶å†³å®šä¸è˜ä»–ï¼Œä»–ä¾¿ä¸ä¼šå›æ¥ã€‚é¢è¯•æ—¶æ€»èƒ½æ¸…æ¥šäº†è§£æ±‚èŒè€…çš„é€‚åˆç¨‹åº¦ï¼Œå¹¶èƒ½å’Œä¹‹å‰çš„æ¯ä¸ªäººä½œæ¯”è¾ƒã€‚é—®å‡­ä»€ä¹ˆç­–ç•¥ï¼Œæ‰ä½¿é€‰å¾—åˆ°æœ€é€‚åˆæ‹…ä»»ç§˜ä¹¦çš„äººçš„æœºç‡æœ€å¤§ï¼Ÿ References éº¦ç©—ç†è®ºï¼Œåˆåâ€œç§˜ä¹¦é—®é¢˜â€ (1/e å¤„ä¸ºæœ€ä¼˜åˆ†å‰²ç‚¹), http://geek.csdn.net/news/detail/231497","tags":[]},{"title":"Why Functional Programming?","date":"2017-08-17T13:15:35.000Z","path":"2017/08/17/Why-functional-programming/","text":"Why Static Type? æ€§èƒ½ - æ–¹æ³•è°ƒç”¨é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸ºä¸éœ€è¦åœ¨è¿è¡Œæ—¶æ‰æ¥åˆ¤æ–­è°ƒç”¨çš„æ˜¯å“ªä¸ªæ–¹æ³•ã€‚ å¯é æ€§ - ç¼–è¯‘å™¨éªŒè¯äº†ç¨‹åºçš„æ­£ç¡®æ€§ï¼Œå› è€Œè¿è¡Œæ—¶å´©æºƒçš„æ¦‚ç‡æ›´ä½ã€‚ å¯ç»´æŠ¤æ€§ - é™Œç”Ÿä»£ç æ›´å®¹æ˜“ç»´æŠ¤ï¼Œå› ä¸ºä½ å¯ä»¥çœ‹åˆ°ä»£ç ä¸­ç”¨åˆ°çš„å¯¹è±¡çš„ç±»å‹ã€‚ å·¥å…·æ”¯æŒ - é™æ€ç±»å‹ä½¿ IDE èƒ½æä¾›å¯é çš„é‡æ„ã€ç²¾ç¡®çš„ä»£ç è¡¥å…¨ä»¥åŠå…¶ä»–ç‰¹æ€§ã€‚ Benefit of Functional Programming å¤´ç­‰å‡½æ•° - æŠŠå‡½æ•°ï¼ˆä¸€å°æ®µè¡Œä¸ºï¼‰å½“ä½œå€¼ä½¿ç”¨ï¼Œå¯ä»¥ç”¨å˜é‡ä¿å­˜å®ƒï¼ŒæŠŠå®ƒå½“ä½œå‚æ•°ä¼ é€’ï¼Œæˆ–è€…å½“ä½œå…¶ä»–å‡½æ•°çš„è¿”å›å€¼ã€‚ ä¸å¯å˜æ€§ - ä½¿ç”¨ä¸å¯å˜å¯¹è±¡ï¼Œè¿™ä¿è¯äº†å®ƒä»¬çš„çŠ¶æ€åœ¨å…¶åˆ›å»ºä¹‹åä¸èƒ½å†å˜åŒ–ã€‚ æ— å‰¯ä½œç”¨ - ä½¿ç”¨çš„æ˜¯çº¯å‡½æ•°ã€‚æ­¤ç±»å‡½æ•°åœ¨è¾“å…¥ç›¸åŒæ—¶ä¼šäº§ç”ŸåŒæ ·çš„ç»“æœï¼Œå¹¶ä¸”ä¸ä¼šä¿®æ”¹å…¶ä»–å¯¹è±¡çš„çŠ¶æ€ï¼Œä¹Ÿä¸ä¼šå’Œå¤–é¢çš„ä¸–ç•Œäº¤äº’ã€‚ ç®€æ´ å‡½æ•°å¼é£æ ¼çš„ä»£ç  æ¯”ç›¸åº”çš„å‘½ä»¤å¼é£æ ¼çš„ä»£ç æ›´ä¼˜é›…ã€æ›´ç®€ç»ƒï¼Œå› ä¸ºæŠŠå‡½æ•°å½“ä½œå€¼å¯ä»¥è®©ä½ è·å¾—æ›´å¼ºå¤§çš„æŠ½è±¡èƒ½åŠ›ï¼Œä»è€Œé¿å…é‡å¤ä»£ç ã€‚ å‡è®¾ä½ æœ‰ä¸¤æ®µç±»ä¼¼çš„ä»£ç ï¼Œå®ç°ç›¸ä¼¼çš„ä»»åŠ¡ä½†å…·ä½“ç»†èŠ‚ç•¥æœ‰ä¸åŒï¼Œå¯ä»¥è½»æ˜“åœ°å°†è¿™æ®µé€»è¾‘ä¸­å…¬å…±çš„éƒ¨åˆ†æå–åˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶ä»–ä¸åŒçš„éƒ¨åˆ†ä½œä¸ºå‚æ•°ä¼ é€’ç»™å®ƒã€‚è¿™äº›å‚æ•°æœ¬èº«ä¹Ÿæ˜¯å‡½æ•°ï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨ä¸€ç§ç®€æ´çš„è¯­æ³•æ¥è¡¨ç¤ºè¿™äº›åŒ¿åå‡½æ•°ï¼Œè¢«ç§°ä½œ lambda è¡¨è¾¾å¼ã€‚ å¤šçº¿ç¨‹å®‰å…¨ å¤šçº¿ç¨‹ç¨‹åºä¸­æœ€å¤§çš„é”™è¯¯æ¥æºä¹‹ä¸€å°±æ˜¯ï¼Œåœ¨æ²¡æœ‰é‡‡ç”¨é€‚å½“åŒæ­¥æœºåˆ¶çš„æƒ…å†µä¸‹ï¼Œåœ¨ä¸åŒçš„çº¿ç¨‹ä¸Šä¿®æ”¹åŒä¸€ä»½æ•°æ®ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ä¸å¯å˜æ•°æ®ç»“æ„å’Œçº¯å‡½æ•°ï¼Œå°±èƒ½ä¿è¯è¿™æ ·ä¸å®‰å…¨çš„ä¿®æ”¹æ ¹æœ¬ä¸ä¼šå‘ç”Ÿï¼Œä¹Ÿå°±ä¸éœ€è¦è€ƒè™‘ä¸ºå…¶è®¾è®¡å¤æ‚çš„åŒæ­¥æ–¹æ¡ˆã€‚ æµ‹è¯•æ›´åŠ å®¹æ˜“ æ²¡æœ‰å‰¯ä½œç”¨çš„å‡½æ•°å¯ä»¥ç‹¬ç«‹åœ°è¿›è¡Œæµ‹è¯•ï¼Œå› ä¸ºä¸éœ€è¦å†™å¤§é‡çš„è®¾ç½®ä»£ç æ¥æ„é€ å®ƒä»¬æ‰€ä¾èµ–çš„æ•´ä¸ªç¯å¢ƒã€‚ Functional programming, views a program as a mathematical function which is evaluated to produce a result value. That function may call upon nested functions, which in turn may call upon more nested functions. A nested function evaluates to produce a result. From there, that result is passed on to the enclosing function, which uses the nested function values to calculate its own return value. To enable functions to easily pass data to and from other functions, functional programming languages typically define data structures in the most generic possible way, as a collection of (any) things. They also allow functions to be passed to other functions as if they were data parameters. A function in this paradigm is not allowed to produce any side effects such as modifying a global variable that maintains state information. Instead, it is only allowed to receive parameters and perform some operations on them in order to produce its return value. Executing a functional program involves evaluating the outermost function, which in turn causes evaluation of all the nested functions, recursively down to the most basic functions that have no nested functions. Why is functional programming a big deal? Clarity Programming without side effects creates code that is easier to follow - a function is completely described by what goes in and what comes out. A function that produces the right answer today will produce the right answer tomorrow. This creates code that is easier to debug, easier to test, and easier to re-use. Brevity In functional languages, data is implicitly passed from a nested function to its parent function, via a general-purpose collection data type. This makes functional programs much more compact than those of other paradigms, which require substantial â€œhousekeepingâ€ code to pass data from one function to the next. Efficiency Because functions do not have side effects, operations can be re-ordered or performed in parallel in order to optimize performance, or can be skipped entirely if their result is not used by any other function. References Kotlin åˆä½“éªŒï¼šä¸»è¦ç‰¹å¾ä¸åº”ç”¨, http://geek.csdn.net/news/detail/231497 The Rise and Fall of Scala, https://dzone.com/articles/the-rise-and-fall-of-scala","tags":[]},{"title":"è‚¡å¸‚æŠ•èµ„ä¸‰è¦ç´ ","date":"2017-08-04T12:16:44.000Z","path":"2017/08/04/è‚¡å¸‚æŠ•èµ„ä¸‰è¦ç´ /","text":"ä¸è¦å€Ÿé’±æŠ•èµ„ åªä¹°æœ‰é•¿æœŸæŠ•èµ„ä»·å€¼çš„å…¬å¸ ä¸è¦ç”¨çŸ­æœŸçš„é’±å»ä¹°é•¿çº¿çš„è‚¡ç¥¨","tags":[]},{"title":"Customisation of Filco Majestouch 2 Mechanical Keyboard","date":"2017-08-01T16:29:45.000Z","path":"2017/08/02/Customisation-of-Filco-Majestouch-2-Mechanical-Keyboard/","text":"A Filco Majestouch 2 Tenkeyless Mechanical Keyboard, with Cherry Brown switches. It has been replaced with GMK Honeywell keycaps from Originative Co. (https://originative.co/products/honeywell) soon after it bought. Replace Filco Majestouch 2â€™s with GMK Honeywell keycaps. After joined Massdrop Lambo 80% Anodized Aluminum Case for Filco 87 TKL campaign (https://www.massdrop.com/buy/lambo-80-anodized-aluminum-case-for-filco-87-tkl) After a few months waiting, case delivered in, shipped from USA. Get hands warmed up and dirty. Now, show time.","tags":[]},{"title":"æç®€ä¹‹é“ - The interface äººä¸æœºå™¨çš„æ€æƒ³äº¤æµ","date":"2017-07-16T07:50:43.000Z","path":"2017/07/16/æç®€ä¹‹é“-The-interface-äººä¸æœºå™¨çš„æ€æƒ³äº¤æµ/","text":"Why 60%? GH60 GH60 å¯ç¼–ç¨‹é”®ç›˜ (http://blog.komar.be/projects/gh60-programmable-keyboard/), itâ€™s Poker 2 é”®ç›˜çš„ rip-offã€‚å¼€æ”¾å¼å…¬æ¿è®¾è®¡ã€‚ç”µè·¯æ¿ä¸­å›½åˆ¶é€ ã€‚ä¸‰å‘¨å‰ï¼Œåœ¨ AliExpress (https://www.aliexpress.com/item/Customized-DIY-GH60-Case-Shell-PCB-Plate-Switches-LED-Kit-60-Mechanical-Keyboard-Satan-Poker2-GH/32651474350.html) ä¸‹çš„è®¢å•ã€‚ Cherry MX Switch å¾·å›½ Cherry å·¥å‚çš„èŒ¶è½´ã€‚æ•²èµ·é”®ç›˜æ¥ææœ‰æ®µè½ï¼Œå±‚æ¬¡æ„Ÿã€‚ iQunix Lambo æ‹†æ‰åŸè£…çš„å¡‘æ–™é”®ç›˜å£³ã€‚æ¢è£… iQunix Lambo (https://www.aliexpress.com/item/Iqunix-lambo-60-mechanical-keyboard-anode-alumina-shell-base-gh60-poker2/32677061753.html) é“åˆ¶å¤–å£³ã€‚ GMK 3Run Keycap Set æœ€åå†è£…ä¸Šå¾·å›½ GMK å·¥å‚é€ çš„ 3Run ABS keycaps å (https://www.massdrop.com/buy/gmk-3run-keycap-set) ï¼Œä¸€ä¸ªæ‹¥æœ‰è‡ªå·± signatureï¼Œä½“ç°ä¸ªæ€§ï¼Œå“å‘³çš„æœºæ¢°é”®ç›˜è¯ç”Ÿäº†ã€‚ æç®€ä¹‹é“ / Simplicity","tags":[]},{"title":"IT çš„ç»ˆç‚¹æ˜¯è‰ºæœ¯ - The end of IT is Art","date":"2017-07-02T08:26:23.000Z","path":"2017/07/02/The-end-of-IT-is-Art/","text":"Modern, postmodern, and contemporary IT and its history.","tags":[]},{"title":"Ant Colony Optimization (ACO)","date":"2017-06-30T12:46:52.000Z","path":"2017/06/30/Ant-Colony-Optimization-ACO/","text":"Ant Colony Optimization (ACO) for the the Traveling Salesman Problem (TSP). In computer science and operations research, the ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems which can be reduced to finding good paths through graphs. èšç¾¤ç®—æ³•æ˜¯ä¸€ç§ç”¨æ¥å¯»æ‰¾ä¼˜åŒ–è·¯å¾„çš„æ¦‚ç‡å‹ç®—æ³•ã€‚å®ƒç”± Marco Dorigo äº 1992 å¹´åœ¨ä»–çš„åšå£«è®ºæ–‡ä¸­æå‡ºï¼Œå…¶çµæ„Ÿæ¥æºäºèš‚èšåœ¨å¯»æ‰¾é£Ÿç‰©è¿‡ç¨‹ä¸­å‘ç°è·¯å¾„çš„è¡Œä¸ºã€‚è¿™ç§ç®—æ³•å…·æœ‰åˆ†å¸ƒè®¡ç®—ã€ä¿¡æ¯æ­£åé¦ˆå’Œå¯å‘å¼æœç´¢çš„ç‰¹å¾ï¼Œæœ¬è´¨ä¸Šæ˜¯è¿›åŒ–ç®—æ³•ä¸­çš„ä¸€ç§å¯å‘å¼å…¨å±€ä¼˜åŒ–ç®—æ³•ã€‚ èšç¾¤ç³»ç»Ÿ (Ant System æˆ– Ant Colony System) æ˜¯ç”±æ„å¤§åˆ©å­¦è€… Dorigoã€Maniezzo ç­‰äººäº 20 ä¸–çºª 90 å¹´ä»£é¦–å…ˆæå‡ºæ¥çš„ã€‚ä»–ä»¬åœ¨ç ”ç©¶èš‚èšè§…é£Ÿçš„è¿‡ç¨‹ä¸­ï¼Œå‘ç°å•ä¸ªèš‚èšçš„è¡Œä¸ºæ¯”è¾ƒç®€å•ï¼Œä½†æ˜¯èšç¾¤æ•´ä½“å´å¯ä»¥ä½“ç°ä¸€äº›æ™ºèƒ½çš„è¡Œä¸ºã€‚ä¾‹å¦‚èšç¾¤å¯ä»¥åœ¨ä¸åŒçš„ç¯å¢ƒä¸‹ï¼Œå¯»æ‰¾æœ€çŸ­åˆ°è¾¾é£Ÿç‰©æºçš„è·¯å¾„ã€‚è¿™æ˜¯å› ä¸ºèšç¾¤å†…çš„èš‚èšå¯ä»¥é€šè¿‡æŸç§ä¿¡æ¯æœºåˆ¶å®ç°ä¿¡æ¯çš„ä¼ é€’ã€‚ååˆç»è¿›ä¸€æ­¥ç ”ç©¶å‘ç°ï¼Œèš‚èšä¼šåœ¨å…¶ç»è¿‡çš„è·¯å¾„ä¸Šé‡Šæ”¾ä¸€ç§å¯ä»¥ç§°ä¹‹ä¸ºâ€œä¿¡æ¯ç´ â€çš„ç‰©è´¨ï¼Œèšç¾¤å†…çš„èš‚èšå¯¹â€œä¿¡æ¯ç´ â€å…·æœ‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œå®ƒä»¬ä¼šæ²¿ç€â€œä¿¡æ¯ç´ â€æµ“åº¦è¾ƒé«˜è·¯å¾„è¡Œèµ°ï¼Œè€Œæ¯åªè·¯è¿‡çš„èš‚èšéƒ½ä¼šåœ¨è·¯ä¸Šç•™ä¸‹â€œä¿¡æ¯ç´ â€ï¼Œè¿™å°±å½¢æˆä¸€ç§ç±»ä¼¼æ­£åé¦ˆçš„æœºåˆ¶ï¼Œè¿™æ ·ç»è¿‡ä¸€æ®µæ—¶é—´åï¼Œæ•´ä¸ªèšç¾¤å°±ä¼šæ²¿ç€æœ€çŸ­è·¯å¾„åˆ°è¾¾é£Ÿç‰©æºäº†ã€‚ å°†èšç¾¤ç®—æ³•åº”ç”¨äºè§£å†³ä¼˜åŒ–é—®é¢˜çš„åŸºæœ¬æ€è·¯ä¸ºï¼šç”¨èš‚èšçš„è¡Œèµ°è·¯å¾„è¡¨ç¤ºå¾…ä¼˜åŒ–é—®é¢˜çš„å¯è¡Œè§£ï¼Œæ•´ä¸ªèš‚èšç¾¤ä½“çš„æ‰€æœ‰è·¯å¾„æ„æˆå¾…ä¼˜åŒ–é—®é¢˜çš„è§£ç©ºé—´ã€‚è·¯å¾„è¾ƒçŸ­çš„èš‚èšé‡Šæ”¾çš„ä¿¡æ¯ç´ é‡è¾ƒå¤šï¼Œéšç€æ—¶é—´çš„æ¨è¿›ï¼Œè¾ƒçŸ­çš„è·¯å¾„ä¸Šç´¯ç§¯çš„ä¿¡æ¯ç´ æµ“åº¦é€æ¸å¢é«˜ï¼Œé€‰æ‹©è¯¥è·¯å¾„çš„èš‚èšä¸ªæ•°ä¹Ÿæ„ˆæ¥æ„ˆå¤šã€‚æœ€ç»ˆï¼Œæ•´ä¸ªèš‚èšä¼šåœ¨æ­£åé¦ˆçš„ä½œç”¨ä¸‹é›†ä¸­åˆ°æœ€ä½³çš„è·¯å¾„ä¸Šï¼Œæ­¤æ—¶å¯¹åº”çš„ä¾¿æ˜¯å¾…ä¼˜åŒ–é—®é¢˜çš„æœ€ä¼˜è§£ã€‚ èš‚èšæ‰¾åˆ°æœ€çŸ­è·¯å¾„è¦å½’åŠŸäºä¿¡æ¯ç´ å’Œç¯å¢ƒï¼Œå‡è®¾æœ‰ä¸¤æ¡è·¯å¯ä»èšçªé€šå‘é£Ÿç‰©ï¼Œå¼€å§‹æ—¶ä¸¤æ¡è·¯ä¸Šçš„èš‚èšæ•°é‡å·®ä¸å¤šï¼šå½“èš‚èšåˆ°è¾¾ç»ˆç‚¹ä¹‹åä¼šç«‹å³è¿”å›ï¼Œè·ç¦»çŸ­çš„è·¯ä¸Šçš„èš‚èšå¾€è¿”ä¸€æ¬¡æ—¶é—´çŸ­ï¼Œé‡å¤é¢‘ç‡å¿«ï¼Œåœ¨å•ä½æ—¶é—´é‡Œå¾€è¿”èš‚èšçš„æ•°ç›®å°±å¤šï¼Œç•™ä¸‹çš„ä¿¡æ¯ç´ ä¹Ÿå¤šï¼Œä¼šå¸å¼•æ›´å¤šèš‚èšè¿‡æ¥ï¼Œä¼šç•™ä¸‹æ›´å¤šä¿¡æ¯ç´ ã€‚è€Œè·ç¦»é•¿çš„è·¯æ­£ç›¸åï¼Œå› æ­¤è¶Šæ¥è¶Šå¤šçš„èš‚èšèšé›†åˆ°æœ€çŸ­è·¯å¾„ä¸Šæ¥ã€‚ èš‚èšå…·æœ‰çš„æ™ºèƒ½è¡Œä¸ºå¾—ç›Šäºå…¶ç®€å•è¡Œä¸ºè§„åˆ™ï¼Œè¯¥è§„åˆ™è®©å…¶å…·æœ‰å¤šæ ·æ€§å’Œæ­£åé¦ˆã€‚åœ¨è§…é£Ÿæ—¶ï¼Œå¤šæ ·æ€§ä½¿èš‚èšä¸ä¼šèµ°è¿›æ­»èƒ¡åŒè€Œæ— é™å¾ªç¯ï¼Œæ˜¯ä¸€ç§åˆ›æ–°èƒ½åŠ›ï¼›æ­£åé¦ˆä½¿ä¼˜è‰¯ä¿¡æ¯ä¿å­˜ä¸‹æ¥ï¼Œæ˜¯ä¸€ç§å­¦ä¹ å¼ºåŒ–èƒ½åŠ›ã€‚ä¸¤è€…çš„å·§å¦™ç»“åˆä½¿æ™ºèƒ½è¡Œä¸ºæ¶Œç°ï¼Œå¦‚æœå¤šæ ·æ€§è¿‡å‰©ï¼Œç³»ç»Ÿè¿‡äºæ´»è·ƒï¼Œä¼šå¯¼è‡´è¿‡å¤šçš„éšæœºè¿åŠ¨ï¼Œé™·å…¥æ··æ²ŒçŠ¶æ€ï¼›å¦‚æœå¤šæ ·æ€§ä¸å¤Ÿï¼Œæ­£åé¦ˆè¿‡å¼ºï¼Œä¼šå¯¼è‡´åƒµåŒ–ï¼Œå½“ç¯å¢ƒå˜åŒ–æ—¶èšç¾¤ä¸èƒ½ç›¸åº”è°ƒæ•´ã€‚ ä¸å…¶ä»–ä¼˜åŒ–ç®—æ³•ç›¸æ¯”ï¼Œèšç¾¤ç®—æ³•å…·æœ‰ä»¥ä¸‹å‡ ä¸ªç‰¹ç‚¹ï¼š (1) é‡‡ç”¨æ­£åé¦ˆæœºåˆ¶ï¼Œä½¿å¾—æœç´¢è¿‡ç¨‹ä¸æ–­æ”¶æ•›ï¼Œæœ€ç»ˆé€¼è¿‘æœ€ä¼˜è§£ã€‚(2) æ¯ä¸ªä¸ªä½“å¯ä»¥é€šè¿‡é‡Šæ”¾ä¿¡æ¯ç´ æ¥æ”¹å˜å‘¨å›´çš„ç¯å¢ƒï¼Œä¸”æ¯ä¸ªä¸ªä½“èƒ½å¤Ÿæ„ŸçŸ¥å‘¨å›´ç¯å¢ƒçš„å®æ—¶å˜åŒ–ï¼Œä¸ªä½“é—´é€šè¿‡ç¯å¢ƒè¿›è¡Œé—´æ¥åœ°é€šè®¯ã€‚(3) æœç´¢è¿‡ç¨‹é‡‡ç”¨åˆ†å¸ƒå¼è®¡ç®—æ–¹å¼ï¼Œå¤šä¸ªä¸ªä½“åŒæ—¶è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œå¤§å¤§æé«˜äº†ç®—æ³•çš„è®¡ç®—èƒ½åŠ›å’Œè¿è¡Œæ•ˆç‡ã€‚(4) å¯å‘å¼çš„æ¦‚ç‡æœç´¢æ–¹å¼ä¸å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œæ˜“äºå¯»æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚ è¯¥ç®—æ³•åº”ç”¨äºå…¶ä»–ç»„åˆä¼˜åŒ–é—®é¢˜ï¼Œå¦‚æ—…è¡Œå•†é—®é¢˜ã€æŒ‡æ´¾é—®é¢˜ã€Job Shop è°ƒåº¦é—®é¢˜ã€è½¦è¾†è·¯ç”±é—®é¢˜ã€å›¾ç€è‰²é—®é¢˜å’Œç½‘ç»œè·¯ç”±é—®é¢˜ç­‰ã€‚æœ€è¿‘å‡ å¹´ï¼Œè¯¥ç®—æ³•åœ¨ç½‘ç»œè·¯ç”±ä¸­çš„åº”ç”¨å—åˆ°è¶Šæ¥è¶Šå¤šå­¦è€…çš„å…³æ³¨ï¼Œå¹¶æå‡ºäº†ä¸€äº›æ–°çš„åŸºäºèš‚èšç®—æ³•çš„è·¯ç”±ç®—æ³•ã€‚åŒä¼ ç»Ÿçš„è·¯ç”±ç®—æ³•ç›¸æ¯”è¾ƒï¼Œè¯¥ç®—æ³•åœ¨ç½‘ç»œè·¯ç”±ä¸­å…·æœ‰ä¿¡æ¯åˆ†å¸ƒå¼æ€§ã€åŠ¨æ€æ€§ã€éšæœºæ€§å’Œå¼‚æ­¥æ€§ç­‰ç‰¹ç‚¹ï¼Œè€Œè¿™äº›ç‰¹ç‚¹æ­£å¥½èƒ½æ»¡è¶³ç½‘ç»œè·¯ç”±çš„éœ€è¦ã€‚ VisualizationA visual demo of Ant Colony Optimisation written in Javascript (ES6): Another visual demo of Ant Colony Optimisation: References Ant colony optimization algorithms, https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms Visualisation of Ant Colony Optimisation, http://poolik.github.io/visual-aco/#/visualisation A visual demo of Ant Colony Optimisation applied to TSP written in Javascript, http://gordyd.github.io/js-aco/ ç™¾åº¦ç™¾ç§‘ - èšç¾¤ç®—æ³•, https://wapbaike.baidu.com/item/%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95","tags":[]},{"title":"Influence Without Authority","date":"2017-03-12T11:31:23.000Z","path":"2017/03/12/Influence-Without-Authority/","text":"The Psychology of Persuasion.","tags":[]},{"title":"Spring Data - powerful and succinct abstraction","date":"2017-03-03T23:05:30.000Z","path":"2017/03/04/Spring-Data-powerful-and-succinct-abstraction/","text":"Database tier definition Database tables, indexes and foreign keys defined in Liquibase configuration: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122databaseChangeLog: - changeSet: id: 1 author: Terrence Miao changes: - createTable: tableName: draft_order columns: - column: name: id type: int autoIncrement: true constraints: primaryKey: true nullable: false - column: name: c_number type: varchar(32) constraints: nullable: false - column: name: source_time_in_ms type: bigint constraints: nullable: false - column: name: source_item_id type: varchar(255) constraints: nullable: false - column: name: shipment type: json constraints: nullable: false - column: name: shipment_id type: varchar(255) constraints: nullable: true - column: name: quantity type: int constraints: nullable: false - column: name: source_system type: varchar(255) constraints: nullable: false - column: name: status type: varchar(32) constraints: nullable: false - createIndex: columns: - column: name: source_item_id indexName: idx_source_item_id tableName: draft_order unique: false - createIndex: columns: - column: name: c_number - column: name: source_item_id indexName: idx_c_number_source_item_id tableName: draft_order unique: true - createTable: tableName: draft_order_combined columns: - column: name: id type: int autoIncrement: true constraints: primaryKey: true nullable: false - column: name: combined_id type: varchar(64) constraints: nullable: false - column: name: draft_order_id type: int constraints: nullable: false - addForeignKeyConstraint: baseColumnNames: draft_order_id baseTableName: draft_order_combined constraintName: fk_draft_order_combined_draft_order onDelete: CASCADE onUpdate: RESTRICT referencedColumnNames: id referencedTableName: draft_order - changeSet: id: 2 author: Terrence Miao changes: - addColumn: columns: - column: # For MySQL 5.7.x above, the first TIMESTAMP column in the table gets current timestamp as the default value, likely. So # if an INSERT or UPDATE without supplying a value, the column will get the current timestamp. Any subsequent TIMESTAMP # columns should have a default value explicitly defined. If you have two TIMESTAMP columns and if you don't specify a # default value for the second column, you will get this error while trying to create the table: # ERROR 1067 (42000): Invalid default value for 'COLUMN_NAME' name: date_created type: timestamp(3) constraints: nullable: false - column: name: date_updated type: timestamp(3) defaultValueComputed: LOCALTIMESTAMP(3) constraints: nullable: false tableName: draft_order DAO definition Draft Order 1234567891011121314151617181920212223242526272829303132@Entity@Table(name = \"draft_order\")public class DraftOrder implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Integer id; @Column(name = \"c_number\") private String cNumber; @Column(name = \"source_time_in_ms\") private Long sourceTimeInMs; @Column(name = \"source_item_id\") private String sourceItemId; @Column(name = \"shipment\", columnDefinition = \"json\") private String shipment; @Column(name = \"shipment_id\") private String shipmentId; @Column(name = \"quantity\") private Integer quantity; @Column(name = \"source_system\") private String sourceSystem; @Column(name = \"status\") private String status;&#125; Draft Order Combined 123456789101112131415@Entity@Table(name = \"draft_order_combined\")public class DraftOrderCombined implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Integer id; @Column(name = \"combined_id\") private String combinedId; @OneToOne(cascade = CascadeType.ALL) @JoinColumn(name = \"draft_order_id\") private DraftOrder draftOrder;&#125; An middle Aggregation Object 123456789101112public class CombinedIdSourceTimeInMs &#123; private Long counter; private String combinedId; private Long sourceTimeInMs; public CombinedIdSourceTimeInMs(Long counter, String combinedId, Long sourceTimeInMs) &#123; this.counter = counter; this.combinedId = combinedId; this.sourceTimeInMs = sourceTimeInMs; &#125;&#125; CRUD Repository definition DraftOrderRepository 1234567891011121314public interface DraftOrderRepository extends CrudRepository&lt;DraftOrder, Integer&gt; &#123; List&lt;DraftOrder&gt; findByCNumberAndStatusOrderBySourceTimeInMsDesc(String cNumber, String status, Pageable pageable); List&lt;DraftOrder&gt; findByCNumberAndSourceItemIdIn(String cNumber, List&lt;String&gt; sourceItemIds); DraftOrder findByCNumberAndSourceItemId(String cNumber, String sourceItemId); List&lt;DraftOrder&gt; findByShipmentIdInAndStatusAndSourceSystem(List&lt;String&gt; shipmentIds, String status, String sourceSystem); List&lt;DraftOrder&gt; findByCNumberAndId(String cNumber, Integer id); Long countByCNumberAndStatus(String cNumber, String status);&#125; DraftOrderCombinedRepository 12345678910111213141516171819public interface DraftOrderCombinedRepository extends CrudRepository&lt;DraftOrderCombined, Integer&gt; &#123; String FIND_QUERY = \"SELECT new org.paradise.data.dao.CombinedIdSourceTimeInMs\" + \"(count(doc) as counter, doc.combinedId as combinedId, min(doc.draftOrder.sourceTimeInMs) as sourceTimeInMs) \" + \" FROM DraftOrderCombined doc WHERE doc.draftOrder.cNumber = :cNumber AND doc.draftOrder.status = :status \" + \" GROUP BY combinedId \" + \" ORDER BY sourceTimeInMs DESC\"; String COUNT_QUERY = \"SELECT count(1) FROM \" + \"(SELECT count(1) FROM DraftOrderCombined doc WHERE doc.draftOrder.cNumber = :cNumber AND doc.draftOrder.status = :status\" + \" GROUP BY doc.combinedId)\"; @Query(value = FIND_QUERY, countQuery = COUNT_QUERY) List&lt;CombinedIdSourceTimeInMs&gt; countPerCombinedIdAndSourceTimeInMs(@Param(\"cNumber\") String cNumber, @Param(\"status\") String status, Pageable pageable); List&lt;DraftOrderCombined&gt; findByCombinedIdOrderByDraftOrderDaoSourceTimeInMsDesc(String combinedId);&#125; References Spring Data JPA Reference Documentation, https://docs.spring.io/spring-data/jpa/docs/current/reference/html/","tags":[]},{"title":"SQL script generates random data and insert into MySQL database","date":"2017-03-03T12:02:17.000Z","path":"2017/03/03/SQL-script-generates-random-data-and-insert-into-MySQL-database/","text":"1DROP PROCEDURE InsertRandomRecords; 12345678910111213141516DELIMITER $$CREATE PROCEDURE InsertRandomRecords(IN NumRows INT) BEGIN DECLARE i INT; SET i = 1; START TRANSACTION; WHILE i &lt;= NumRows DO INSERT INTO draftorders.draft_order (c_number, source_time_in_ms, source_item_id, shipment, shipment_id, quantity, source_system, status) VALUES ('C01234567890', RAND()*1000000000, CONCAT('randomSourceRef-', UUID_SHORT()), '&#123;\"to\": &#123;\"name\": \"T T\", \"lines\": [\"Lvl 100\", \"123 smith st\"], \"phone\": \"0356567567\", \"state\": \"VIC\", \"suburb\": \"Greensborough\", \"postcode\": \"3088\", \"business_name\": \"In debt\"&#125;, \"from\": &#123;\"name\": \"Carl Block\", \"lines\": [\"1341 Dandenong Road\"], \"state\": \"VIC\", \"suburb\": \"Geelong\", \"postcode\": \"3220\"&#125;, \"items\": [&#123;\"width\": \"10\", \"height\": \"10\", \"length\": \"10\", \"weight\": \"10\", \"product_id\": \"3D85\", \"item_reference\": \"blocked\", \"authority_to_leave\": true, \"allow_partial_delivery\": true, \"contains_dangerous_goods\": true&#125;], \"shipment_reference\": \"My second shipment ref\", \"customer_reference_1\": \"cr1234\", \"customer_reference_2\": \"cr5678\"&#125;', UUID(), 1, 'EBAY', ELT(1 + FLOOR(RAND()*3), 'DRAFT', 'READY_TO_SHIP', 'SHIPPED')); SET i = i + 1; END WHILE; COMMIT; END$$DELIMITER ; To generate 1,000,000 draft orders: 1CALL InsertRandomRecords(1000000);","tags":[]},{"title":"Set up and run AWS Lambda 'hello' function with serverless","date":"2017-02-26T08:00:26.000Z","path":"2017/02/26/Set-up-and-run-AWS-Lambda-hello-function-with-serverless/","text":"serverlessWith latest Node.js 6.x.x installed, then install serverless globally: 1$ npm install serverless -g AWS LambdaCreate a AWS Lambda skeleton project with serverless: 1234567891011121314$ mkdir serverless-example &amp;&amp; cd $_$ sls create -t aws-nodejsServerless: Generating boilerplate... _______ __| _ .-----.----.--.--.-----.----| .-----.-----.-----.| |___| -__| _| | | -__| _| | -__|__ --|__ --||____ |_____|__| \\___/|_____|__| |__|_____|_____|_____|| | | The Serverless Application Framework| | serverless.com, v1.7.0 -------'Serverless: Successfully generated boilerplate for template: \"aws-nodejs\"Serverless: NOTE: Please update the \"service\" property in serverless.yml with your service name Policies set up for Lambda function For AWS user â€œec2-userâ€, now need to have some policies with permissions to let â€œserverlessâ€ create role, Lambda function and deployment it â€¦ Roles for Lambda function Lambda function role created after Lambda function added and deployed into AWS. DeploymentMake sure AWS environment has been set up, including access key, user, group, policies â€¦ Pack and deploy Lambda example into AWS: 123456789101112131415161718$ sls deploy -r ap-southeast-2 -s devServerless: Packaging service...Serverless: Uploading CloudFormation file to S3...Serverless: Uploading service .zip file to S3 (583 B)...Serverless: Updating Stack...Serverless: Checking Stack update progress.....................Serverless: Stack update finished...Service Informationsservice: serverless-examplestage: devregion: ap-southeast-2api keys: Noneendpoints: Nonefunctions: serverless-example-dev-hello Lambda â€œhelloâ€ function A â€œhelloâ€ Lambda function has been created in Lambda after itâ€™s deployed into AWS by â€œserverlessâ€. Events generated during Lambda function deployment Deployment events generated during Lambda â€œhelloâ€ function deployed into AWS. Add Lambda Trigger on AWS API Gateway Manually create a Lambda Trigger. This time we use AWS API Gateway to trigger / invoke Lambda â€œhelloâ€ function. Exposed Lambda API Gateway After Lambda Trigger created, an exposed RESTful interface for Lambda â€œhelloâ€ function. Say â€œhelloâ€Set up AWS API Gateway trigger for Lambda â€œhelloâ€ function. Go to url, e.g.: Function â€œhelloâ€ log: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; \"message\": \"Go Serverless v1.0! Your function executed successfully!\", \"input\": &#123; \"resource\": \"/serverless-example-dev-hello\", \"path\": \"/serverless-example-dev-hello\", \"httpMethod\": \"GET\", \"headers\": &#123; \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\", \"Accept-Encoding\": \"gzip, deflate, sdch, br\", \"Accept-Language\": \"en-AU,en-GB;q=0.8,en-US;q=0.6,en;q=0.4\", \"CloudFront-Forwarded-Proto\": \"https\", \"CloudFront-Is-Desktop-Viewer\": \"true\", \"CloudFront-Is-Mobile-Viewer\": \"false\", \"CloudFront-Is-SmartTV-Viewer\": \"false\", \"CloudFront-Is-Tablet-Viewer\": \"false\", \"CloudFront-Viewer-Country\": \"AU\", \"Host\": \"b5dyhej16l.execute-api.ap-southeast-2.amazonaws.com\", \"Referer\": \"https://ap-southeast-2.console.aws.amazon.com/lambda/home?region=ap-southeast-2\", \"upgrade-insecure-requests\": \"1\", \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36\", \"Via\": \"2.0 6884828476070d32978b45d03c1cc437.cloudfront.net (CloudFront)\", \"X-Amz-Cf-Id\": \"mvToMffe1AsUJNcMJKUh-Rx26oBJsRBe2n9I1df3xqIAIENPR_ku3A==\", \"X-Amzn-Trace-Id\": \"Root=1-58aae2ff-0b0c5e4059cc97576211ba4a\", \"X-Forwarded-For\": \"101.181.175.227, 54.239.202.65\", \"X-Forwarded-Port\": \"443\", \"X-Forwarded-Proto\": \"https\" &#125;, \"queryStringParameters\": null, \"pathParameters\": null, \"stageVariables\": null, \"requestContext\": &#123; \"accountId\": \"624388274630\", \"resourceId\": \"5jbqsp\", \"stage\": \"prod\", \"requestId\": \"51ba2876-f769-11e6-b507-4b10c8a6886a\", \"identity\": &#123; \"cognitoIdentityPoolId\": null, \"accountId\": null, \"cognitoIdentityId\": null, \"caller\": null, \"apiKey\": null, \"sourceIp\": \"101.181.175.227\", \"accessKey\": null, \"cognitoAuthenticationType\": null, \"cognitoAuthenticationProvider\": null, \"userArn\": null, \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36\", \"user\": null &#125;, \"resourcePath\": \"/serverless-example-dev-hello\", \"httpMethod\": \"GET\", \"apiId\": \"b5dyhej16l\" &#125;, \"body\": null, \"isBase64Encoded\": false &#125;&#125; References serverless framework, https://serverless.com/ Example source code and artefact, https://github.com/TerrenceMiao/AWS/tree/master/serverless-example","tags":[]},{"title":"The State Quadrants - Functional vs. Object Oriented","date":"2017-02-18T12:19:56.000Z","path":"2017/02/18/The-State-Quadrants-Functional-vs-Object-Oriented/","text":"","tags":[]},{"title":"Factorial function implementation in Java 8","date":"2017-02-18T11:15:33.000Z","path":"2017/02/18/Factorial-function-implementation-in-Java-8/","text":"Implementation1234567891011121314151617181920212223242526package org.paradise.function;import java.util.HashMap;import java.util.Map;import java.util.function.Function;/** * Created by terrence on 12/12/2016. */public final class FactorialFunction &#123; public static final Map&lt;Integer, Long&gt; FACTORIAL_MAP = new HashMap&lt;&gt;(); public static final Function&lt;Integer, Long&gt; FACTORIAL = (x) -&gt; FACTORIAL_MAP.computeIfAbsent(x, n -&gt; n * FactorialFunction.FACTORIAL.apply(n - 1)); static &#123; FACTORIAL_MAP.put(1, 1L); // FACTORIAL(1) &#125; private FactorialFunction() &#123; &#125;&#125; Unit test123456789101112131415161718192021package org.paradise.function;import org.junit.Test;import static org.junit.Assert.assertEquals;/** * Created by terrence on 12/12/2016. */public class FactorialFunctionTest &#123; @Test public void testFactorialFunction() throws Exception &#123; assertEquals(\"Incorrect result\", Long.valueOf(1), FactorialFunction.FACTORIAL.apply(1)); assertEquals(\"Incorrect result\", Long.valueOf(2), FactorialFunction.FACTORIAL.apply(2)); assertEquals(\"Incorrect result\", Long.valueOf(3628800), FactorialFunction.FACTORIAL.apply(10)); &#125;&#125;","tags":[]},{"title":"Fibonacci function implementation in Java 8","date":"2017-02-18T11:09:56.000Z","path":"2017/02/18/Fibonacci-function-implementation-in-Java-8/","text":"Implementation123456789101112131415161718192021222324252627package org.paradise.function;import java.util.HashMap;import java.util.Map;import java.util.function.Function;/** * Created by terrence on 12/12/2016. */public final class FibonacciFunction &#123; public static final Map&lt;Integer, Long&gt; FIBONACCI_MAP = new HashMap&lt;&gt;(); public static final Function&lt;Integer, Long&gt; FIBONACCI = (x) -&gt; FIBONACCI_MAP.computeIfAbsent(x, n -&gt; FibonacciFunction.FIBONACCI.apply(n - 2) + FibonacciFunction.FIBONACCI.apply(n - 1)); static &#123; FIBONACCI_MAP.put(0, 0L); // FIBONACCI(0) FIBONACCI_MAP.put(1, 1L); // FIBONACCI(1) &#125; private FibonacciFunction() &#123; &#125;&#125; Unit test12345678910111213141516171819202122232425262728293031package org.paradise.function;import org.junit.Test;import static org.junit.Assert.assertEquals;/** * Created by terrence on 12/12/2016. */public class FibonacciFunctionTest &#123; @Test public void testFibonacciFunction() throws Exception &#123; assertEquals(\"Incorrect result\", Long.valueOf(0), FibonacciFunction.FIBONACCI.apply(0)); assertEquals(\"Incorrect result\", Long.valueOf(1), FibonacciFunction.FIBONACCI.apply(1)); assertEquals(\"Incorrect result\", Long.valueOf(1), FibonacciFunction.FIBONACCI.apply(2)); assertEquals(\"Incorrect result\", Long.valueOf(2), FibonacciFunction.FIBONACCI.apply(3)); assertEquals(\"Incorrect result\", Long.valueOf(3), FibonacciFunction.FIBONACCI.apply(4)); assertEquals(\"Incorrect result\", Long.valueOf(5), FibonacciFunction.FIBONACCI.apply(5)); assertEquals(\"Incorrect result\", Long.valueOf(8), FibonacciFunction.FIBONACCI.apply(6)); assertEquals(\"Incorrect result\", Long.valueOf(13), FibonacciFunction.FIBONACCI.apply(7)); assertEquals(\"Incorrect result\", Long.valueOf(21), FibonacciFunction.FIBONACCI.apply(8)); assertEquals(\"Incorrect result\", Long.valueOf(34), FibonacciFunction.FIBONACCI.apply(9)); assertEquals(\"Incorrect result\", Long.valueOf(55), FibonacciFunction.FIBONACCI.apply(10)); assertEquals(\"Incorrect result\", Long.valueOf(12586269025L), FibonacciFunction.FIBONACCI.apply(50)); &#125;&#125;","tags":[]},{"title":"Remote debugging Java applications run on Tomcat","date":"2017-02-11T02:52:26.000Z","path":"2017/02/11/Remote-debugging-Java-applications-run-on-Tomcat/","text":"Enable JVM option to attach a remote debugger: 1$ export JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 â€œsuspendâ€ set to â€œyâ€ is to let remote debugger start loading the application. Now, start running Tomcat. JVM debugging port is bound on port 5005i on the machine runs on Tomcat. Next, set up SSH tunnel mirror remotei host (ip-10-213-79-77.ap-southeast-2.compute.internal) 5005 port to localhost on port 5005. For example: 1$ ssh -L 5005:ip-10-213-79-77.ap-southeast-2.compute.internal:5005 -l ec2-user ip-10-213-79-77.ap-southeast-2.compute.internal You can start remote debugging in IDE like IntelliJ and debug the code since.","tags":[]},{"title":"Web Components are coming","date":"2017-02-01T12:56:19.000Z","path":"2017/02/01/Web-Components-are-coming/","text":"AngularJS is going to continue to succeed for some time. But change is inevitable. Web Components are coming. WebComponents create the ability to do all the sorts of markup-driven programming like AngularJS, ReactJS, but less ecosystem dependent. Because DOM is integration point for all the kinds of JavaScript frameworks. Web Components make it MUCH easier to interoperate between components. The future isnâ€™t here yet, but it will change fundamental assumptions about how a JavaScript framework should act and what it should be responsible for. Those shifts in assumptions frequently cause frameworks will drop out of the ecosystem quickly than expected. URL: https://www.webcomponents.org/","tags":[]},{"title":"Perfection","date":"2017-02-01T12:35:24.000Z","path":"2017/02/01/Perfection/","text":"Il semble que la perfection soit atteinte non quand il nâ€™y a plus rien Ã  ajouter, mais quand il nâ€™y a plus rien Ã  retrancher. 1- Antoine de saint Exupery It seems that perfection is attained not when there is nothing more to add, but when there is nothing more to remove.","tags":[]},{"title":"Setup Docker Private Registry in Nexus Repository OSS 3.x.x","date":"2017-01-28T03:37:50.000Z","path":"2017/01/28/Setup-Docker-Private-Registry-in-Nexus-Repository-OSS-3-x-x/","text":"Make sure Nexus Repository has been setup with Self Signed certificate, certificate for host/server e.g. â€œsilencer.bigpondâ€. The following instructions have been successfully tested in Nexus version 3.2.0-01. Create Docker Hub repository in Nexus Create Docker Internal repository in Nexus Create Docker Group repository in Nexus Run Docker with Docker NativeAdd Docker Private Registry in Insecure Registries Now this approach supports docker pull and docker push. Work around with â€œx509: certificate signed by unknown authorityâ€œ error by adding â€œâ€“disable-content-trustâ€ option on docker push command line if Docker doesnâ€™t accept Self-Signed certificate. Add Docker Private Registry serverâ€™s certificate into Docker Virtual Machine CA list1234567891011121314151617181920ğœ† keytool -printcert -rfc -sslserver silencer.bigpond:8444 &gt; silencer.bigpond.pem-----BEGIN CERTIFICATE-----MIIDkDCCAnigAwIBAgIEAqo9kTANBgkqhkiG9w0BAQsFADBgMQswCQYDVQQGEwJBVTEUMBIGA1UECBMLVW5zcGVjaWZpZWQxFDASBgNVBAcTC1Vuc3BlY2lmaWVkMREwDwYDVQQKEwhTb25hdHlwZTESMBAGA1UEAwwJKi5iaWdwb25kMB4XDTE2MDYxMjExMzY1M1oXDTMwMDIxOTExMzY1M1owYDELMAkGA1UEBhMCQVUxFDASBgNVBAgTC1Vuc3BlY2lmaWVkMRQwEgYDVQQHEwtVbnNwZWNpZmllZDERMA8GA1UEChMIU29uYXR5cGUxEjAQBgNVBAMMCSouYmlncG9uZDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJNO5mDpBDQQ8n4t0P2z8ChWzIFQ3Pf+5U8x6P17O3WtKTfsbuRYobHYmas5tVdVdnLIqpb4JV9DWIoS+CNG6cRLy3GIWWT7CbjsrpDlOTArslvk3KuzQ0dsZNflRfdd+ughI2LczehEfhzPJzA+ZU8Am1CadM+VUa+T6MilFQMXpWfjND6BNnV+qr/MX1QQfSjiWt7oWBex0BB0VPv9ooBZUqO+8jk5fUY8wEIa/kqLUqIKGxIUx9BMQBwBJwDKZmK93DXSPvAFYbKQjj6/nbV9R1VWmR7fhkLG+Ixlx5ld2dxv4+xvXmS8s4NanBtKZWUfEYVPp7gUF9HZoW9A1jcCAwEAAaNSMFAwDAYDVR0TBAUwAwEB/zAhBgNVHREEGjAYghBzaWxlbmNlci5iaWdwb25khwQKAAAJMB0GA1UdDgQWBBQV3WTuC+GI8lHtH0uL+kYqTG+vczANBgkqhkiG9w0BAQsFAAOCAQEAUwL+qnKVT0ENZEZnDjB+cjPfvkeWOD05PrGUOn4YB4vllq2S6Cgfm0OaZ+vMt3KMXPf9pIgZ797jdPhOP/s5IVJItldky+u/Hk9gNtUwEjpgl0MjhSm/PqxR5XoJdkYlvUdtq+PTrU5RU3v3GImeOmlI4mM5PaZ6OT8HC5VMX5s9RawBr/5EbJHRM7EN8r3g4Y/2109YoHoiWAhnN6TC3RhmCoQqGOiPsS732KHUz3KqXVbq9VTRdA3dXqFj1cUSet1TXTPisaiehffvbqYm2vrJ5WYgqCwb8TadDg66TToj080qvA8cXAF7qlA8pOImrbVOs7tdANSAs+AOcqCkiA==-----END CERTIFICATE----- 1ğœ† screen ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/tty Inside Docker Virtual Machine, follow these steps: 123$ sudo cat silencer.bigpond.pem &gt;&gt; /etc/ssl/certs/ca-certificates.crt$ sudo /etc/init.d/docker restart$ tail -f /var/lib/boot2docker/docker.log With Docker ToolboxAdd Docker Private Registry serverâ€™s certificate into Docker Virtual Machine CA list12ğœ† keytool -printcert -rfc -sslserver silencer.bigpond:8444 &gt; silencer.bigpond.pemğœ† docker-machine ssh default Inside Docker Virtual Machine, follow these steps:123$ sudo cat silencer.bigpond.pem &gt;&gt; /etc/ssl/certs/ca-certificates.crt$ sudo /etc/init.d/docker restart$ tail -f /var/lib/boot2docker/docker.log Press Ctrl + D and Ctrl + D to exist SCREEN program. Type â€œscreen -râ€ to re-entry SCREEN program. Additional CertificatesDownload 3rd party Repository serverâ€™s certificate. Use tool like KeyStore Explorer to add 3rd party server certificates into existing keystore.jks. Due to a certificate chain that does not exist within the existing Java truststore, Java does not trust the certificate and fails to connect to the application. Test1234ğœ† docker login silencer.bigpond:18443Username: adminPassword: admin123Login Succeeded 12ğœ† docker search silencer.bigpond:18443/tomcatğœ† docker pull silencer.bigpond:18443/jtech/tomcat:latest 1234ğœ† docker login silencer.bigpond:18444Username: adminPassword: admin123Login Succeeded 123ğœ† docker build -t jtech/camel-spring:latest .ğœ† docker tag jtech/camel-spring silencer.bigpond:18444/jtech/camel-spring:latestğœ† docker push silencer.bigpond:18444/jtech/camel-spring:latest 1ğœ† docker push --disable-content-trust silencer.bigpond:18444/jtech/camel-spring:latest Note Due to Docker Virtual Machine is immutable, the CA certificate added change made inside VM is ephemeral, and lost after VM is restarted Docker Private Registry ONLY supports HTTPS, NOT HTTP Nexus repository MUST register and use server certificate for host e.g. â€œsilencer.bigpondâ€, NOT â€œlocalhost.bigpondâ€ to run Docker Private Registry Try with â€œâ€“disable-content-trustâ€ if error like â€œGet https://silencer.gateway:18444/v1/_ping: x509: certificate signed by unknown authorityâ€ Reference Private Registry for Docker, https://books.sonatype.com/nexus-book/3.0/reference/docker.html Using Self-Signed Certificates with Nexus Repository Manager and Docker Daemon, https://support.sonatype.com/hc/en-us/articles/217542177 SSL Certificate Guide, https://support.sonatype.com/hc/en-us/articles/213465768 Setup HTTPS access Nexus Repository, https://github.com/TerrenceMiao/nexus/wiki/Setup-HTTPS-access-Nexus-Repository-Manager-OSS-3.xx Add Self Signed CA certificate into Dockerâ€™s CA list, https://github.com/klippx/inject-docker-certs Adding Self Signed certificate in Docker Native for Mac, https://forums.docker.com/t/adding-self-signed-certificates/9761","tags":[]},{"title":"Setup HTTPS access in Nexus Repository Manager OSS 3.x.x","date":"2017-01-28T02:20:47.000Z","path":"2017/01/28/Setup-HTTPS-access-in-Nexus-Repository-Manager-OSS-3-x-x/","text":"Generate Self Signed certificateOn a Mac at home, with Bigpond internet access. Full host name is silencer.bigpond and IP Address is 10.0.0.9. 12terrence@Silencer /Applications/nexus-3.0.0-03/etc/ssl00:13:05 ğœ† keytool -genkeypair -keystore keystore.jks -storepass changeit -keypass changeit -alias jetty -keyalg RSA -keysize 2048 -validity 5000 -dname \"CN=*.bigpond, O=Sonatype, L=Unspecified, ST=Unspecified, C=AU\" -ext \"SAN=DNS:silencer.bigpond,IP:10.0.0.9\" -ext \"BC=ca:true\" OR run â€œnslookup 127.0.0.1â€ return full domain hostname e.g. â€œlocalhost.bigpondâ€. 12terrence@Silencer /Applications/nexus-3.0.0-03/etc/ssl00:13:05 ğœ† keytool -genkeypair -keystore keystore.jks -storepass changeit -keypass changeit -alias jetty -keyalg RSA -keysize 2048 -validity 5000 -dname \"CN=*.bigpond, O=Sonatype, L=Unspecified, ST=Unspecified, C=AU\" -ext \"SAN=DNS:localhost.bigpond,IP:127.0.0.1\" -ext \"BC=ca:true\" Now, with latest Nexus (version 3.2.0-01) you can use self-signed server certificate without specifying IP address. 12terrence@Silencer /usr/local/nexus-3.2.0-01/etc/ssl00:13:05 ğœ† keytool -genkeypair -keystore keystore.jks -storepass changeit -keypass changeit -alias jetty -keyalg RSA -keysize 2048 -validity 5000 -dname \"CN=*.gateway, O=Sonatype, L=Unspecified, ST=Unspecified, C=AU\" -ext \"SAN=DNS:silencer.gateway\" -ext \"BC=ca:true\" Enable HTTPS accessChange jetty-https.xml file: 123456789101112terrence@Silencer /Applications/nexus-3.0.0-03/etc00:18:59 ğœ† diff jetty-https.xml jetty-https.xml.orig25,26c25,26&lt; &lt;Set name=\"KeyStorePassword\"&gt;changeit&lt;/Set&gt;&lt; &lt;Set name=\"KeyManagerPassword\"&gt;changeit&lt;/Set&gt;---&gt; &lt;Set name=\"KeyStorePassword\"&gt;OBF:1v2j1uum1xtv1zej1zer1xtn1uvk1v1v&lt;/Set&gt;&gt; &lt;Set name=\"KeyManagerPassword\"&gt;OBF:1v2j1uum1xtv1zej1zer1xtn1uvk1v1v&lt;/Set&gt;28c28&lt; &lt;Set name=\"TrustStorePassword\"&gt;changeit&lt;/Set&gt;---&gt; &lt;Set name=\"TrustStorePassword\"&gt;OBF:1v2j1uum1xtv1zej1zer1xtn1uvk1v1v&lt;/Set&gt; Add SSL port and include jetty-https.xml in file: 12345678terrence@Silencer /Applications/nexus-3.0.0-03/etc00:19:06 ğœ† diff org.sonatype.nexus.cfg org.sonatype.nexus.cfg.orig3d2&lt; application-port-ssl=84445c4&lt; nexus-args=$&#123;karaf.etc&#125;/jetty.xml,$&#123;karaf.etc&#125;/jetty-http.xml,$&#123;karaf.etc&#125;/jetty-https.xml,$&#123;karaf.etc&#125;/jetty-http-redirect-to-https.xml,$&#123;karaf.etc&#125;/jetty-requestlog.xml---&gt; nexus-args=$&#123;karaf.etc&#125;/jetty.xml,$&#123;karaf.etc&#125;/jetty-http.xml,$&#123;karaf.etc&#125;/jetty-requestlog.xml Retrieve serverâ€™s certificate 1234567891011121314151617181920ğœ† keytool -printcert -rfc -sslserver silencer.bigpond:8444 &gt; silencer.bigpond.pem-----BEGIN CERTIFICATE-----MIIDkDCCAnigAwIBAgIEAqo9kTANBgkqhkiG9w0BAQsFADBgMQswCQYDVQQGEwJBVTEUMBIGA1UECBMLVW5zcGVjaWZpZWQxFDASBgNVBAcTC1Vuc3BlY2lmaWVkMREwDwYDVQQKEwhTb25hdHlwZTESMBAGA1UEAwwJKi5iaWdwb25kMB4XDTE2MDYxMjExMzY1M1oXDTMwMDIxOTExMzY1M1owYDELMAkGA1UEBhMCQVUxFDASBgNVBAgTC1Vuc3BlY2lmaWVkMRQwEgYDVQQHEwtVbnNwZWNpZmllZDERMA8GA1UEChMIU29uYXR5cGUxEjAQBgNVBAMMCSouYmlncG9uZDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJNO5mDpBDQQ8n4t0P2z8ChWzIFQ3Pf+5U8x6P17O3WtKTfsbuRYobHYmas5tVdVdnLIqpb4JV9DWIoS+CNG6cRLy3GIWWT7CbjsrpDlOTArslvk3KuzQ0dsZNflRfdd+ughI2LczehEfhzPJzA+ZU8Am1CadM+VUa+T6MilFQMXpWfjND6BNnV+qr/MX1QQfSjiWt7oWBex0BB0VPv9ooBZUqO+8jk5fUY8wEIa/kqLUqIKGxIUx9BMQBwBJwDKZmK93DXSPvAFYbKQjj6/nbV9R1VWmR7fhkLG+Ixlx5ld2dxv4+xvXmS8s4NanBtKZWUfEYVPp7gUF9HZoW9A1jcCAwEAAaNSMFAwDAYDVR0TBAUwAwEB/zAhBgNVHREEGjAYghBzaWxlbmNlci5iaWdwb25khwQKAAAJMB0GA1UdDgQWBBQV3WTuC+GI8lHtH0uL+kYqTG+vczANBgkqhkiG9w0BAQsFAAOCAQEAUwL+qnKVT0ENZEZnDjB+cjPfvkeWOD05PrGUOn4YB4vllq2S6Cgfm0OaZ+vMt3KMXPf9pIgZ797jdPhOP/s5IVJItldky+u/Hk9gNtUwEjpgl0MjhSm/PqxR5XoJdkYlvUdtq+PTrU5RU3v3GImeOmlI4mM5PaZ6OT8HC5VMX5s9RawBr/5EbJHRM7EN8r3g4Y/2109YoHoiWAhnN6TC3RhmCoQqGOiPsS732KHUz3KqXVbq9VTRdA3dXqFj1cUSet1TXTPisaiehffvbqYm2vrJ5WYgqCwb8TadDg66TToj080qvA8cXAF7qlA8pOImrbVOs7tdANSAs+AOcqCkiA==-----END CERTIFICATE----- To get another Source Code Repository serverâ€™s certificate 1ğœ† keytool -printcert -rfc -sslserver bitbucket.cd.paradise.org:443 &gt; bitbucket.cd.paradise.org.pem TestRestart Nexus and access: https://localhost:8444 Note Use utility tool â€œKeyStore Explorerâ€ add additional CA certificates into keystore.jks file, especially when Gradle / Maven output error like: 1&gt; sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target","tags":[]},{"title":"Hello World","date":"2017-01-13T08:43:03.000Z","path":"2017/01/13/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]}